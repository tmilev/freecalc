\documentclass[12pt]{book}
%Notes.
%EITHER 1. Compile with the dvi->ps->pdf toolchain
%OR     2. Compile with pdflatex -shell-escape 

%Notes: modify command \optionalDisplay to speed up computation times.
\usepackage{etex}
\usepackage{amsmath, amsfonts, amssymb, verbatim, hyperref}
\usepackage{makeidx}

\usepackage{auto-pst-pdf}
\usepackage{pst-math}
\usepackage{pst-solides3d}
\usepackage{pst-3dplot}

% To make everything into one file, we include the following style file directly.
% This might need to be updated over time.
%\usepackage{../pstricks-commands}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% List of commands in this document
%
%
% \psFullDot        (Default dot to be used for points on graphs of functions).
% \psHollowDot      (Default dot to be used for points on graphs of functions).
% \psHollowDotBlue 
% \psFullDotBlack
% \psFullDotBlue
% \psLabelXOne      (labels one on x axis)
% \psLabelYOne      (labels one on y axis)
% \psLabels         (writes x and y labels )
% \psLabelsWithOnes (same as \psLabels + in addition labels 1 on the x and y axis)
%\psaxesStandard (default display of axes with x and y label, takes two arguments)
%\psLabelNumberXaxis
%\psLabelNumberYaxis
%\psXTickWithLabel
%\psYTickWithLabel
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\psHollowDot}[2]{
\pscircle*[fillcolor=white, linecolor=red](#1, #2){0.07}
\pscircle*[fillcolor=white, linecolor=white](#1, #2){0.04}
}
\newcommand{\psFullDot}[2]{
\pscircle*[fillcolor=white, linecolor=red](#1, #2){0.07}
}

\newcommand{\psHollowDotBlue}[2]{
\pscircle*[fillcolor=white, linecolor=blue](#1, #2){0.07}
\pscircle*[fillcolor=white, linecolor=white](#1, #2){0.04}
}
\newcommand{\psFullDotBlack}[2]{
\pscircle*[fillcolor=white, linecolor=black](#1, #2){0.07}
}
\newcommand{\psFullDotBlue}[2]{
\pscircle*[fillcolor=white, linecolor=blue](#1, #2){0.07}
}
\newcommand{\psXTickColored}[2]{\psline[linecolor=#1](#2, -0.1)(#2,0.1)}

\newcommand{\psXTick}[1]{\psline(#1, -0.1)(#1,0.1)}
\newcommand{\psYTick}[1]{\psline(-0.1, #1)(0.1, #1)}
\newcommand{\psXYTick}[2]{\psXTick{#1} \psYTick{#2}}

\newcommand{\psXTickWithLabel}[2]{\psXTick{#1}\rput[t](#1,-0.2){#2}}
\newcommand{\psYTickWithLabel}[2]{\psYTick{#1}\rput[r](-0.2,#1){#2}}

\newcommand{\psLabelNumberXaxis}[1]{\psXTickWithLabel{#1}{#1}}
\newcommand{\psLabelNumberYaxis}[1]{\psYTickWithLabel{#1}{#1}}

\newcommand{\psLabelNumberXYaxes}[2]{\psLabelNumberXaxis{#1} \psLabelNumberYaxis{#2} }

\newcommand{\psLabelXOne}{\psLabelNumberXaxis{1} }
\newcommand{\psLabelYOne}{\psLabelNumberYaxis{1} }

\newcommand{\psLabelOnXaxis}[2]{\psXTick{#1}\rput[t](#1,-0.2){#2}}
\newcommand{\psLabelOnYaxis}[2]{\psYTick{#1}\rput[r](-0.2, #1){#2}}

\newcommand{\psLabels}[2]{\rput[t](#1, -0.1){$x$}\rput[r](-0.1, #2){$y$}}
\newcommand{\psLabelsWithOnes}[2]{\psline(1, -0.1)(1,0.1) \rput[t](1, -0.2 ) { $1$} \psline(-0.1, 1)(0.1, 1) \rput[r](-0.2, 1 ) { $1$} \psLabels{#1}{#2}}

\newcommand{\psaxesStandard}[4]{\psaxes[ticks=none, labels=none]{<->}(0,0)(#1,#2)(#3,#4)\psLabels{#3}{#4}}
\newcommand{\psColorTangent}{blue}
\newcommand{\psColorGraph}{red}
\newcommand{\psColorAreaUnderGraph}{cyan}
\newcommand{\psColorNegativeAreaUnderGraph}{orange}

\newcommand{\psMachine}[2]{
\pscustom*[linecolor=#2]{
\psline(1,1.1)(1,0.1)(1.5,0.1)(2, 0.6)(2.5, 0.6)(2.5, -0.6)(2, -0.6)(1.5,-0.1)(1,-0.1)(1,-1.1)(-1,-1.1)(-1,-0.1)(-1.5,-0.1)(-2, -0.6)(-2.5, -0.6)(-2.5, 0.6)(-2, 0.6)(-1.5,0.1)(-1,0.1)(-1,1.1)
}
\pscircle*[linecolor=white](0,0){0.3}
\rput(0,0){#1}
}

\newcommand{\diff}{\text{d}}

\usepackage{multicol}
\usepackage{rotating}

\usepackage{cancel}
\usepackage{caption}
%\usepackage{pstricks}

\renewcommand{\Im}{\mathrm{Im}}
\renewcommand{\Re}{\mathrm{Re}}
\newcommand{\eqdef}{\textbf{:=}}
\newcommand{\eqAttention}{\stackrel{(!)}{=}}
\newcommand{\importantFormula}[1]{\begin{equation} \boxed{#1} \end{equation}}
\newcommand{\importantText}[1]{ \framebox{#1}}
\newenvironment{tableFixed}{~\\~\medskip\begin{minipage}{\textwidth}\captionsetup{type=table} }{ \medskip \end{minipage} \medskip }
\newenvironment{figureFixed}{~\\~\medskip\begin{minipage}{\textwidth} \captionsetup{type=figure} }{ \medskip \end{minipage} \medskip }

\newenvironment{proof}[1][]{ \textbf{Proof#1.} }{$\Box$\medskip}
\newenvironment{proofOptional}[1][]{ \textbf{(Optional) Proof#1.}}{$\Box$\medskip}
\newenvironment{solution}{\textbf{Solution.} }{$\Box$}
\newtheorem{problem}{Problem}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Proposition}


\DeclareMathOperator{\arsinh}{arsinh}
\DeclareMathOperator{\arcosh}{arcosh}
\DeclareMathOperator{\arctanh}{arctanh}
\DeclareMathOperator{\maclaurin}{Maclaurin}
\DeclareMathOperator{\taylor}{Taylor}

\newcommand{\optionalDisplay}[1]{#1 }
%\renewcommand{\optionalDisplay}[1]{#1}
\newcommand{\doublebrace}[4]{\left\{\begin{array}{ll} #1 & #2 \\#3 & #4  \end{array} \right.}

\newcommand{\optionalMaterial}{\textbf{(Optional)}}
\renewcommand{\emph}{\textbf}

\makeindex
\author{Authors: the freecalc project contributors \\~\\ Notes composition started by Todor Milev.}
\title{Calculus for beginners \\Draft version \\ ~\\
\[
\lim\limits_{x\to 5^+}\frac{1}{x-5}= ?
\]
~\\
\[
\lim\limits_{x\to 8^+}\frac{1}{x-8}=\infty 
\]
~\\
$\Rightarrow$
~\\
\[
\lim\limits_{x\to 5^+}\frac{1}{x-5}= \begin{postscript}\rotatebox{90}{5}\end{postscript}
\]
}
\begin{document}
\maketitle

{
~

\bigskip

~

\bigskip

~

\bigskip

~

\bigskip

\begin{center}
\Huge{\color{red}\rotatebox{45}{\textbf{ DRAFT VERSION}}}
\end{center}
} %end centering

%\noindent \textbf{Study materials. } The present notes and the textbook by James Stewart, Calculus, 7th edition, published by Brooks Cole, 2012. ISBN-13: 978-0-538-49781-7, ISBN-10: 0-538-49781-5. 


\chapter*{Preface}


\section*{Disclaimer} 
This text is in preparation and does contain typos and errors. Please read critically!

\section*{Additional materials} Students are strongly encouraged to look up mathematics on Wikipedia. 


\section*{License to redistribute and modify} This text and its source code is licensed to you under the Creative Commons license CC BY 3.0. The license states that you are free
\begin{itemize}
\item to Share - to copy, distribute and transmit the work;
\item to Remix - to adapt, change, etc., the work
\item to make commercial use of the work.
\end{itemize}

For more details see the full text of the license \url{http://creativecommons.org/licenses/by/3.0/}.

\noindent \textbf{Compiling the .tex file: a note to the student.} If you would like to modify this file, you need to learn how to use \LaTeX. Note that all graphics in the notes are drawn using commands in the .tex source (the text uses no external images). The .tex file should compile ``out of the box'' on a Linux distribution with \LaTeX~ installed.

A semi-permanent link of the .tex source of this textbook can be found here. If the link is no longer functioning, please search for ``freecalc'' project on the web.

\url{https://www.assembla.com/code/freecalc/subversion/nodes/202/trunk/lectures/CalcNotes}

\section*{Using the textbook}

Some material in this book is marked as \optionalMaterial. Such material is considered to be outside of the scope of a regular Calculus course, yet may be needed to either provide a more rigorous exposition, or to provide a broader perspective on the topic. 

While material marked as \optionalMaterial ~should be accessible to a motivated student, it is expected to be of higher difficulty than the rest of the book. The author does not recommend the material marked as \optionalMaterial ~ as a requirement for successful completion of a course in Calculus.

\section*{Scope of the textbook}
This textbook does not attempt a rigorous exposition of the subject. However, all sacrifices of rigor have been made by omitting material and giving informal explanations of proofs of theorems. It has been the goal of this textbook to use completely standard, precise, and, even if slightly simplified, professional mathematical language. 

As the most important example of this we note that, while we have avoided a formal definition of differential forms, all statements about differential forms are fully compatible with the commonly accepted professional mathematical definition. To the calculus instructors reading this text, we recall that differential forms are defined as the dual (in the sense of linear algebra) vector space of the tangent vector space, which in turn is most frequently defined using linear differential operators on manifolds.

\tableofcontents

%\begin{comment}




%Here is a list of common notation and abbreviations. %This list will grow as the course progresses. Feel free to request additional notation.
%\begin{itemize}
%\item The expression $x\in Y$ is read ``$x$ belongs to $Y$''.
%\item $\mathbb Z$  denotes the integers.
%\item $\mathbb Z_{\geq 0}$ denotes the non-negative integers.
%\item $\mathbb R$ denotes the real numbers.
%\item $\mathbb R^2$ denotes the two dimensional vector space, i.e., the set of pairs of real numbers $(r_1, r_2)$. Such a pair of real numbers is usually referred to as a 2-dimensional vector. The numbers $r_1, r_2$ are called coordinates.
%\item $\mathbb R^3$ denotes the two dimensional vector space, i.e., the set of triples of real numbers $(r_1, r_2, r_3)$. Such a triple of real numbers is usually referred to as a 3-dimensional vector. The numbers $r_1, r_2, r_3$ are called coordinates.
%\item $\mathbb R^n$ is the generalization of $\mathbb R^2$, $\mathbb R^3$, for arbitrary number of coordinates.
%\item $\mathbb C$ denotes the complex numbers, defined in section \ref{secComplexNumbers}.
%\item $\displaystyle\sum_{k=1}^n f(k)\eqdef f(1)+f(2)+\dots +f(k-1)+f(k)$ is read ``sum of $f(k)$ as $k$ runs over the values $1,2,\dots, n$''. Below the sum sign we indicate the variable which varies as we take the sum. This variable is often called ``dummy variable''. 
%\item $\displaystyle\int\limits_{x=a}^b f(x)dx$ is read ``integral of $f(x)dx$ as $x$ runs in the interval $[a,b]$''. The similarities in the notation for $\sum$ and the $\int$ (can you spot such similarities?) are not a coincidence!
%\end{itemize}

\chapter{Basic mathematical language}


\section{Equalities vs equalities by definition }
Throughout this textbook, we use $\eqdef$ to postulate (``take as axiom'', ``assume as given without proof'') a formula, and we use $=$ to derive/deduce a formula. For example, when we write 
\[ 2+2=4
\]
we mean that we derive this equality (in this case, through the rules of elementary school arithmetics). When we derive an equality, both sides of the equality are already known objects (in the above case, the right hand side is the number $4$ and the left hand side is an expression build by the operation $+$ and two copies of the number $2$).  On the other hand, when we write 
\[
\tan x\eqdef\frac{\sin x}{\cos x}\quad ,
\]
we do not deduce the above formula; rather, we \emph{define} the expression $\tan x$ via the expression $\displaystyle\frac{\sin x}{\cos x}$.

\textbf{A note to computer scientists. } In many of the modern computer programming languages (for example, C, C++, java, Python, javascript), the equality sign is used in a way that does not corresponds to either $=$, or to $\eqdef$. In those programming languages, the $=$ sign instructs a computer to change the state of a given variable. This programming language operation does not have a corresponding symbolic mathematical notation; instead it roughly corresponds to the mathematical use of the expression ``Set $x$ to be \dots ''.
\section{Sets and elements}
\url{http://en.wikipedia.org/wiki/Set_(mathematics)}

In mathematics, a \emph{set} is a collection of distinct objects, considered as an object in its own right. The objects contained by a set are also called \emph{elements} of the set. We note that in the definition of a set, the words ``collection'', ``distinct'' and ``object'' are used as English language words, rather than as mathematical terms. In this way, the mathematical notions of sets and an elements of a set are based on the common sense and understanding of the reader. 

Sets are customarily denoted by using curly brackets enclosing the elements of the set. For example,
\[
\{1,2,5\}
\]
is the set consisting of the three elements $1,2,5$. When an element $x$ is contained in a set $X$, we write 
\[
x\in X
\]
and we read it as ``$x$ belongs to $X$''. Alternatively, we may say that $x$ lies in $X$ or $x$ is in $X$. When and element $y$ does not belong to a set $X$, we write 
\[
y\notin X\quad. 
\]
For example, $1\in \{1,2,5\}$ and $3\notin \{1,2,5\}$.
\section{Functions: domains, codomains, ranges} \label{secInverseFunctionBasics}

\url{http://en.wikipedia.org/wiki/Injective_function} \quad . 


\begin{definition}\index{function} A function $f$ is a rule assigning an element of a set $D$ to each element of another set $C$. We write
\[
f: D\to C \quad .
\]
We call the set $D$ the \emph{domain} of $f$ and $C$ the \emph{codomain} of $f$.
\end{definition}
We can imagine a function as a machine that takes input from its domain (``input set''), and produces output located in its codomain (``target set''). Furthermore, for each object at the input, exactly one output object is produced. 
\begin{figureFixed}
\optionalDisplay{
\psset{xunit=0.6cm,yunit=0.6cm}
\begin{pspicture}(0,0)(5,5)
\rput[r](-1.2,0){$D$}
\psellipse*[linecolor=cyan](0,0)(1, 3)
\psFullDot{0}{2.5}
\psFullDot{0}{1.5}
\psFullDot{0}{0.5}
\psFullDot{0}{-0.5}
\psFullDot{0}{-1.5}
\psFullDot{0}{-2.5}
\rput[t](0, -3.2){Domain}

\rput[l](6.2,0){$C$}
\psellipse*[linecolor=cyan](5,0)(1, 3)
\psFullDot{5}{2.5}
\psFullDot{5}{1.25}
\psFullDot{5}{0}
\psFullDot{5}{-1.25}
\psFullDot{5}{-2.5}
\psFullDot{5}{-2.5}
\rput[t](5, -3.2){Co-domain}
\rput(2.5, 2.5){$f$}
\psline[linestyle=dashed]{->}(0,2.5)(5,1.25)
\psline[linestyle=dashed]{->}(0,1.5)(5,-1.25)
\psline[linestyle=dashed]{->}(0,0.5)(5,-2.5)
\psline[linestyle=dashed]{->}(0,-0.5)(5,2.5)
\psline[linestyle=dashed]{->}(0,-1.5)(5,-1.25)
\psline[linestyle=dashed]{->}(0,-2.5)(5,2.5)
\end{pspicture}


} %end optional display
\caption{A function $f$ can be represented as a set of arrows from its domain to its co-domain.\label{figFunctionAsArrows}}
\end{figureFixed}
We call the elements of the domain the \emph{arguments} of the function. For an element $x\in D$, we write $f(x) $ to denote the output of the function $f$ with input $x$. We call $f(x)$ the \emph{image} of $x$ under $f$; we may alternatively say that $f$ \emph{maps} $x$ to $f(x)$.

It is not required that all elements in the codomain be obtainable by applying $f$ to the input. For example, for the function $f$ given in Figure \ref{figFunctionAsArrows}, one of the elements of the codomain $C$ is not the image of any element of the domain $D$ (which element?). 

\begin{definition}\index{function!range}
Given a function $f:D\to C$, the set 
\[
 \{f(x) | x\in D \},
\]
is called the \emph{range} of $f$. 
\end{definition}
In other words, the range of $f$ is the set of all images of elements in the domain of $f$.
\section{Inverse functions}

\begin{definition} ~
\begin{itemize}
\item \index{injection}\index{one to one} A function $f$ is \emph{injective (or one to one)} if  for all $x_1\neq x_2$ we have that $f(x_1)\neq f(x_2)$. 
\item \index{function!preimage} If $f(x)=r$ we say that $x$ is a \emph{pre-image} of $r$.
\item \index{surjection}
A function $f$ is a \emph{surjection (or onto)} if for every element $r$ in the codomain $C$, there exists an element $x$ with $f(x)=r$.
\item \index{bijection} A function is a \emph{bijection (or one to one and onto)} if it is both a surjection and an injection.
\end{itemize}
\end{definition}
In other words, $f$ is injection if it sends different elements to different elements, and $f$ is a surjection if all elements in its  codomain (``target set'') are obtained as images of the function $f$. In this way, $f$ is bijection if it sends different elements to different elements and every element in the codomain of $f$ has a preimage.

Let $f: \mathbb R\to \mathbb R$ (``$f$ maps the set of real numbers to the set of real numbers''). Let $y=f(x)$ be the graph of $f$.
\begin{criterion}[Horizontal line test]
$f$ is injective if every horizontal line $y=const$ intersects the graph $y=f(x)$ no more than once.
\end{criterion}

\begin{definition}\index{function!inverse}
Let $f:X\to Y$ be a function from a set $X$ onto its codomain $Y$, such that $f$ is a bijection (different elements are sent to different ones and every element in the target set has a preimage). Then the function $g: Y\to X$ is called an inverse of $f$ if $g(f(x))=x$ for all $x\in X$ and $f(g(y))=Y$ for all $y\in Y$. 
\end{definition}

\subsection{Inverse function notation }
\noindent The inverse of $f$ is denoted as $f^{-1}$. This notation is one of the most frequent causes of student confusion. In attempt to resolve that confusion we make a few important notes on notation. First and foremost,

\importantText{
do not confuse $f^{-1}(x)$ with $\frac{1}{f(x)} =\left(f(x)\right)^{-1}$.
}

\noindent The two notations are different: the position of the superscript $^{-1}$ is different for $f^{-1}(x)$ and $\left(f(x)\right)^{-1}$.

It is important to note that the notation $f^{2}(x)$ is an abbreviation for $(f(x))^2$,  $f^{3}(x)$ is an abbreviation of $(f(x))^3$, and so on. Nevertheless, $f^{-1}(x)$ is not the abbreviation of $\left(f(x)\right)^{-1}$ and hence does not follow the pattern. The mathematical use of superscripts of functions follows the following rules.
\[
f^{n}(x)= \left\{ \begin{array}{ll}\text{stands for } \left(f(x)\right)^n & \text{when } n=1,2,3,\dots \\
\text{stands for inverse of } f \text{ applied to }x&\text{when } n=-1\\
\text{should not be used }  & \text{when } n\neq -1, 1,2,3,\dots .
\end{array} \right.
\]
\textbf{Advice.} To avoid confusion, use whenever possible/convenient the completely unambiguous $\frac{1}{f(x)}$ instead of   $\left(f(x)\right)^{-1}$.

\importantText{
\begin{tabular}{l}
Whenever in doubt about notation, remove the ambiguity by \\
proper use of English language: write \\
\importantText{``Let $g$ be the inverse function of $f$. Then $g(x)=\dots$  ''}\\
instead of \\
\importantText{$f^{-1}(x)=\dots$}\quad .
\end{tabular}
}

For the student familiar with trigonometry, we make one more note. The inverse functions of the trigonometric functions $\sin, \cos, \tan$ have special names: the inverse function of $\sin $ is called $ \arcsin$, the inverse of $\cos $ is $\arccos$, the inverse of $\tan $ is $\arctan$. It is the personal opinion of one of the authors that you should

\importantText{
\begin{tabular}{l}
avoid the use the superscript $^{-1}$ to denote inverses \\
of trigonometric functions. Write \\ 
\importantText{
$\arcsin x, \arccos x, \arctan x$
}
\\
instead of
\\
\importantText{
$\sin^{-1}x, \cos^{-1}x, \tan^{-1}x$.
}
\end{tabular}
}
\chapter{Sequences and numbers}
In this chapter we define the most common sets of numbers used in mathematics - the integers, the rational numbers, the real numbers and the complex numbers. 

We may recommend that a Calculus course skip this Chapter as this material should be familiar to the student from a precalculus course. 

\section{Integers and sequences}\label{secSequences}
In this section we assume knowledge of the set of integers, and use that knowledge to define sequences.
\section{\optionalMaterial From integers to rational numbers }\label{secRationalNumbers}
\begin{definition}
A rational number is a number that can be expressed as the quotient of two integers. The set of all rational numbers is denoted by $\mathbb Q$.
\end{definition}
As we know from high school arithmetics, if \[\frac{p}{q}=\frac{s}{t}\quad ,\] 
then $tp=qs$ and so \[tp-qs=0\quad .\] 
That is why we may view the set of rational number $\mathbb Q $ as the set of pairs of numbers $(p,q)$ with condition that we identify $ (p,q)$ with $(s,t)$ if $tp-qs=0$.

\section{\optionalMaterial From rational numbers to real numbers}\label{secReals}



\section{\optionalMaterial Real number axioms}\label{secRealNumbersDef}
\section{ Complex numbers}

\label{secComplexNumbers}

\noindent \url{http://en.wikipedia.org/wiki/Complex_number}

\index{complex numbers}\index{imaginary!numbers}\index{imaginary!unit} The complex numbers are a set of numbers which contain the real numbers. The complex numbers are defined as the set of all pairs of real numbers $(a,b)$, equipped with addition, subtraction, multiplication and division operations, which we define later in the present section. The set complex numbers is denoted by $\mathbb C$. When referring to a complex number, instead of using the notation $(a,b)$ it is accepted to use the notation 
\[
a+i b\quad .
\]
In this way, the set of complex numbers $\mathbb C$ becomes 
\[
\mathbb C\eqdef \{a+ib| a,b\in \mathbb R\}\quad. 
\]
It is commonly accepted to abbreviate $ a+i*0$ as $a$, $0+i*b$ as $i*b$, $0+0*i$ as $0$ and $0+1*i$ as $i$. Complex numbers of the form $a=a+i*0$ are in addition called real numbers and complex numbers of the form $0+i*b=i*b$ are called imaginary numbers. \index{complex numbers!imaginary part of}\index{complex numbers!real part of} \index{real!part} \index{imaginary!part}The number $a$ in $a+ib$ is referred to as the \emph{real part} of $a+ib$ and the number $b$ in $a+ib$ is referred to as the of $a+ib$. We write
\[
\Re (a+ib)\eqdef a \quad, \quad \quad \Im (a+ib)\eqdef b \quad .
\]
The names of the various parts of $z=a+ib$ are given below.
\[
z=\underbrace{\overbrace{a}^{\text{Real part}=\Re z}+ \overbrace{i\underbrace{b}_{\text{Imaginary part}=\Im z}}^{\text{imaginary number}} }_{\text{Complex number}}
\]
Complex number addition is defined in the manner suggested by the complex number notation:
\[(a+i b)+ (c+id)\eqdef (a+c) + i(b+d) \quad .
\]
In order to have that subtraction be the inverse operation of the addition operation subtraction must be defined by 
\[
(a+i b)- (c+id)\eqdef (a-c) + i(b-d) \quad .
\]
Complex number addition is defined in the manner suggested by complex number notation, with the additional requirement that
\[
i^2\eqdef -1\quad .
\]
In other words, complex multiplication is defined as
\[
(a+ i b)(c+id)=ac +i^2 bd +i bc + i ad= (ac-bd)+i(bc+ad) \quad .
\]
Before we define complex division we cover the operation of complex conjugation. The \emph{complex conjugate} of a number $z$, denoted by $\bar z$, is defined via
\index{complex!conjugation}
\[\overline{a+ib}\eqdef a-ib \quad .
\]

\begin{proposition}
Complex conjugation respects both addition and multiplication:
\importantFormula{
\label{eqComplexConjugationIsAddHMM} 
\overline {z +w}= \bar z +\bar w\quad .
}
\importantFormula{
\label{eqComplexConjugationIsFieldHMM}  \overline {z w}= \bar z \bar w\quad .
}

\end{proposition}
\begin{proof}
We leave the proof of \eqref{eqComplexConjugationIsAddHMM} to the reader. Let $z= a+ib$ and $w=c+id$. Then
\begin{equation*}
\begin{array}{rcl}\overline {zw}&=& \overline{(a+ib)(c+id)}= \overline{ac-bd +i(ad+bc) }\\&=& ac-bd-i(ad+bc)= a(c-id)-ibc+i^{2}bd\\&=& a(c-id)-ib(c-id)= (a-ib)(c-id)\\&=&\bar z \bar w\quad .
\end{array}
\end{equation*}
\end{proof}
\index{complex!absolute value}  The absolute value of a complex number $z$, denoted by $|z|$, is defined via
\[
|a+ib|\eqdef \sqrt{a^2+b^2}\quad .
\]
We have that 
\[
|a+ib|^2\eqdef (a+ib)\overline{(a+ib)}=(a+ib)(a-ib)=a^2+b^2 \quad .
\]

Division of a complex number by a real number is defined in the manner suggested by complex number notation:
\[
\frac{a+ib}{c}= \frac{a}{c}+i\frac{b}{c}, \quad c\neq 0\quad .
\]
In order to have that be the inverse operation to multiplication, it follows that the division of $1$ by a number must be defined by
\[
\frac{1}{c+id}= \frac{c-id}{(c+id)(c-id)}=\frac{c}{c^2+d^2}-i\frac{d}{c^2+d^2}  \quad c+id\neq 0.
\]
Therefore division of arbitrary complex numbers is defined as 
\[
\frac{a+ib}{c+id}=(a+ib)\frac{1}{c+id}= \frac{(a+ib)(c-id)}{(c+id)(c-id)}= \frac{ac+bd}{c^2+d^2}+i\frac{(bc-ad)}{c^2+d^2} \quad .
\]

\subsection{Problems and exercises}
\begin{problem}\label{probComplexNumbersBasicOperations}
Carry out the operations. For items 4-6 you can first take a look at the Newton Binomial formula \eqref{eqNewtonBinomialFormula}.
\begin{multicols}{3}
\begin{enumerate}
\item $(5+3i)^2$.
\item $(5+3i)/(2-3i)$.
\item $(5+3i)^{-2}$.
\item $(1+i)^3$.
\item $(1+i)^4$.
\item \label{eq(1+i)^5} $(1+i)^5$.
\item \label{eq(1+i)^-5} $(1+i)^{-5}$.
\end{enumerate}
\end{multicols}
\end{problem}
\begin{solution}

\ref{eq(1+i)^5}.
By the Newton Binomial formula  \eqref{eqNewtonBinomialFormula}, we have that $(1+i)^5= 1 + 5i + 10 i^2+ 10i^3+5i^4+i^5= 1-10+5 +i(5-10+1)=-4-4i$.

\ref{eq(1+i)^-5}. Using the preceding example, we have that \[
(1+i)^{-5}=\frac{1}{(1+i)^5}=\frac{1}{ -4-4i}=\frac{-4+4i}{(-4-4i)(-4+4i)}=\frac{-4+4i}{32}=-\frac{1}{8}+\frac{1}{8}i\quad .
\]

\end{solution}
\section[\optionalMaterial A dictionary of numbers]{\optionalMaterial A dictionary of numbers: integers, rational numbers, algebraic numbers, transcendental numbers, real numbers, complex numbers}
%\end{comment}
%\begin{comment}
\chapter{A few algebra techniques}
In the present chapter we develop a basic formulas, and state a few basic combinatorial facts. The chapter relies on a conceptual, high-school level understanding of the notion of ``formula'', but does not require any practical knowledge other than applying the rules of arithmetic.
\section{Factorials, binomial coefficients}\label{secFactorial}
Let $n$ be a positive integer, i.e., let $n \in \mathbb Z_{\geq 0}$ (non-negative integers). 
\begin{definition}[Factorial] \index{factorial} 
For $n>0$, we define the number $n!$ by
\[
n!\eqdef n*(n-1)*(n-2)*\dots *4*3*2*1 \quad  .
\] 
and we define 
\[
0!\eqdef 1\quad .
\]
\end{definition}
The number $n!$ is read as ``$n$ factorial''. The first few factorial values are 
\[\begin{array}{rcl}
0!&=&1\\
1!&=&1\\
2!&=&2\\
3!&=&6\\
4!&=&4*3!=24\\
5!&=&5*4!=120\\ 
6!&=&6*5!=720\\
7!&=&7*6!=5040  \\
&\vdots&\quad \quad .
\end{array}
\]

\index{binomial!formula} Newton binomial formula.
\index{binomial!coefficient definition} Let $k\in \mathbb Z_{>0}$. The binomial coefficient $\binom{x}{k}$ is read as ``$x$ choose $k$''.
\importantFormula{\label{eqBinomialCoeffDefinition}
\binom{x}{k}\eqdef \frac{ \overbrace{ x(x-1)(x-2)(x-3)\dots (x-k+1)}^{\mathrm{k~multiplicands~total}} } {k!} \quad .
}

\[\binom{x}{0}\eqdef 1 \quad .
\]

Suppose $k,n\in \mathbb Z_{\geq 0}$ (non-negative integers). Then

\[\binom{n}{k} = \frac{n!}{(n-k)! k!}\quad .
\]

\section{Newton binomial formula}\label{secNewtonBinomialReview}
\url{http://en.wikipedia.org/wiki/Binomial_theorem}



\importantFormula{\label{eqNewtonBinomialFormula}
\begin{array}{rcl}
(a+b)^{n} &=&\displaystyle \sum\limits_{i=0}^n \binom {n}{k} a^{n-k}b^{k} \\
&=&\displaystyle \binom{n}{0} a^n + \binom{n}{1}\phantom{n}a^{n-1}b +\dots+ \binom{n}{n-1} \phantom{n} ab^{n-1} + \binom{n}{n} b^n\\
&=&\displaystyle \phantom{\binom{n}{0}}a^n + \phantom{\binom{n}1} n a^{n-1}b+\dots + \phantom{\binom{n}{n-1}} na b^{n-1}+\phantom{\binom{n}{n}} b^n \quad .
\end{array}
}

\chapter{Limits, continuity, derivatives}
\section{The tangent problem}
Consider the computer- generated plot of the function $f(x)=\frac{x^2}2$ below. The line drawn in blue in Figure \ref{figTangentIdeaFor} appears to just touch the graph of the function $y=\frac{x^2}2$ at the point $P=(2,f(2))=(2, \frac{2^2}{2}) =(2,2)$. In mathematical language we say that the blue line is tangent to the graph of $ y=\frac{x^2}2$ (``tangent'' comes from Latin, ``to touch''). We shall give a formal definition of a tangent line later in Section \ref{secDefTangent}; until then, we shall refer to a ``tangent line'' informally, relying on the reader's intuition.

We are seeking an equation for the tangent through $P=(2,2)=(2, f(2))$ drawn in \ref{figTangentIdeaFor}. We know one point on the tangent line - namely the point $P=(2,2)$. Recall that every non-vertical line has equation 
\[
y=mx+c
\]
for some numbers $m$ and $c$, where $m$ is called the slope
of the line and $c$ is called the $y$-intercept of the line. As the tangent line passes through the point $P=(2,2)$, it has equation $y-2=m(x-2)$ for some slope $m$ that we yet need to define. 

\begin{figureFixed}
\begin{center}
\optionalDisplay{
%\psset{xunit=1cm, yunit=1cm}
%\begin{pspicture}(-1, -5)(3,5) 
%\psframe*[linecolor=white](-1,-5)(3,5) 
%\tiny 
%\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3,4.5)
%Function formula: 1/2 ((x)^{2}) 
%\rput(1,3){$y= \frac{x^2}2$} 
%\psplot[linecolor=\psColorGraph, plotpoints=1000]{-1}{3}{x 2 exp 0.5 mul }
%\psFullDot{2}{2}
%\rput[tl](2.1, 1.9){$P=(2,2)$}
%\psXTick{2}
%\psFullDot{0}{0}
%\rput[tl](0.1, -0.1){$Q=\left(0,0\right)$}
%\psplot[linecolor=\psColorTangent, plotpoints=1000]{-0.5}{3}{x}
%\end{pspicture} 
\noindent \psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1, -5)(3,5) 
\psframe*[linecolor=white](-1,-5)(3,5) 
\tiny 
\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3.1,4.805)
%Function formula: 1/2 ((x)^{2}) 
\rput(1,3){$y= \frac{x^2}2$} 
\psplot[linecolor=\psColorGraph, plotpoints=1000]{-1}{3.1}{x 2 exp 0.5 mul }
\psFullDot{2}{2}
\rput[br](1.9, 2.1){$P$}
\rput[tl](2.1, 1.9){$(2,2)$}
\psXTick{2}
\psFullDot{1}{0.5}
\rput[br](0.9, 0.6){$Q$}
\rput[tl](1.1, 0.4){$\left(1,0.5\right)$}
\psXTick{1}
\psline[linecolor=\psColorTangent](0.3333333,-0.5)(3,3.5)
\end{pspicture} 
\psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1, -5)(3,5) 
\psframe*[linecolor=white](-1,-5)(3,5) 
\tiny 
\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3.1,4.805)
%Function formula: 1/2 ((x)^{2}) 
\rput(1,3){$y= \frac{x^2}2$} 
\psplot[linecolor=\psColorGraph, plotpoints=1000]{-1}{3.1}{x 2 exp 0.5 mul }
\psFullDot{2}{2}
\rput[br](1.9, 2.1){$P$}
\rput[tl](2.1, 1.9){$(2,2)$}
\psXTick{2}
\psFullDot{1.5}{1.125}
\rput[br](1.4, 1.225){$Q$}
\rput[tl](1.5, 1.025){$\left(1.5, 1.125\right)$}
\psXTick{1.5}
%Function formula: 7/4 x-3/2 
\psplot[linecolor=\psColorTangent, plotpoints=1000]{0.571428571}{3.1}{-1.5 x 1.75 mul add }
\end{pspicture} 
\psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1, -5)(3,5) 
\psframe*[linecolor=white](-1,-5)(3,5) 
\tiny 
\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3.1,4.805)
%Function formula: 1/2 ((x)^{2}) 
\rput(1,3){$y= \frac{x^2}2$} 
\psplot[linecolor=\psColorGraph, plotpoints=1000]{-1}{3.1}{x 2 exp 0.5 mul }
\psFullDot{2}{2}
\rput[br](1.9, 2.1){$P$}
\rput[tl](2.1, 1.9){$(2,2)$}
\psXTick{2}
\psFullDot{1.9}{1.805}
\rput[br](1.8, 1.705){$Q$}
\rput[tl](1.7, 1.505){$\left(1.5, 1.125\right)$}
\psXTick{1.9}
%Function formula: 39/20 x-19/10 
\psplot[linecolor=\psColorTangent, plotpoints=1000]{0.717948718}{3.1}{-1.9 x 1.95 mul add }
\end{pspicture} 

\noindent \psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1, -5)(3,5) 
\psframe*[linecolor=white](-1,-5)(3,5) 
\tiny 
\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3.1,4.805)
%Function formula: 1/2 ((x)^{2}) 
\rput(1,3){$y= \frac{x^2}2$} 
\psplot[linecolor=\psColorGraph, plotpoints=1000]{-1}{3.1}{x 2 exp 0.5 mul }
\psFullDot{2}{2}
\rput[br](1.9, 2.1){$P$}
\rput[tl](2.1, 1.9){$(2,2)$}
\psXTick{2}
\psline[linecolor=\psColorTangent](0.75,-0.5)(3,4)
\end{pspicture} 

\noindent \psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1, -5)(3,5) 
\psframe*[linecolor=white](-1,-5)(3,5) 
\tiny 
\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3.1,4.805)
\psFullDot{2}{2}
\rput[r](1.9, 2){$(2,2)$}
\rput[tl](2.1, 1.9){$P$}
\psXTick{2}
\psFullDot{2.1}{2.205}
\rput[br](2, 2.305){$(2.1,2.205)$}
\rput[tl](2.2, 2.205){$Q$}
\psXTick{2.1}
\rput(1,4){$y= \frac{x^2}2$} 
%Function formula: 41/20 x-21/10 
\psplot[linecolor=\psColorTangent, plotpoints=1000]{0.780488}{3.1}{-2.1 x 2.05 mul add }
%Function formula: 1/2 x^{2} 
\psplot[linecolor=\psColorGraph, plotpoints=1000]{-1}{3.1}{x 2 exp 0.5 mul }
\end{pspicture} 
\psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1, -5)(3,5) 
\psframe*[linecolor=white](-1,-5)(3,5) 
\tiny 
\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3.1,4.805)
\psFullDot{2}{2}
\psFullDot{2}{2}
\rput[r](1.9, 2){$(2,2)$}
\rput[tl](2.1, 1.9){$P$}
\psXTick{2}
\psFullDot{2.5}{3.125}
\rput[br](2.4, 3.225){$(2.5, 3.125)$}
\rput[tl](2.6, 3.025){$Q$}
\psXTick{2.5}
\rput(1,4){$y= \frac{x^2}2$} 
%Function formula: 9/4 x-5/2 
\psplot[linecolor=\psColorTangent, plotpoints=1000]{0.888889}{3.1}{-2.5 x 2.25 mul add }
%Function formula: 1/2 x^{2} 
\psplot[linecolor=\psColorGraph, plotpoints=1000]{-1}{3.1}{x 2 exp 0.5 mul }
\end{pspicture} 
\psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1, -5)(3,5) 
\psframe*[linecolor=white](-1,-5)(3,5) 
\tiny 
\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3.1,4.805)
\psFullDot{2}{2}
\rput[r](1.9, 2){$(2,2)$}
\rput[tl](2.1, 1.9){$P$}
\psXTick{2}
\psFullDot{3}{4.5}
\rput[br](2.9, 4.5){$(3, 4.5)$}
\rput[tl](2.95, 4.3){$Q$}
\psXTick{3}
\rput(1,4){$y= \frac{x^2}2$} 
%Function formula: 5/2 x-3 
\psplot[linecolor=\psColorTangent, plotpoints=1000]{1}{3.1}{-3 x 2.5 mul add }
%Function formula: 1/2 x^{2} 
\psplot[linecolor=\psColorGraph, plotpoints=1000]{-1}{3.1}{x 2 exp 0.5 mul }
\end{pspicture} 
}
\end{center}
\caption{A tangent line touches the graph of a function, and is approximated by secant lines \label{figTangentIdeaFor}}
\end{figureFixed}
It is natural to approximate the tangent line using secant lines passing through the point $ P=(2, 2)$ and nearby points $Q=(t,f(t))=(t, \frac{t^2}{2})$ lying on the graph of $f(x)$. The line passing through $P=(2,2) $ and $Q=(t,f(t))$ has slope $m_Q:=\frac{f(t)-2}{t-2}$ and therefore has equation
\[
y-2=m_Q(x-2), \quad\quad \quad\text{where~} m_Q= \left(\frac{f(t)-2}{t-2}\right).
\]
As the equation of the tangent line is $y-2=m(x-2)$, we choose to approximate $m$ by the numbers $m_Q$ as $Q$ gets close to the point $P$. On the other hand, the point $Q=(t,f(t))$ gets closer to $P= (x, f(x))$ as $t$ gets closer to $x$.
In Table \ref{tableTangentIdeaForTable}, we have computed the values of $f(t)$ and $m_Q=\frac{f(t)-2}{t-2}$ for various values of $t$ close to $x=2$. We see that as $t$ gets closer to $2$, $m_Q$ appear to get closer to the number $2$, and indeed, we can say that $m_Q$ ``gets infinitely close to $2$ as $t$ approaches $2$'' and so we can define $m=2$. Then the equation of the equation of the tangent line at $P$ becomes 
\[
y-2=2(x-2)\quad ,
\]
which is exactly the equation of the line plotted in blue in Figure \ref{figTangentIdeaFor}.
\begin{tableFixed}
\begin{center}
\begin{tabular}{c|c|c|c|c}
$x$& $f(x)=\frac{t^2}2$& $t-2$ & $f(t)-f(2)$ & $m_Q=\frac{f(t)-f(2)}{t-2}$ \\\hline
0& 0& -2& -2& 1 \\
1& 0.5& -1& -1.5& 1.5 \\
1.5& 1.125& -0.5& -0.875& 1.75\\
1.9& 1.805& -0.1& -0.195& 1.95\\
1.99& 1.98005& -0.01& -0.01995& 1.995\\
1.999& 1.998& -0.001& -0.0019995& 1.9995\\\hline
2.001& 2.002& 0.001& 0.0020005& 2.0005\\
2.01& 2.02005& 0.01& 0.02005& 2.005\\
2.1& 2.205& 0.1& 0.205& 2.05\\
2.5& 3.125& 0.5& 1.125& 2.25\\
3& 4.5& 1& 2.5& 2.5\\
4& 8& 2& 6& 3 \\
\end{tabular}
\end{center}
\caption{ Values of $f(x)$ and slope of line through $P,Q$. \label{tableTangentIdeaForTable}}
\end{tableFixed}
In order to give a strict definition of tangent, we need to define formally what it means for $m_Q$ to ``get infinitely close'' to the number $2$. The colloquial phrase ``to get infinitely close to,'' corresponds to the mathematical notion of taking limits.
\section{Limits}
\subsection{Definition of limit}\label{secDefLimit}
Before we proceed to the formal definition of limit (Definition \ref{defLimit} below) let us explain limits informally.  We say that $f(x)$ tends to $L$ as $x$ tends to $a$ and we write
\[
\lim_{x\to a} f(x)=L
\]
if, no matter how small positive number $\varepsilon$ we pick, we can always choose a small neighborhood of $a$ so that the distance between $f(x)$ and $L$ is less than $\varepsilon$. Given an $\varepsilon$ and $\delta$, let us draw a rectangle centered at $(a,L)$ with base $ 2\delta$ and height $2\varepsilon$  (Figure \ref{figLimitDef} below). Then $\lim_{x\to a}f(x)=L$ means that, no matter how small the height of the rectangle is, one can select its width so small that when the argument $x$ takes the values given by the rectangle base, the graph of $f(x)$ lies entirely in that rectangle (except possibly the point $(a, f(a))$). 

%We note that for a smaller $\varepsilon$, one would expect in general a smaller neighborhood around $a$\footnote{ but not necessarily: think of constant functions.}.

\begin{figureFixed}
\optionalDisplay{
\begin{pspicture}(-0.5,-0.5)(3,2.5)
\tiny
\psaxes[labels=none, ticks=none]{<->}(0,0)(-0.6,-0.5)(3,2.5)
\psLabels{3}{2.5}
\rput[l](2.8,1.7){\tiny $y=f(x)$}

\psplot[linecolor=red]{-0.5}{3}{x x mul 4 div}
\pscircle*(2,1){0.05}
\pscircle*(2,0){0.05}
\pscircle*(0,1){0.05}
\rput[b](2,0.1){\tiny $a$}
\rput[l](0.1,1){\tiny $L$}
\psline[linestyle=dotted](-0.5,0.8)(3, 0.8)
\psline[linestyle=dotted](-0.5,1.2)(3, 1.2)
\psline[linecolor=blue]{<->}(-0.5,0.8)(-0.5,1.2)
\rput[r](-0.6, 1){ $2\varepsilon$}

\psline[linestyle=dotted](1.84,-0.5)(1.84, 2.5)
\psline[linestyle=dotted](2.16,-0.5)(2.16, 2.5)
\psline[linecolor=blue]{<->}(1.84,2.5)(2.16,2.5)
\rput[t](2, 2.4){ $2\delta$}
\end{pspicture}
}
\caption{\label{figLimitDef} $\lim\limits_{x\to a} f(x)=L$ can be interpreted as being able to select a rectangle centered at $(a, L)$ with sides so that for $x$ near $a$, the entire graph of $y=f(x)$ lies inside that rectangle, except possibly at $a$. }
\end{figureFixed}

Formally, the definition of limit is as follows.
\begin{definition}[the $\varepsilon, \delta$-definition of limit] \label{defLimit}
We say that the limit of $f(x)$ as $x$ tends to $a$ exists and equals $L$ and we write 
\[
\lim_{x\to a} f(x)=L
\]
if for every $\varepsilon>0$ there exists $\delta>0$ such that for all $x$ with $0<|x-a|<\delta$ it follows that  $|f(x)-L|<\varepsilon$.
\end{definition}
\subsection{A note on the limit notation.}
We make an important note about calculus notation: we defined the notation
\[
\lim\limits_{x\to a} f(x)=L
\]
on condition that for every $\varepsilon>0$ there exists a $\delta>0$ with certain properties,  the ``$\varepsilon, \delta$''-definition. In particular, we defined the notation $\lim\limits_{x\to a}f(x)$ only if the limit exists and is equal to some particular number $L$. However, in the rest of this book (as well as most other calculus textbooks), we use the notation
\[
\lim\limits_{x\to a}f(x)
\]
even if there exists no number $L$ that satisfies the requirements of Definition \ref{defLimit}. If that is the case, instead of saying the English language negation of Definition \ref{defLimit}, we simply say that `` $\lim\limits_{x\to a}f(x)$ does not exist''. We ask the reader to accept this slight informality in view of its great convenience and universal acceptance.

We note that English language dictates that the negation of Definition \ref{defLimit} is given as follows. 
\begin{proposition}\label{propLimitDoesntExist}
$\lim\limits_{x\to a}f(x) $ does not exist if, for any number $L$, there exists a number $\varepsilon>0$ such that for all $\delta>0$ (no matter how small), there always exists an $x$ with $0<|x-a|<\delta$ so that $|f(x)-L|>\varepsilon$.
\end{proposition}

\subsection{Definition of neighborhood}
When discussing limits, it is useful to speak of neighborhoods of points. Informally, a neighborhood of a point $a$ is an arbitrary collection of points that  includes all points sufficiently close to $a$ except possibly $a$. Formally, the definition of a neighborhood is given as follows.

We say that a $J$ is a \emph{punctured interval} around $a$ if $J$ is of the form 
\[
J= (a-\varepsilon,a)\cup (a, a+\varepsilon)=\{x| \varepsilon>|x-a|>0\}.
\]
for some positive number $\varepsilon>0$.
\begin{definition}
Let $a$ be a real number. We say that the set $I$ is a \emph{neighborhood of $a$} if it contains a punctured interval around $a$.
\end{definition}
In other words,  $I$  is a neighborhood of $a$  if $J\subset I$ for some set $J$ of the form $ J= (a-\varepsilon,a)\cup (a, a+\varepsilon)$ for some positive $\varepsilon>0$.

In the language of neighborhoods, we can reformulate the definition of limit as follows.

\begin{definition}[reformulation of Definition \ref{defLimit}]
\[\lim\limits_{x\to a} f(x)=L
\]
if for every $\varepsilon>0$ there exists a neighborhood of $a$ such that for all $x$ in that neighborhood the distance between $f(x)$ and $L$ is less than $\varepsilon$.
\end{definition}
\subsection{Examples of limits using the definition}
\begin{example}
Compute the limit 
\[\lim\limits_{x\to 1}\frac{x-1}{x^2-1}\]
\end{example}
\begin{solution}
\psset{xunit=1cm,yunit=1cm}
\begin{pspicture}(-0.5,-0.5)(2,2.5)
\psframe*[linecolor=white](-0.5, -0.5)(2, 2.5)
\tiny
\psaxes[Dy=0.5, labels=none]{<->}(0,0)(-0.5,-0.5)(2,2.5)
\psYTickWithLabel{0.5}{$0.5$}
\psXTickWithLabel{1}{$1$}
\psplot[linecolor=red]{-0.5}{2}{1 1 x add div}
\psHollowDot{1}{0.5}
\end{pspicture}

\end{solution}
\begin{example}
Compute the limit 
\[\lim\limits_{x\to 0}\frac{\sin x}{x}\]
\end{example}
\begin{solution}
\psset{xunit=1cm,yunit=1cm}
\begin{pspicture}(-2,-0.5)(2.2,1.5)
\psframe*[linecolor=white](-2, -0.5)(2.2, 1.7)
\tiny
\psaxesStandard{-2}{-0.5}{2}{1.5}
\rput[br](-0.1,1.1){ $1$}
\psplot[linecolor=red]{-2}{2}{ x 57.295779513 mul sin x div}
\psHollowDot{0}{1}
\end{pspicture}

\end{solution}
\begin{example}
Show the limit 
\[
\lim\limits_{x\to 0}\sin \frac{\pi}{x}
\]
does not exist.
\end{example}
\begin{solution}
\begin{pspicture}(-3,-1.5)(3,1.5)
\psframe*[linecolor=white](-3, -1.5)(3.2, 1.7)
\psaxesStandard{-3}{-1.5}{3}{1.5}
\tiny
\rput[t](1,-0.1){ $1$}
\psplot[linewidth=0.3pt, linecolor=red, plotpoints=10000]{-3}{-0.01}{3.14159 x div 57.295779513 mul sin}
\psplot[linewidth=0.1pt, linecolor=red, plotpoints=10000]{0.01}{3}{3.14159 x div 57.295779513 mul sin}
\end{pspicture}

\end{solution}
\section{Continuity}
%In this section we define continuous functions. 
A function $f$ from a subset of the real numbers to the real numbers is continuous if, roughly speaking, its graph is a single unbroken curve with no "holes" or "jumps". %The formal definition follows. 

%Let  be a function from a subset of the real numbers to the real numbers.
\begin{definition}
We say that $f$ is continuous at $a$ if 
\begin{enumerate}
\item $f$ is defined at $a$.
\item $\lim\limits_{x\to a} f(x)=f(a)$.
\end{enumerate}
\end{definition}

\section{Computing limits using algebra}
In the present section, we learn algebraic techniques to evaluate limits quickly. We will start by computing a couple of basic ``building block'' limits by directly using the Definition \ref{defLimit}. Then we will proceed by showing how these building block limits can be used to compute more complex ones using a number of ``limit laws''. We will summarize the acquired knowledge by giving a procedure for computing limits.
\subsection{Limit laws}
Let us compute the limit
\[
\lim_{x\to 2} x\quad .
\]
We start by interpreting the limit geometrically. The function $f(x)=x$ is a straight line passing through the origin and $f(2)=2$. No matter how small $\varepsilon$ is chosen, if one chooses $\delta=\varepsilon$, then the graph of $f(x)=x$ locally ``fits'' (see Figure \ref{figContinuityID}) in a square with height $2\varepsilon$, and width $2\delta$ centered at $(2,2)$, and so $\lim\limits_{x\to 2} x=2$.

\begin{figureFixed}
\optionalDisplay{
\psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1.000000, -5)(3.500000,5) 
\psframe*[linecolor=white](-1.000000,-5)(3.500000,5) 
\tiny 
\psaxesStandard{-0.500000}{-0.5}{3.000000}{3} %Function formula: x 
\rput(1,1.5){$y=x$} 
\psline[linecolor=\psColorGraph](-0.5, -0.5)(3,3)
\psFullDot{2}{2}
\psXTickWithLabel{2}{$2$}
\psYTickWithLabel{2}{$2$}

\psline[linestyle=dotted](-0.5,2.2)(3, 2.2)
\psline[linestyle=dotted](-0.5,1.8)(3, 1.8)
\psline[linecolor=blue]{<->}(1.5,1.8)(1.5,2.2)
\rput[r](1.4, 2){ $2\varepsilon$}

\psline[linestyle=dotted](1.8,-0.5)(1.8, 3)
\psline[linestyle=dotted](2.2,-0.5)(2.2, 3)
\psline[linecolor=blue]{<->}(1.8,2.5)(2.2,2.5)
\rput[b](2, 2.6){ $2\delta$}

\end{pspicture} 
}
\caption{\label{figContinuityID} The continuity of the identity function $f(x)=x$.}
\end{figureFixed}

Let us now show that $\lim\limits_{x\to 2}x=2 $ algebraically, using the definition of limit (Definition \ref{defLimit}). Indeed, this is straightforward: for every $\varepsilon>0$, we can select $\delta=\varepsilon$ so that whenever $|x-2|<\delta$, we have $|f(x)-2|=|x-2|<\varepsilon$ (this is obvious as $\delta=\varepsilon$). 

In the previous example, there was nothing special about the choice $x=2 $: the above considerations would have worked equally well  $x=a$ for any other number $a$, and so we have shown that $\lim\limits_{x\to a}x=a $ for all $a$.

We leave it to the reader to reason that for an arbitrary constant $c $, we have that $\lim \limits_{x\to c}=c$. Geometrically this is clear from the fact that the graph of $f(x)=c$ is a straight line; we leave the very easy algebraic consideration to the reader. We summarize our observations in the following theorem.
\begin{theorem}The following basic ``building block'' limit laws hold.
\begin{enumerate}
\item $\lim\limits_{x\to a} x=a$.
\item $\lim\limits_{x\to a} c= c$, where $c$ is an arbitrary constant. 
\end{enumerate}
\end{theorem}
In the next theorem, we show how to compose the above basic ``building block'' limits into more complicated limits. 
\begin{theorem} Let $f$ and $g$ be functions defined in punctured neighborhoods of $a$ and suppose both limits $\lim\limits_{x\to a} f(x) $ and $\lim\limits_{x\to a} g(x) $ exist. Then the limits on the right hand side exist and the equalities hold.
\[
\begin{array}{lcll}
\displaystyle \lim\limits_{x\to a} c f(x) \pm \lim\limits_{x\to a}d g(x)&=& \displaystyle \lim\limits_{x\to a}c f(x)\pm d g(x) &  c,d \text{ are arbitrary constants}\\~\\
\displaystyle \lim\limits_{x\to a} f(x)\lim\limits_{x\to a}  g(x) &=&\displaystyle  \lim \limits_{x\to a} f(x)g(x)\\~\\
\displaystyle \frac{\lim\limits_{x\to  } f(x)} {\lim\limits_{x\to a}g(x)} &=&\displaystyle  \lim \limits_{x\to a} \frac{f(x)}{g(x)} &\text{ provided }\displaystyle \lim\limits_{x\to a} g(x)\neq 0 \quad .
\end{array}
\]
\end{theorem}
\begin{proofOptional}~We will only prove the limit law $\displaystyle \lim\limits_{x\to a} f(x)\lim\limits_{x\to a}  g(x)= \lim\limits_{x\to a} f(x) g(x)$; the remaining limit laws are left as an exercise to the reader.

Let $\lim\limits_{x\to a}f(x)=L$ and $\lim\limits_{x\to a}g(x)=M$; we aim to show that $\lim\limits_{x\to a} f(x)g(x) $ exists and is equal to $M L$. In order to do that, we must estimate the difference $ |L M - f(x)g(x)|=|L M - (f(x)-L+L)g(x)|$. We can compute that 

\begin{equation}\label{eqlimftimesg}
\begin{array}{rcl}
|L M - f(x)g(x)|&=&|L M - (f(x)-L+L)(g(x))|\\&=& |LM-L g(x) - (f(x)-L)g(x)  |\\& \stackrel{\Delta-\text{ineq}}{\leq}& |L||M-g(x)|+|(f(x)-L)g(x)| \\
&=& |L||M-g(x)|+|(f(x)-L)(g(x)-M+M)|\\
&\stackrel{\Delta-\text{ineq}}{\leq}& |L||M-g(x)|+ |f(x)-L||g(x)-M|
\\&&+ |f(x)-L||M| 
\end{array}
\end{equation}

Informally, we need to show that for any $\varepsilon>0$ we have that $|LM-f(x)g(x)|<\varepsilon$ for all $x$ sufficiently close to $a$. Consider the inequality \eqref{eqlimftimesg}. As $\lim\limits_{x\to a} g(x)=M$ and $\lim\limits_{x\to a}f(x)=L $, we know that the terms $|M-g(x)|$ and $|f(x)-L|$ become arbitrarily small for all $x$ sufficiently close to $a$. Then all three summands on the right hand side of \eqref{eqlimftimesg} become arbitrarily small, and so $|LM-f(x)g(x)|<\varepsilon$ for all $x$ sufficiently close to $a$.

The above considerations give an informal proof of the theorem; let us now cast them into a formal proof. Let $\varepsilon>0$. As $\lim\limits_{x\to a}f(x)=L$ there exists a number $\delta_1>0$ such that $|f(x)-L| |M|<\frac{\varepsilon}3$ for all $x$ for which $|x-a|<\delta_1$. Similarly as $\lim\limits_{x\to a}g(x)=M$ there exists a number $\delta_2$ such that $|L||g(x)-M| <\frac{\varepsilon}3$. Finally, there exists a number $\delta_3$ such that $|f(x)-L|<1 $ for all  $x$ for which $|x-a|<\delta_3$. Select $\delta $ to be the minimum of $\delta_1, \delta_2, \delta_3$. Then  for all $x$ for which $|x-a|<\delta$, we have that 
\[
\begin{array}{rcl}
\displaystyle |L M - f(x)g(x)|&\leq&\displaystyle |L||M-g(x)|+ |f(x)-L||g(x)-M|+ |f(x)-L||M| \\~\\&\leq&\displaystyle \frac{\varepsilon}{3}+\frac{\varepsilon}{3}+\frac{\varepsilon}{3}=\varepsilon\quad .
\end{array}
\]
So far, for any $\varepsilon>0$ , we showed how to select a $\delta>0$ such that $|LM-f(x)g(x)|<\varepsilon$ for all $x$ such that $|x-a|<\delta$, which is precisely what we were aiming to show.
\end{proofOptional}
 

\subsection{Examples of limits computations with algebra}



\subsection{Algorithm for computing certain limits}


\section{Derivatives}

\subsection{Definition of derivative}\label{secDerivative}
\begin{definition}
We say that a function $f(x)$ is differentiable at a point $a$ if the limit 
\begin{equation}\label{eqDefDerivative}
f'(a)\eqdef \lim_{h\to 0} \frac{f(a+h)-f(a)}{h}
\end{equation}
exists. If the limit exists, we say that it equals the derivative of $f$ at the point $a$.
\end{definition}

\subsection{Definition of non-vertical tangent}
\label{secDefTangent}

\subsection{Computing derivatives}
Recall that for a function of a variable $z$, $\frac{d}{dx}(f(x))$ denotes the derivative of $f(x)$. The notations $\frac{d}{dx}(f(x))$ and $f'(x)$ are equivalent. 

Let $n$ be an integer. 
\[
\frac{d}{dx} (x^n)= n x^{n-1}\quad .
\]
\[
\frac{d}{dx} (f(x)g(x)) = f'(x)g(x)+f(x)g'(x) \quad .
\]
Let $f(y)$ and $g(x)$ be differentiable functions.  Then
\[
\frac{d}{dx} \left(f( g(x))\right)= g'(x) f'(g(x)) \quad .
\]




\section{Differentials}
Let $f$ be a differentiable function of a variable $x$. Introduce formal expressions $\diff(f(x))$ (abbreviated as $\diff f$) and $\diff x$ so that they obey the following rule.
\importantFormula{
f'(x)\diff x\eqdef \diff(f(x))\quad \quad,
}
in abbreviated notation 
\importantFormula{
f'\diff x = \diff f\quad \quad .
}

$\diff (f(x))$ is called the \emph{differential} of $f$, and expressions of the form $g(x)dx$ are called \emph{differential forms}.

Differentials obey the laws
\[
\begin{array}{rcll}
d(f+C)&=&df \quad \quad &\mathrm{if~} C \mathrm{~is~constant}\\
d(Cf)&=&Cdf \quad \quad &\mathrm{if~} C \mathrm{~is~constant}\\
d(f+g)&=&df+dg \quad\quad\quad\quad \quad &(\mathrm{linearity})\\
d(fg)&=&gdf+fdg \quad\quad\quad\quad \quad &(\mathrm{Leibniz~rule})\\
\end{array}
\]
%Try to prove these laws from the definition!
We recall the notation $\displaystyle \int f(x)\diff x$ stands for the class of anti-derivatives of $f(x)$, i.e., for the functions $F(x)$ for which $F'(x) = f(x)$.

The fact that $f(x)+C=\displaystyle \int f'(x)dx$ is translated into differential notation as 
\importantFormula{
\displaystyle \diff \left(\int f(x)dx\right)= f(x)dx\quad ,
}
and
\importantFormula{
\int \diff f(x)= f(x)+C\quad .
}



\chapter{Exponents, complex numbers and trigonometric functions}

\section{Exponent function, $\sin$ and $\cos$}
\subsection{Exponent function: integer vs non-integer exponent}
We recall that when $n$ is an integer,  $a^{n}$ denotes the expression $\underbrace{a*\dots *a}_{n\text{ times}}$. 


\subsection{Definition of $e^x$}
Consider the infinite sum
\importantFormula{\label{eqExponentFunctionDefinition}
\begin{array}{rcl}
\displaystyle e^{z}&\eqdef &\displaystyle \sum_{n=0}^{\infty} \frac{z^n}{n!} = 1+z+\frac{z^{2}}{2}+\frac{z^3}{6}+\frac{z^4}{24}+\frac{z^5}{120}+\dots+\frac{z^n}{n!}+\dots\\
\end{array} \quad .
}
We immediately justify the above notation with the following.
\begin{definition}\label{defNaturalExponent}
\index{exponent function} The natural exponent function $e^z$ is the function defined via equation \eqref{eqExponentFunctionDefinition}.
\end{definition}
The expression \eqref{eqExponentFunctionDefinition} for $e^z$ gives indeed a formula for the natural exponent, i.e., the exponent with base $e= e^1= 1+ \frac{1}{1} +\frac{1}2 +\frac{1}{3!} +\frac{1}{4!}+\dots \approx 2.71828$. We will learn why that is so in later in this section. Until we do so we ask the reader to think of $e^{z}$ as an infinite sum, which will only later to be the familiar exponent which was studied\footnote{perhaps less formally} in earlier mathematical courses. We start by explaining how formula \eqref{eqExponentFunctionDefinition} is evaluated. Let 
\[
\displaystyle f(z,N):=\sum\limits_{n=0}^{N} \frac{z^n}{n!}
\]
Then \eqref{eqExponentFunctionDefinition} becomes 
\[
e^z\eqdef \displaystyle\lim\limits_{N\to \infty} f(z, N)\quad .
\]
The above equality tells us how to find numerical approximations for $1+z+\frac{z^{2}}{2}+\frac{z^3}{6}+\frac{z^4}{24}+\frac{z^5}{120}+\dots+\frac{z^n}{n!}+\dots$. More precisely, pick a large number $N$, evaluate the sum of the first $N$ summands, discard the remaining ones, and declare the so obtained number an approximation of \eqref{eqExponentFunctionDefinition}. Up to the author's knowledge, this is in fact how computers evaluate the expression $e^x$. The question of how to pick a sufficiently large $N$ so that the error is sufficiently small will be handled in Section \ref{secSeriesConvergence}. 

Equation \eqref{eqExponentFunctionDefinition} raises an immediate important question: does the limit used in the definition exist? The answer is affirmative.
\begin{theorem} The infinite sum \eqref{eqExponentFunctionDefinition} is well-defined, i.e. the limit $\displaystyle\lim_{N\to \infty} f(z, N)$ exists for all $z$.
\end{theorem}
We postpone the proof of this theorem to Section \ref{secSeriesConvergence}. 

It is one of the basic rules of arithmetics is that $a^n a^{m}= a^{n+m} $. We defined $e^x$ via the infinite sum \eqref{eqExponentFunctionDefinition}, rather than using the rules of arithmetics. It is then not clear why, if at all, the infinite sum 
\[
e^{z+w} =\left(\sum\limits_{n=0}^{\infty} \frac{(z+w)^n}{n!}\right)
\]
should be equal to the product of infinite sums
\[
e^{z} e^w=\left(\sum\limits_{n=0}^{\infty} \frac{z^n}{n!}\right)\left(\sum\limits_{n=0}^{\infty} \frac{w}{n!} \right) .
\]
The equality $e^{z+w}= e^{z}e^{w}$ does indeed hold, as we show in the the following Theorem \ref{thExponentArgumentsAddWhenMultiplying}. In other words, exponents add when we multiply exponent functions. In this way Theorem \ref{thExponentArgumentsAddWhenMultiplying} shows that when $n$ is an integer, 
\begin{equation}\label{eqExponentToIntegerPowerRespectsArithmetics}
e^{n z}= e^{\underbrace{z+\dots +z}_{n\text{ times}} }=\underbrace{ e^{z}*\dots e^{z}}_{n \text{ times}}  \quad .
\end{equation}
The above equality in turn justifies our original claim that the definition of  $e^{z}$ using an infinite sum \eqref{eqExponentFunctionDefinition} is consistent with the rules of arithmetics.
\begin{theorem}\label{thExponentArgumentsAddWhenMultiplying}
Recall that $e^x$ is defined via \eqref{eqExponentFunctionDefinition}. Then we have that \importantFormula{
e^{z} e^{w}=e^{z+w}\quad ,
}
\end{theorem}
\begin{proofOptional}
\begin{equation*}
\begin{array}{rcl}
\displaystyle
\displaystyle e^{z} e^w&=&\displaystyle  \sum_{n=0}^{\infty} \frac{z^n}{n!} \sum_{m=0}^{\infty} \frac{w^m}{m!} \\ &\eqAttention&  \displaystyle  \sum_{s=0}^{\infty}\sum_{k=0 }^s \frac{z^{k}w^{s-k}}{k! (s-k)!} \\ &=& \displaystyle  \sum_{s=0}^{\infty}\sum_{k=0 }^s   \frac{z^{k}w^{s-k}}{s!} \frac{s!}{k! (s-k)!}\\ &\stackrel{\mathrm{Newton~binomial}}{=}& \displaystyle  \sum_{s=0}^{\infty} \frac{(z+w)^s}{s!}=e^{z+w}\quad .
\end{array}
\end{equation*}
\end{proofOptional}
\begin{theorem}
\[
e^0=1\quad .
\]
\end{theorem}
\begin{proofOptional}
\[ e^0 = 1+ 0+ \frac{0^2}{2}+ \frac{0^3}{6}+\dots +\frac{0^n}{n!}+\dots = 1\quad .
\]
\end{proofOptional}

\begin{theorem}
$\frac{1}{e^{x}}= e^{-x}$\quad .
\end{theorem}
\begin{proof}
\[e^{-x}e^x=e^0 = 1\quad .
\]
Therefore $e^{-x}=\frac{1}{e^x}.$
\end{proof}

\optionalMaterial We end this section by stating an important alternative to \ref{eqExponentFunctionDefinition} for expressing $e^z$ as a limit.   
\begin{theorem}\label{th(1+x/n)^n=e^x}
We have that
\begin{equation}\label{eq(1+x/n)^n=e^x}
\lim_{n\to \infty} \left(1+\frac{z}n\right)^n= e^z\quad .
\end{equation}
The limit holds for arbitrary $z\in \mathbb C$.
\end{theorem}
The limit \eqref{eq(1+x/n)^n=e^x} can be used as an alternative definition of $e^{z}$, but that results in a slightly longer exposition of the theory.  

The limit \eqref{eq(1+x/n)^n=e^x} has important theoretical and practical applications. For example, if a compound interest of $ k\%$ is accumulated over $100/k $ compounding periods, the accumulation of the sum of money is 
\[
(1+\frac{k}{100})^{\frac{100}{k}}, 
\]
which, if $k$ is small, is an approximation for $e$.

For $z\in \mathbb R$, we will be in position to prove Theorem \ref{th(1+x/n)^n=e^x} in Section \ref{secLHospitalRevisited}.
\subsection{Definition of $\sin x$ and $\cos x$}
\index{sine function} \index{cosine function} 

\begin{definition}
The functions $\sin z$ and $\cos z$ are defined as
\importantFormula{
\sin z\eqdef \frac{e^{iz}-e^{-iz}}{2i} \quad ,
}
\importantFormula{
\cos z\eqdef \frac{e^{iz}+e^{-iz}}{2}\quad ,
}
where we recall that $i$ stands for the imaginary unit ($i^2=-1$).
\end{definition} 
\begin{theorem}\label{thSinCosMaclaurinSeries} We have the following equalities
\begin{equation}\label{eqSinCosMaclaurinSeries}
\begin{array}{rcl}
\cos x= &=&1-\frac{x^2}{2}+ \frac{x^4}{4!}- \frac{x^6}{6!} +\dots - \frac{x^{4n+2}}{(4n+2)!}+ \frac{x^{4n+4}}{ (4n+4)!} +\dots\\
&=& \displaystyle \sum_{n=0}^\infty (-1)^n \frac{x^{2n} }{ (2n)!} \\
\sin x& =& x-\frac{x^3}{3!}+ \frac{x^5}{5!}- \frac{x^7}{7!} +\dots + \frac{x^{4n+1}}{(4n+1 )!}- \frac{x^{4n+3}}{ (4n+3)!} +\dots\\
&=&\displaystyle \sum_{n=0}^{\infty}(-1)^n \frac{x^{2n+1}}{ (2n+1)!} \quad .
\end{array}
\end{equation}
\end{theorem}
\begin{proofOptional}
\[
\begin{array}{rcl}
\displaystyle\cos(x)&=& \displaystyle \frac{e^{ix}+e^{-ix}}{2}= \frac 12 \left(\sum_{i=0}^\infty (ix)^{n}+ \sum_{i=0}^\infty (-ix)^{n}\right)\\
&=& \phantom{+} \frac12\left( 1+ \cancel{ix} + \frac{(ix)^2}2 +\cancel{\frac{(ix)^3}6}+\dots + \frac{(ix)^{2n}}{(2n)!}+ \cancel{\frac{(ix)^{2n+1}}{(2n+1)!}} +\dots \right)
\\
&&+  \frac12\left( 1-\cancel{ix} + \frac{(-ix)^2}2 +\cancel{\frac{(-ix)^3}6}+\dots + \frac{(-ix)^{2n}}{(2n)!}+ \cancel{\frac{(-ix)^{2n+1}}{(2n+1)!}}+\dots \right)\\
&=& 1+\underbrace{i^2}_{=-1}\frac{x^{2}}2+\underbrace{i^4}_{=1}\frac{x^{4}}{4!}+\dots\\
&=&1-\frac{x^2}{2}+\frac{x^4}{4!}-\frac{x^6}{6!}+\dots -\frac{x^{4n+2}}{(4n+2)!}+ \frac{x^{4n+4}}{(4n+4)!}+\dots\\
&=&\displaystyle \sum_{n=0}^\infty (-1)^n\frac{x^{2n}}{(2n)!}
\end{array}
\]


\[
\begin{array}{rcl}
\displaystyle\sin(x)&=& \frac{e^{ix}-e^{-ix}}{2i}=\displaystyle \frac {-i}{2} \left(\sum_{i=0}^\infty (ix)^{n}- \sum_{i=0}^\infty (-ix)^{n}\right)\\
&=&-\frac{i}{2}\left(\cancel{1} + ix + \cancel{\frac{(ix)^2}2} +{\frac{(ix)^3}6}+\dots + \cancel{\frac{(ix)^{2n}}{(2n)!}}+ {\frac{(ix)^{2n+1}}{(2n+1)!}} +\dots \right)\\
&&+  \frac{i}{2}\left(\cancel{1} -{ix} + \cancel{\frac{(-ix)^2}2} +{\frac{(-ix)^3}6}+\dots + \cancel{\frac{(-ix)^{2n}}{(2n)!}}+ {\frac{(-ix)^{2n+1}}{(2n+1)!}}+\dots \right)\\
&=&- \underbrace{i^2}_{=-1}x-\underbrace{i^4}_{=1}\frac{x^3}{3!}-\underbrace{i^{6}}_{=-1}\frac{x^5}{5!} -\dots\\
&=&x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+\dots +\frac{x^{4n+1}}{(4n+1)!}- \frac{x^{4n+3}}{(4n+3)!}+\dots\\
&=&\displaystyle \sum_{n=0}^{\infty}(-1)^n\frac{x^{2n+1}}{(2n+1)!}\quad .
\end{array}
\]
\end{proofOptional}
\begin{corollary}\label{corSinCosOfRealIsReal}
Let $x $ be a real number. Then $\sin x$ and $\cos x$ are real numbers.
\end{corollary}
\begin{proof}
We look at the power series for $\sin x$ and $\cos x$ given by \eqref{eqSinCosMaclaurinSeries} and see that $i$ does not participate in either expression. Therefore substituting real numbers in the expressions for $\sin x$ and $\cos x$ given by \eqref{eqSinCosMaclaurinSeries} yields a real number.
\end{proof}

\begin{theorem}
\importantFormula{\label{eqsinSquaredPlusCosSquared}
\sin^2 z +\cos^2 z= 1\quad .
}

\end{theorem}
\begin{proofOptional}
\begin{equation*}
\begin{array}{rcl}
\sin^2 z +\cos^2 z&=& \frac{(e^{iz}+e^{-iz})^2}{2^2}+\frac{(e^{iz}+e^{-iz})^2}{(2i)^2}=\\
&=&\frac{ \cancel{(e^{iz})^2} + 2 e^{iz}e^{-iz} + \cancel{(e^{-iz})^2}}{4} \\
&&-\frac{(\cancel{(e^{iz})^2}-2e^{iz}e^{-iz} + \cancel{(e^{-iz})^2})}{4} \\&=& e^{iz}e^{-iz}= e^{iz-iz}=e^0=1\quad .
\end{array}
\end{equation*}
\end{proofOptional}
\begin{corollary}
If $x\in \mathbb R$ then $|\sin x|\leq 1$ and $|\cos x| \leq 1$.
\end{corollary}
\begin{proof}
By  \eqref{eqsinSquaredPlusCosSquared} $\sin^2 x+\cos ^2 x=1$. Since $x$ is a real number, so are $\sin x$ and $\cos x$ (Corollary \ref{corSinCosOfRealIsReal}) and therefore $\sin^2x\leq 1$ and $\cos^2 x$ are positive real numbers. Therefore  $|\sin x|^2\leq 1$ and $|\cos x|^2\leq 1$ and therefore $|\sin x|\leq 1$ and $|\cos x|\leq 1$.
\end{proof}
\subsection{Euler's formula}
\begin{theorem}[Euler's formula]
We have the following equality.
\importantFormula{\label{eqEulerExp}
e^{iz}= \cos z+ i\sin z \quad .
}
\end{theorem}
\begin{proofOptional}
\[
\begin{array}{rcl}
\displaystyle e^{iz}&=&\displaystyle  \frac{2}{2}e^{iz}= \frac{2e^{iz} + e^{-iz} - e^{-iz}}{2} = \frac{e^{iz}+e^{-iz}}2 +\frac{e^{iz}-e^{-iz}}2\\
&=& \cos z + \frac{i(e^{iz}-e^{-iz})}{2i}\\
&=& \displaystyle \cos z+i \sin z\quad .
\end{array}
\]

\end{proofOptional}

\begin{theorem}\label{thSinCosAngleSum}
\[
\sin(x+ y)=\sin x\cos y +\sin y\cos x  
\]
\[
\cos (x+y)= \cos x \cos y - \sin x \sin y
\]
\end{theorem}
Before we proceed with the proof, we show a quick way to remember the above formulas. Suppose $x,y\in \mathbb R$. Then
\[
\begin{array}{rcl}\cos (x+y) + i\sin (x+y)&\stackrel{\mathrm{Euler~formula~}\eqref{eqEulerExp}}{=}& e^{i(x+y)} =e^{ix}e^{iy}\\&=& (\cos x +i \sin y)(\cos y + i \sin y)\\&=& \cos x \cos y - \sin x \sin y\\&& +i(\sin x \cos y+ \sin y \cos x) \quad .\end{array}
\]
Now since both $x,y$ are real the only way to satisfy the above equality is to have the formulas $\sin(x+ y)=\sin x\cos y +\sin y\cos x  $ and $\cos (x+y)=\cos x \cos y - \sin x \sin y$. However, this argumentation works only for $x,y\in \mathbb R$. 

The considerations above give a proof of Theorem \ref{thSinCosAngleSum} when $x, y$ are real. To give a proof for arbitrary complex numbers one has to do a slightly longer computation. 

\begin{proofOptional}[ of Theorem \ref{thSinCosAngleSum}]

\[
\begin{array}{rcl}
\cos x \cos y - \sin x \sin y&=& \frac{(e^{ix}+e^{-ix})(e^{iy}+e^{-iy})}4 -\frac{(e^{ix}-e^{-ix})(e^{iy}-e^{-iy}) }{4i^2}\\
&=& \frac{ e^{i(x+y)}+ \cancel{e^{i(y-x)}}+{e^{-i(x+y)}} + \cancel{e^{i(x-y)}} +e^{i(x+y)}+e^{-i(x+y)}- \cancel{e^{i(x-y)}}-\cancel{e^{i(y-x)}} }4\\
&=& \cos(x+y)\\
\sin x \cos y + \cos x \sin y&=& \frac{(e^{ix}-e^{-ix})(e^{iy}+e^{-iy})}{4i} +\frac{(e^{ix}+e^{-ix})(e^{iy}-e^{-iy}) }{4i}\\
&=& \frac{ e^{i(x+y)}- \cancel{e^{i(y-x)}}-{e^{-i(x+y)}} + \cancel{e^{i(x-y)}} +e^{i(x+y)}-e^{-i(x+y)}- \cancel{e^{i(x-y)}}+\cancel{e^{i(y-x)}} }4\\
&=& \sin(x+y)

\end{array}
\]
\end{proofOptional}




We have that $(e^{iz})^2= e^{2iz}= \cos (2z)+ i\sin(2z)$. On the other hand, $(e^{iz})^2 = (\cos z+i\sin z)^2= \cos^2z+2i\sin z\cos z - \sin^2 z$. Therefore $\cos (2z)+ i\sin(2z)= (\cos^2 z- \sin^2 z)+ i(2\sin z\cos z) $ and therefore we get the formulas 
\begin{equation}\label{eqSin2xCos2x}
\sin (2z)=2\sin z \cos z \quad\quad \quad \cos(2z)=\cos^2 z- \sin^2 z.
\end{equation}
There are two equivalent ways of defining $\sin z$ and $\cos z$: one given here, and one studied in some high schools and in alternative expositions of the calculus course. When using the high school approach, one defines that the functions $\sin \varphi$ and $\cos\varphi$ measure the lengths of the legs of the right angle triangle with another angle $\varphi$ and hypotenuse of length 1, as indicated on figure \ref{figUnitCircleSineCosine}. The angle $\varphi$ is defined as the length of the arc of the unit circle colored in red. 

The fact that $\varphi$ measures the arc drawn with red in the figure \ref{figUnitCircleSineCosine} is to be taken, for the time being, on ``good faith''. This  will be fixed after studying lengths of curves in Section \ref{secCurveLength}.


\begin{figure}[h]\caption{The $\sin\varphi$ and $\cos\varphi$ as legs of a right angle triangle with other angle $\varphi$.}

\optionalDisplay{
\psset{xunit=3cm,yunit=3cm}
\begin{pspicture}(-2,-2)(2,2)
\psline[linecolor=gray]{->}(-1.5,0)(1.5,0) % x-axis
\psline[linecolor=gray]{->}(0,-1.5)(0,1.5) % y-axis
\rput[l](1.5,0){$\Re z$}
\rput[b](0,1.5){$\Im z$}
\rput[bl](0.03,1.03){$i$}
\rput[bl](1,-0.1){$1$}
\rput[c](0,1){$\bullet$}
\rput[c](1,0){$\bullet$}
\psplot{-1}{0.5}{1 x x mul sub sqrt} 

\psplot[linecolor=red]{0.5}{1}{1 x x mul sub sqrt} 

\psplot{-1}{1}{1 x x mul sub sqrt -1 mul}
\rput(0.5, 0){\psplot{-0.1}{0}{ 0.01001 x x mul sub sqrt 1 mul}}
\rput[l](0.53, 0.34){$\sin\varphi$ $=\Im \frac{z}{|z|}$}
\rput[t](0.44, -0.03){$\cos\varphi=\Re \frac{z}{|z|}$}

\psline(0,0)(0.5,0.866)
\rput[c](0.5,0.866){$\bullet$}
\rput(0.47,1){$\frac{z}{|z|}$}
\psline[linestyle=dotted](0.5,0.866)(1,1.74)
\rput(1.1,1.81){$z$}
\rput[c](1,1.74){$\bullet$}

\psline(0.5,0)(0.5,0.866)
\rput[l](0.84,0.65){$\varphi$}
\end{pspicture}
} %end optionalDisplay
\label{figUnitCircleSineCosine}
\end{figure} %
\subsubsection{Exercises}
\begin{problem}
Use the trick used to derive \eqref{eqSin2xCos2x} to derive a formula for $\sin(3z)$ and $\cos (3z)$. Your solution may start like this. ``We have that $(e^{iz})^3=e^{3iz}=\cos (3z)+i\sin(3z)$. On the other hand, $(e^{iz})^3= \dots$''.
\end{problem}
\subsection{Definition of $\pi$, polar form of complex numbers}\label{secPolarFormComplexNumbers}
\label{secDefinitionPi}

The proof of the following theorem is a bit more difficult and we omit it from the current version of this textbook.
\begin{theorem}
There exists a positive real number $x$ such that \index{$\pi$}
\importantFormula{
e^{ix}=i\quad .
}
\end{theorem}

\begin{definition}
Define $\pi$ to be the smallest positive real number for which
\importantFormula{
e^{i\frac{\pi}{2} }\eqdef i\quad .
}
\end{definition}



\begin{theorem} The following equalities hold. 
\begin{equation}\label{eqe^ipi}
\begin{array}{rcl}
\displaystyle e^{i\pi}&=&-1\\
\displaystyle e^{2i\pi}&=&=1\\
\displaystyle \sin \frac{\pi}2 &=& 1\\
\displaystyle \cos \frac{\pi}2 &=& 0\quad .
\end{array}
\end{equation}
\end{theorem}
\begin{proofOptional}
The first two equalities are a direct computation.
\[
\begin{array}{rcl}
\displaystyle e^{i\pi}&=&\displaystyle  e^{i\frac{\pi}2}e^{i\frac{\pi}2}= i^2= -1\\
\displaystyle e^{2i\pi}&=&\displaystyle  e^{\pi i} e^{\pi i}= (-1)^2=1\quad .
\end{array}
\]
For the last two equalities we compute:
\[
0+ 1*i=i=e^{i\frac{\pi}2}= \cos \frac{\pi}2 + i\sin \frac\pi 2\quad .
\]
As $\pi$ is real, by Corollary \ref{corSinCosOfRealIsReal}, $\sin\frac{\pi}{2}$ and $\cos \frac{\pi}{2}$ are real. Therefore we can compare both sides to see that $0=\cos \frac{\pi}{2}$ and $1=\sin \frac \pi 2$.
\end{proofOptional}

The length of the circumference of a circle of radius $r$ is $2\pi r$ and the area of a circle of radius $r$ is $\pi r^2$. We will prove these formulas very soon. 

For $x\in \mathbb R$, graphs of $\sin x$ and $\cos x$ can be found in section \ref{secGraphsTrigFunc}.
\begin{theorem} We have that
\[
\sin \left(x+\frac{\pi}{2}\right)= \cos x \quad .
\]
\end{theorem}

\begin{proof}
\[
\cos\left(x+\frac{\pi}{2}\right)+ i\sin \left(x+\frac{\pi}2\right)=e^{i(x+\frac{\pi}{2})}= e^{ix} \underbrace{e^{ i\frac{\pi}2 }}_{=-1}= ie^{ix}= i(\cos x + i \sin x)= i\cos x- \sin x\quad .
\]
Comparing both sides of the above expression shows that $\cos(x+\frac{\pi}{2})=-\sin x$ and $\sin(x+\frac{\pi}{2})= \cos x$.
\end{proof}
This equality is indicated by the red color in the graphs of $\sin$ and $\cos$ in section \ref{secGraphsTrigFunc}. 

Recall that $\overline{a+ib}=a-ib$  (complex conjugation). Then recall \eqref{eqComplexConjugationIsFieldHMM}:
\[
\overline {z w}= \bar z \bar w\quad .
\]

\[
\overline {z^n}= \overline {\underbrace{z*z*\dots*z}_{n\mathrm{~times}}} = \underbrace{\bar z* \bar z *\dots * \bar z}_{n\mathrm{~times}}= \bar {z}^n
\]

\begin{theorem}
Let $y\in \mathbb R$ (i.e., $y$ is a real number). Then 
\begin{equation}\label{eqexponentiyconjugateisinverse}
\overline {e^{iy}}=e^{-iy}= \frac{1}{e^{iy}}\quad .
\end{equation}
\end{theorem}
\[
\overline {e^{iy}}= \sum_{n=0}^\infty \frac{\overbrace{\bar i}^{=-i} \bar y^n}{n!}= \sum_{n=0}^\infty \frac{(i(-y))^n}{n!}=  e^{-iy}= \frac{1}{e^{iy}}\quad .
\]
\begin{theorem}
Let $y\in \mathbb R$. Then $|e^{iy}|=1$. 
\end{theorem}
\begin{proof}
\[
|e^{iy}|^2 \eqdef e^{iy}e^{-iy}\underbrace{=}_{\eqref{eqexponentiyconjugateisinverse}}1 \quad .
\]
For arbitrary $z$ we have that $|z|$ is a non-negative real number and so it follows that $|e^{iy}|=1$. 
\end{proof}

\begin{proposition}
Let $x\in \mathbb R$. Then $e^x$ is a positive real number.
\end{proposition}

Let $\rho, \varphi\in \mathbb R$ (real numbers). Then the expression
\[
e^{\rho+i\varphi}=\underbrace{e^{\rho}}_{|e^{\rho+i\varphi}|}(\cos \varphi + i\sin \varphi)
\]
is defined to be a polar form of the number $e^{\rho+i\varphi}$. Let $z=a+ib$, where $a, b\in \mathbb R$ (real numbers). Let $w\neq 0$. Suppose $\varphi$ is such a number that
\[
(a+ib)= \underbrace{\sqrt{a^2+b^2}}_{=|z|}(\cos \varphi + i \sin \varphi)\quad. 
\]
Then $\varphi$ is called argument of $a+ib$. We write \[\varphi=\arg z\quad .\] In other words
\[z=\underbrace{|z|}_{\sqrt{\Im(z)^2+\Re(z)^2}}\left(\cos(\arg z) + i\sin (\arg z)\right)\quad .
\]
Look at figure \ref{figUnitCircleSineCosine}. Identify the parts of figure \ref{figUnitCircleSineCosine} corresponding to every term you see in the above formula.
Let $e^{\rho+i\phi}$ and $e^{\tau+i\psi}$ be two complex numbers. Then $e^{\rho+i\phi}e^{\tau+i\psi} = e^{\rho+\tau+i(\phi+\psi)}$. Therefore 
\importantFormula{
\mathrm{Multiplying~complex~numbers~adds~arguments~and~multiplies~absolute~values.}
}

\subsection{$\log z$, $\ln x$, $\tan z$, $\arctan z$, $\arcsin x$, $\arccos x$}
Consider the exponent function $e: \mathbb R \to \mathbb R$. Then the range of the exponent function is the positive real numbers. Define $\ln y$ to be the inverse of $e^x$ in this range. In other words, $\ln(y)$ is defined when $y$ is a positive real number. In mathematical notation, $\ln: (0,+\infty)\to \mathbb R$. Then we have that
\importantFormula{
\ln (e^x)\eqdef x\quad\quad \quad \quad  e^{\ln y}\eqdef y\quad .
}

Let $z$ be a complex number for which $\cos z\neq 0$. 

\importantFormula{
\tan z\eqdef \frac{\sin z}{\cos z}\quad .
}

Let $x$ be a real number. Consider the tangent function acting in the interval $(-\frac{\pi}{2}, \frac{\pi}{2})$, in mathematical notation, $\tan :(-\frac{\pi}{2}, \frac{\pi}{2})\to \mathbb R$. Then the function $\tan(x)$ has an inverse which is called $\arctan(x)$. In other words,
\[
\tan (\arctan(x))= x\quad .
\]

Let $D$ be the set of complex numbers which are neither negative real numbers nor zero. Consider the exponent function $e: D\to \mathbb C$ which maps $z\in D$ to $e^z$. Then define $\log_{(-\pi,\pi)} z$, or $\log z$ for short, to be the inverse of this function, that is,
\importantFormula{
e^{\log z} = z\quad \quad \quad \log (e^w)=w\quad .
}
The imaginary part of $\log_{(-\pi,\pi)}(z)$ is a number in the interval $(-\pi,\pi)$, i.e., 
\[
-\pi<\Im(\log z)<\pi\quad . 
\]

The notation $\ln x$  is used when the argument $x$ is a positive real number. The function $\ln$ is called ``natural logarithm''. The notation $\log z$ is used when the argument $z$ is a complex number. If it so happens that $z$ is a complex number that is also a positive real number, then $\log(z)=\ln(z)$. Therefore the main role of the notation $\log$ and $\ln$ is to remind us of whether we are computing with complex or with real numbers.

In non-mathematical texts, the notation $\log_{10} x\eqdef \frac{\ln x}{\ln 10}$ is sometimes used to denote ``logarithm base 10'' (similar notation can be used for other bases as well). In mathematical texts, the notation $\log_{10} x$ is not accepted and should be avoided. Even when writing papers in the natural sciences, where the use of the notation $\log_{10} x$ is acceptable, I  recommend avoiding this notation.

Let $\varphi\in (-\frac{\pi}{2} ,\frac{\pi}{2})$ (i.e., $\varphi$ is a real number in the interval $(0,\frac{\pi}{2})$). Then 
\begin{equation}\label{eqSinCosViaTan}
\begin{array}{rcl}
\sin \varphi &=& \frac{ \tan \varphi}{\sqrt  {1+\tan^2 \varphi  }}\\
\cos \varphi &=& \frac{1}{\sqrt  {1+\tan^2 \varphi  }}\quad .\\
\end{array}
\end{equation}
\begin{proof}
We compute that
\[\frac{1}{1+\tan^2 \varphi} = \frac{1}{1+\frac{\sin^2 \varphi}{\cos^2\varphi}}= \frac{1}{\frac{\sin^2\varphi+\cos^2\varphi}{\cos^2\varphi}}= \cos^2{\varphi}\quad , 
\]
and as both sides are positive real numbers we can take the square root on both sides to get the second equality in \eqref{eqSinCosViaTan}. The fact that both sides are positive real numbers is important: we have not defined square root for negative real numbers or for arbitrary complex numbers. The remaining equality is more tricky:
\begin{equation}\label{eqSinViaTanTemp}
\sin \varphi = \pm \sqrt{1-\cos^2\varphi}= \pm \sqrt{1-\frac{1}{1+\tan^2\varphi}}= \pm \sqrt{\frac{\tan^2\varphi}{1+\tan^2\varphi}}=
\frac{\pm (\pm \tan\varphi)}{\sqrt{1+\tan^2\varphi}}\quad ,
\end{equation}
where we have used that for an arbitrary real number $t$ we have that $|t|=\sqrt{t^2}=\pm t$. In the interval $\varphi\in (-\frac{\pi}{2} ,\frac{\pi}{2})$, $\sin \varphi$ and $\tan \varphi$ have the same sign. Therefore in \eqref{eqSinViaTanTemp} the expression $\pm(\pm \tan\varphi)$ has the same sign as $\tan \varphi$ and we can omit the $\pm$ signs.
\end{proof}

Let $z$ be a complex number such that $\Re z>0$. Then
\importantFormula{
\log z = \ln |z| + i\arctan \left(\frac{\Im z}{\Re z}\right)\quad .
}
\begin{proofOptional} 
~We aim to prove that $e^{\log z}=z$. Set 
\[
\arctan \left(\frac{\Im z}{\Re z}\right)\quad .
\]
By definition $\varphi\in  (-\frac{\pi}{2}, \frac{\pi}{2})$ and therefore we may apply \eqref{eqSinCosViaTan} as follows.
\[
e^{\ln |z| + i\varphi}= e^{\ln |z|} (\cos\varphi+i\sin\varphi )=|z|(\cos\varphi+i\sin\varphi ) \stackrel{\eqref{eqSinCosViaTan}}{=} 
|z| \left(\frac{1}{\sqrt{1+\tan^2\varphi}} +i \frac{\tan \varphi}{\sqrt{1+\tan^2\varphi}} \right)\quad .
\]
On the other hand, as $\varphi\in  (-\frac{\pi}{2}, \frac{\pi}{2})$, we have that $\tan\varphi= \tan\left(\arctan\left(\frac{\Im z}{\Re z}\right)\right)= \frac{\Im z}{\Re z}$. Therefore
\[
\begin{array}{rcl}
\displaystyle e^{\ln |z| + i\varphi}&=&\displaystyle \underbrace{|z|}_{\sqrt{(\Re z)^2+(\Im z)^2}} \left(\frac{1}{\sqrt{1+\frac{(\Im z)^2}{(\Re z)^2}}} +i \frac{\frac{\Im z}{\Re z}}{\sqrt{1+\frac{(\Im z)^2}{(\Re z)^2}}} \right) \\
&=&\displaystyle \cancel{\sqrt{(\Re z)^2+(\Im z)^2}} \left(\frac{|\Re z|}{\cancel{\sqrt{(\Re z)^2+(\Im z)^2}}} +i \frac{|\Re z|\frac{\Im z}{\Re z}}{\cancel{\sqrt{(\Re z)^2+(\Im z)^2}}} \right) \\
&=&\displaystyle|\Re z | + i|\Re z| \frac{\Im z}{\Re z}\quad .
\end{array}
\]
Caution to the last equality: we cannot cancel $|\Re z| $ with $\Re z$ if the two expressions have different sign. However, we requested in the start that $z$ was such a number that $\Re z>0$, and therefore $|\Re z|=\Re z $. Therefore 
\[
e^{\ln |z| + i\varphi}=\Re z+i\Im z = z\quad ,
\]
which completes the proof.
\end{proofOptional}

\subsubsection{Exercises}
\begin{problem}\label{probComplexLogs}
Plot the number $z$ on the complex plane (you may use one drawing only for all the numbers). Find all real numbers $\varphi$ and $\rho$ for which $z=e^{\rho+i\varphi}$. Your answer may contain expressions of the form $\arcsin x$, $\arccos x$, $\arctan x$, $\ln x$, only if $x$ is a real number.
\begin{enumerate}
\item \label{prob1plussqrt3} $z=1+i\sqrt{3}$.
\item \label{prob2plus3i} $z=-2-3i$.
\item $z=1-i\sqrt{(3)}$.
\item $z=1+i$.
\item $z=-1-i$.
\item $z=\frac{\sqrt{3}+i}4$.
\item $z=-i$.
\item $z=3+4i$.
\end{enumerate}
\end{problem}
\begin{solution} \textbf{\ref{probComplexLogs}.\ref{prob1plussqrt3} (long version).}

We have that 
\[
|z|=\sqrt{z\bar z}= \sqrt{(1+i\sqrt{3})(1-i\sqrt{3})}=\sqrt{1^2+\sqrt{3}^2}=\sqrt{4}=2\quad .
\]
Recall from \eqref{eqEulerExp} $e^{\rho +i \varphi}= e^{\rho}(\cos\varphi+i\sin \varphi) $ and therefore 
\[
\begin{array}{rcl}
\cos \varphi &=& \frac{|z|\cos \varphi}{|z|}= \frac{\Re z}{|z|} =  \frac{1}{2}\\
\sin \varphi &=& \frac{|z|\sin \varphi}{|z|}= \frac{\Im z}{|z|} =  \frac{\sqrt{3}}2\\
\tan \varphi &=& \frac{\sin \varphi}{\cos \varphi}=  \frac{\sqrt{3}}3\quad .
\end{array}
\]
Therefore $\varphi$ is of the form $\varphi =  \arctan \left(\frac{\sqrt{3}}3\right)= \frac{\pi}3+k\pi$. However $\varphi$ cannot be of the form $ \frac{\pi}3+(2k+1)\pi$ because $\cos \left(\frac{\pi}3+(2k+1)\pi\right)=-\frac 12$. On the other hand, $\sin (\frac{\pi}{3}+2k\pi) = \frac{\sqrt{3}}2$ and $\cos (\frac{\pi}{3}+2k\pi) = \frac {1}{2})$. Therefore 
\[
\varphi =\frac{\pi}{3}+2k\pi, \quad \quad \mathrm{for~all~} k\in \mathbb Z
\] 
(Recall that $\mathbb Z$ denotes the integers).

As studied in class $e^{\rho}=|z|=2$, and therefore $\rho = \ln (e^\rho)= \ln |z|=\ln 2 $. Therefore we get the answer
\[
1+i\sqrt{3} = e^{\ln 2 +i \left(\frac{\pi}3+2k\pi  \right) } 
\]
for all $k\in \mathbb Z$. To finish the task we need to plot the number $z$.

\optionalDisplay{
\psset{xunit=1cm,yunit=1cm}
\begin{pspicture*}(-3,-4)(4,4)
\psline[linecolor=gray](-1.5,0)(2.5,0) % x-axis
\psline[linecolor=gray](0,-1.5)(0,2.5) % y-axis
\rput[l](2.5,0){$\Re z$}
\rput[b](0,2.5){$\Im z$}
\rput[bl](0.03,1.03){$i$}
\rput[bl](1,-0.1){$1$}
\rput[c](0,1){$\bullet$}
\rput[c](1,0){$\bullet$}
\psplot{-1}{0.5}{1 x x mul sub sqrt} 

\psplot[linecolor=red]{0.5}{1}{1 x x mul sub sqrt} 

\psplot{-1}{1}{1 x x mul sub sqrt -1 mul}

\psline(0,0)(0.5,0.866)
\rput[c](0.5,0.866){$\bullet$}
\rput[l](0.47,1){$\frac{z}{|z|}$}
\psline[linestyle=dotted](0.5,0.866)(1,1.74)
\rput[l](1.1,1.81){$z=1+i\sqrt{3} $}
\rput[c](1,1.74){$\bullet$}

\rput[t](-2,-3){$-2-3i$}
\psline[linestyle=dotted](0,0)(-2,-3)
\rput[c](-2,-3){$\bullet$}

\rput[l](0.84,0.65){$\varphi= \frac\pi 3 $}
\end{pspicture*}
} %optionalDisplay

\textbf{Solution \ref{probComplexLogs}.\ref{prob1plussqrt3} (short version).} We draw the number $z$ as above. We compute that $\sin \varphi = \frac{\Im z}{|z|}= \sqrt{3}/2$, $\cos \varphi= \frac{\Re z}{|z|}=1/2$. Therefore we have that
\[
1+i\sqrt{3}= e^{\ln|1+i\sqrt{3}| + i(\frac{\pi}3 +2k\pi)}= e^{\ln 2 + i(\frac{\pi}3 +2k\pi)}\quad .
\] 

\textbf{Solution \ref{probComplexLogs}.\ref{prob2plus3i}. } We draw the number as indicated on the figure. We compute that $\sin \varphi =-\frac{3}{\sqrt{13}}$, $\cos \varphi=-\frac{2}{\sqrt{13}}$, $\tan \varphi = \frac{3}{2}$. By the convention of our course, $\arctan \varphi\in (-\frac{\pi}{2}, \frac{\pi}{2})$. Therefore $\varphi= \left(\arctan\left(\frac{3}{2}\right) +\pi\right)+2k\pi $ for all $k\in \mathbb Z$. Finally, we get
\[
\begin{array}{rcl}
-2-3i&=& e^{\ln|-2-3i| + i\left(\left(\arctan\left(\frac{3}{2}\right) +\pi\right) +2k\pi\right)}= e^{\ln \sqrt{13}  + i\left(\left(\arctan\left(\frac{3}{2}\right) +\pi\right) +2k\pi\right)} \\&=&  e^{\frac{1}2\ln 13  + i\left(\left(\arctan\left(\frac{3}{2}\right) +\pi\right) +2k\pi\right)}\quad .
\end{array}
\] 
\end{solution}
\begin{problem}
Simplify the expressions.
\begin{enumerate}
\item $\sin(\arccos x)$.
\item $\tan (\arccos x)$.
\item $\cos (\arctan x)$.
\item $\sin (2 \arctan x)$.
\end{enumerate}
\end{problem}
\begin{problem} 
\begin{itemize}
\item Let $a+b\neq k\pi $ for any $k\in \mathbb Z$ (integers). Prove that
\[
\frac{\tan a + \tan b}{1-\tan a \tan b }= \tan (a+b)\quad .
\]
\item Let $x$ and $y$ be real. Prove that, for $xy\neq 1$, we have
\[\arctan x+\arctan y= \arctan \frac{x+y}{1-xy}
\]
if the left hand side lies between $(-\frac{\pi}{2}, \frac{\pi}{2})$.
\end{itemize}
\end{problem}


\begin{problem}\label{probFracLin}
Find $f^{-1}$.
\begin{enumerate}
\item \label{probFracLinTransfExample}  $f(x)= \frac{5x+6}{4x+5}$.
\item  $f(x)= \frac{2x-3}{-3x+4}$.
\item \label{probFracLinPreservingUnitCircle} Let $a$ be a complex number. $\displaystyle f(x)=\frac{x-a}{\bar a x-1}$.
\item  Let $x$ be a complex number that lies on the unit circle, i.e., $x=e^{i\varphi}$ for some real number $\varphi$, or equivalently a number such that $ x \bar x = |x|^2 =1$. Prove that $f(x)$ lies on the unit circle, where $f(x)$ is the function given in point \ref{probFracLinPreservingUnitCircle} of the current problem.
\end{enumerate}
If you would like to find about the origin of this problem, google ``fractional linear transformation''. 
\end{problem}
\begin{solution}
\textbf{\ref{probFracLin}.\ref{probFracLinTransfExample}.} Set $f(x)=y$. Then 
\[
\begin{array}{rcl}
y&=&\displaystyle \frac{5x+6}{4x+5}\\
y(4x+5)&=&5x+6\\
x(4y-5)&=&-5y+6\\
x&=&\displaystyle \frac{-5y+6}{4y-5}\quad .
\end{array}
\]
Therefore the function $x=g(y):=\frac{-5y+6}{4y-5}$ is the inverse of $f(x)$. In mathematical notation, $g=f^{-1}$. The function $g$ is defined for $y\neq 5/4$.

Let us check our work.
\[
g(f(x))=  \frac{-5f(x) +6}{4f(x)-5}=  \frac{-5\frac{(5x+6)}{4x+5} +6}{4\frac{(5x+6)}{4x+5}-5}= \frac{-5(5x+6) +6(4x+5)}{4(5x+6)-5(4x+5)}=\frac{-x}{-1}=x\quad ,
\]
as expected.
\end{solution}
\subsection{Plots of trigonometric functions and their inverses when $x$ is real}\label{secGraphsTrigFunc}
Let $x\in \mathbb R$ ($x$- real number). Then $\sin x$ and $\cos x$ can be drawn as follows.

\optionalDisplay{
\psset{xunit=0.5cm,yunit=0.5cm}
\begin{pspicture*}(-10,-3)(10,3)
\psline[linecolor=gray]{->}(-9.5,0)(9.5,0) % x-axis
\psline[linecolor=gray]{->}(0,-2.5)(0,2.5) % y-axis
\rput[l](9.5,0){$x$}
\rput[b](0,2.5){$y$}
\rput(4,1.6){$y=\sin x$}
\rput[lt](1,0){1}
\psline[linecolor=gray](1,-0.1)(1,0.1) % x unit mark
\rput[bl](0,1){1}
\psline[linecolor=gray](-0.1,1)(0.1,1) % y unit mark

\psplot{-9.5}{0}{x 180 mul 3.1415 div sin} 
\psplot{1.571}{9.5}{x 180 mul 3.1415 div sin} 

\psplot[linecolor=red,linewidth=1.5pt]{0}{1.571}{x 180 mul 3.1415 div sin} 
\end{pspicture*}


\begin{pspicture*}(-10,-3)(10,3)
\psline[linecolor=gray]{->}(-9.5,0)(9.5,0) % x-axis
\psline[linecolor=gray]{->}(0,-2.5)(0,2.5) % y-axis
\rput[l](9.5,0){$x$}
\rput[b](0,2.5){$y$}
\rput(4,1.6){$y=\cos x$}
\rput[lt](1,0){1}
\psline[linecolor=gray](1,-0.1)(1,0.1) % x unit mark
\rput[bl](0,1){1}
\psline[linecolor=gray](-0.1,1)(0.1,1) % y unit mark

\psplot{-9.5}{-1.571}{x 180 mul 3.1415 div cos} 
\psplot{0}{9.5}{x 180 mul 3.1415 div cos} 

\psplot[linecolor=red,linewidth=1.5pt]{-1.571}{0}{x 180 mul 3.1415 div cos} 

\end{pspicture*}

 Let $x\in \mathbb R$ (i.e., let $x$ be real). Then $e^x$ and $\ln x$ may be plotted as

\psset{xunit=0.5cm,yunit=0.5cm}
\begin{pspicture*}(-3,-10)(10,10)
\psline[linecolor=gray]{->}(-3,0)(9.5,0) % x-axis
\psline[linecolor=gray]{->}(0,-3)(0,9.5) % y-axis
\rput[l](9.5,0){$x$}
\rput[b](0,9.5){$y$}
\rput(2,7.6){$y=e^x$}
\rput[lt](1,0){1}
\psline[linecolor=gray](1,-0.1)(1,0.1) % x unit mark
\rput[br](0,1){1}
\psline[linecolor=gray](-0.1,1)(0.1,1) % y unit mark
\rput[c](1,2.71828){$\bullet$}
\rput[l](1.1,2.71828){$e\approx 2.71828$}

\psplot{0}{3}{ 2.7182818284 x exp} 
\psplot{-3}{0}{1 2.7182818284 div x -1 mul exp} 
\psplot{0.0497870684}{10}{  x ln} 
\rput(7.0,2.3){$y=\ln x$}
\psline[linestyle=dotted](-3,-3)(9.5,9.5)
\end{pspicture*}

Let $x\in \mathbb R$. Then $\tan x$ is plotted as follows.

\psset{xunit=0.5cm,yunit=0.5cm}
\begin{pspicture*}(-5,-10)(10,10)
\psline[linecolor=gray]{->}(-5,0)(5.5,0) % x-axis
\psline[linecolor=gray]{->}(0,-3)(0,9.5) % y-axis
\rput[l](5.5,0){$x$}
\rput[b](0,9.5){$y$}
\rput[t](1,-0.1){1}
\psline[linecolor=gray](1,-0.1)(1,0.1) % x unit mark
\rput[lb](1.570796327,0.1){$\frac{\pi}2$}
\psline[linecolor=gray](1.570796327,-0.1)(1.570796327,0.1) % pi/2 unit mark
\rput[br](0,1){1}
\psline[linecolor=gray](-0.1,1)(0.1,1) % y unit mark

\multido{\r=-1.570796327+1.570796327}{3}{%
\put(\r,0){\psplot{-1.552}{1.552}{ 180 x mul  3.1415 div tan} }
\put(\r,0){\psline[linestyle=dotted](1.570796327,-10)(1.570796327,10) }
}

\end{pspicture*}
\psset{xunit=0.5cm,yunit=0.5cm}
\begin{pspicture*}(-5,-5)(5,5)
\psline[linecolor=gray]{->}(-5,0)(4.5,0) % x-axis
\psline[linecolor=gray]{->}(0,-3)(0,4.5) % y-axis
\rput[l](4.5,0){$x$}
\rput[b](0,4.5){$y$}
\rput[t](1,-0.1){1}
\psline[linecolor=gray](1,-0.1)(1,0.1) % x unit mark
\rput[lb](0.1, 1.570796327){$\frac{\pi}2$}
\psline[linecolor=gray](-0.1,1.570796327)(0.1,1.570796327) % pi/2 unit mark
\rput[br](0,1){1}
\psline[linecolor=gray](-0.1,1)(0.1,1) % y unit mark

\psplot{-5}{5}{ x ATAN} 
\psline[linestyle=dotted](-5,1.570796327)(5,1.570796327) 
\psline[linestyle=dotted](-5,-1.570796327)(5,-1.570796327) 
\rput(3,0.5){$y=\arctan x$}
\end{pspicture*}
} %end of optionalDisplay





\subsubsection{Derivatives of trigonometric, exponent functions and their inverses}


Derivative of $e^z$.

\importantFormula{
\frac{d}{d z} e^z= e^z
}
\begin{proofOptional}
\[\begin{array}{rcl}
\frac{d}{dz}e^z &=&\frac{d}{dz}\sum_{n=0}^{\infty} \frac{z^n}{n!} \underbrace{\eqAttention}_{\mathrm{Caution:~sum~is~infinite}} \sum_{n=0}^{\infty} \frac{d}{dz} \frac{z^n}{n!}\\&=&\sum_{n=1}^{\infty} \frac {nz^{n-1}}{n!}= \sum_{n=1}^{\infty} \frac {z^{n-1}}{(n-1)!} =  \sum_{m=0}^{\infty} \frac {z^{m}}{m!}=e^z\quad .
\end{array}
\]
\end{proofOptional}
Derivative of $\ln x$.
\[
(\ln x)' = \frac{1}x 
\]
\begin{proof}
\[
\begin{array}{rcl}
e^{\ln x}&=& x  \quad \quad \quad \frac {d}{dx}\\
(e^{\ln x})(\ln x)' &= &1\\
 \ln x&=&\frac{1}x\quad .
\end{array}
\]
\end{proof}

Derivative of $x^a$ for $x\in \mathbb R$ and $a\in \mathbb R$, $a\neq 0$. Let $a\in \mathbb R$ and $x\in \mathbb R$. Then
\begin{equation}\label{eqXtotheAthDerivative}
(x^a)'=ax^{a-1}\quad .
\end{equation}
\begin{proof} As $x=e^{\ln x}$ we can define $x^a$ as
\[
x^a\eqdef  e^{a\ln x}\quad .
\]
Therefore 
\[
(x^{a})'=(e^{a\ln x})'=ae^{a\ln x} (\ln x)' =a x^{a}\frac{1}{x} = ax^{a-1} \quad .
\]
\end{proof}

Derivative of $\sin z, \cos z, \tan z, \cot x$.
\[
\begin{array}{rcl}
(\sin x)'&=&\cos x\\
(\cos x)'&=&-\sin x\\
(\tan x)'&=&\frac{1}{\cos ^2 x}\\
(\cot x)'&=&-\frac{1}{\sin ^2 x}\quad .
\end{array}
\]
\begin{proof}
\[
\begin{array}{rcl}
(\sin x)'&=&\left(\frac{e^{ix}-e^{-ix}}{2i}\right)'=\frac{ie^{ix}-(-i)e^{-ix}}{2i}=\frac{e^{ix}+e^{-ix}}2=\cos x\\
(\cos x)'&=&\left(\frac{e^{ix}+e^{-ix}}{2}\right)'=\frac{ie^{ix}-ie^{-ix}}{2}=-\frac{e^{ix}-e^{-ix}}{2i}=-\sin x \\
(\tan x)'&=&\frac{(\sin x)'\cos x- \sin x (\cos x)'}{\cos x^{2}}=\frac{\sin^2x+\cos^2x}{\cos^2x}=\frac{1}{\cos^2x} \quad .\\
\end{array}
\]
\end{proof}

Derivative of $\arcsin x, \arccos x, \arctan x$.
\[
\begin{array}{rcl}
(\arcsin x)'&=&\frac{1}{\sqrt{1-x^2}} \\
(\arccos x)'&=&-\frac{1}{\sqrt{1-x^2}}\\
(\arctan x)'&=&\frac{1}{1+x^2}\quad .\\
\end{array}
\]
\begin{proof}
\[
\begin{array}{rcl}
\sin(\arcsin x)&=& x  \quad\quad\left|\frac{d}{dx}\right. \\
(\arcsin x)'\cos (\arcsin x) &=& 1\\
(\arcsin x)'\sqrt{1-\sin^2(\arcsin x) } &=& 1\\
(\arcsin x)' &=&\frac{1}{\sqrt{1-x^2}}\quad .
\end{array}
\]
\[
\begin{array}{rcl}
\cos(\arccos x)&=& x \quad\quad\left|\frac{d}{dx}\right. \\
-(\arccos x)'\sin (\arccos x) &=& 1\\
-(\arccos x)'\sqrt{1-\cos^2(\arccos x) } &=& 1\\
(\arccos x)' &=&-\frac{1}{\sqrt{1-x^2}}\quad .
\end{array}
\]
\[
\begin{array}{rcl}
\tan(\arctan x)&=& x \quad\quad\left|\frac{d}{dx}\right. \\
-(\arctan x)'\frac{1}{ (\cos{\arctan x})} &=& 1 \quad \quad \quad \mathrm{use~} \frac{1}{\cos^2x}= 1+\tan^2x\\
-(\arctan x)'(1+\tan^2(\arctan x) &=& 1\\
(\arctan x)' &=&\frac{1}{1+x^2}\quad .
\end{array}
\]

\end{proof}


\textbf{Optional material.} (!)Holomorphic functions. We say that a function $f$ is \emph{holomorphic} in a neighborhood of  $z$ if the limit 
\[
f'(z)\eqdef \lim_{\substack{h\to 0\\ h\in \mathbb C}} \frac{ f(z+h)-f(z)}{h}
\]
exists.

You may notice that the above looks very similar to the definition that $f$ be differentiable in a neighborhood of $x$. The difference between ``differentiable'' and ``holomorphic'' is that holomorphic functions are required to have limits no matter from which direction in the complex plane does $h$ approach $0$. On the other hand, for a function to be differentiable, we only need to have limit when $h$ approaches $0$ along the real axis (either from the left, negative side, or from the right, positive side). Thus it is ``easier'' for a function to be differentiable, than it is to be holomorphic. All holomorphic functions, when restricted to the reals, are differentiable, but it is easy to construct real functions that are differentiable but cannot be extended to a holomorphic function (one such function is given in problem \ref{probDifferentiableNonAnalyticFunctionExample}).

\textbf{Note.} You are not required to remember this definition and you will not be tested on it. In order to fully understand this definition you must study the subject of complex analysis. Here is the Wikipedia page:

\url{http://en.wikipedia.org/wiki/Complex_analysis} \quad .

\optionalMaterial
\[\begin{array}{l}
\mathrm{The~derivatives~of~holomorphic~functions~are~computed} \\
\mathrm{by~the~same~rules~as~those~for~real~functions.} \\
\end{array}
\]
This means you can use identities such as $(ix)'=i$. $(e^{ix})'=ie^{ix}$, $(\cos (ix))'= -i\sin(ix)$. Of the functions we studied so far, $e^z, \sin z, \cos z$ are holomoprhic functions everywhere, the functions $\arctan z$, $\log z$ are holomorphic in neighborhoods of the values of $z$ for which they are defined, and the functions $\Im z, \Re z, |z|$ are not holomorphic.




Definition of hyperbolic sine, hyperbolic cosine and hyperbolic tangent.

Derivative of $\sinh z$, $\cosh z$, $\tanh z$, $\coth z$.

Derivative of $ \arsinh z$, $ \arcosh z$, $ \arctanh z$.
%\end{comment}
\chapter{Techniques of integration}
\section{Integration by parts}
Integration by parts can be formulated using differential forms as
\importantFormula{
\int f dg+ \int g df = fg\quad, 
}
or alternatively as
\importantFormula{
\int f(x) g'(x)dx = fg- \int g(x) f'(x)dx \quad .
}
Combining the above with the fundamental theorem of Calculus yields the definite integral version
\importantFormula{
\int\limits_{x=a}^{b} f(x) g'(x)dx = \left.fg\phantom\int\right|_{x=a}^b - \int\limits_{x=a}^b g(x) f'(x)dx \quad.
}
Idea of integration by parts: suppose we already know the integral of $g(x)$, say $d(G(x))=g(x)dx$. Suppose for some reason we consider the integral of $f'(x)dx$ to be easier than the integral of $f(x)dx$. If we want to integrate $f(x)g(x)dx= f(x)d(G(x))$, then we may use integration by parts to get
\[
\int f(x)\underbrace{g(x)dx}_{=d(G(x))}= f(x)g(x)- \int G(x) \underbrace{f'(x)dx}_{=d(f(x))}\quad .
\]
If the function $G(x)$ is not too complicated, then $f'(x)G(x)dx$ may be easier to integrate than $f(x)g(x)dx$. If that is the case, integrating by parts pays off, and we should use the technique. 

There is no 100\% recipe when integration by parts makes the problem easier. However, with practice and experience, on small examples, one can quickly judge when integration by parts helps.
\subsection{Examples and exercises}
\begin{problem}
Compute $\displaystyle \int x \sin x dx$.
\end{problem}
\begin{solution}
\[
\begin{array}{rcl}
\displaystyle\int x \underbrace{\sin x dx}_{=-d\cos x} = -\int x d\cos x = -x\cos x + \int \cos x dx = -x \cos x +\sin x +C\quad .
\end{array}
\]
\end{solution}
\begin{problem}
Compute $\displaystyle \int x^2 e^x dx$.
\end{problem}
\begin{solution}
\[
\begin{array}{rcl}
\displaystyle\int x^2 \underbrace{e^x dx}_{d (e^x)} &=& \int x^2 d e^x= x^2e^x - \int e^x2xdx=  x^2e^x - \int 2xd e^x \\
&=& x^2e^x- 2xe^x+ \int 2e^x dx= x^2e^x-2xe^x+2e^x+C\quad .
\end{array}
\]
\end{solution}
\begin{problem}
Compute $\displaystyle \int \ln x dx $.
\end{problem}
\begin{solution}
\[
\int \ln x dx = x\ln x - \int x d(\ln x)= x\ln x -\int \frac{x}{x}dx= x\ln x-x+C\quad . 
\]
\end{solution}
\begin{problem}
Compute $\displaystyle \int \sin x e^x dx$.
\end{problem}
\begin{solution}
\[
\begin{array}{rcl}
\displaystyle\int \sin x \underbrace{e^x dx}_{=de^x}&=&\displaystyle\sin x e^x - \int e^xd(\sin x)=\sin x e^x - \int \cos x \underbrace{e^x dx}_{=de^x}\\
&=& \displaystyle\sin x e^x - e^x\cos x +\int e^x d(\cos x) \\
&=& e^x\sin x -e^x\cos x-\int e^x\sin x dx\quad . 
\end{array}
\]
Transferring $\displaystyle\int e^x\sin x dx$ to the left hand side we get $\displaystyle 2\int \sin x e^x dx = \sin x e^x-e^x\cos x$, or finally, $\displaystyle \int \sin x e^x dx=\frac12 \left(\sin x e^x- e^x\cos x\right)$.
\end{solution}
\begin{problem}
Compute $\displaystyle\int \arctan x dx$.
\end{problem}
\begin{solution}
\[
\begin{array}{rcl}
\displaystyle\int \arctan x dx &=& x\displaystyle \arctan x - \int x d(\arctan x)= x\arctan x - \int \frac{x}{x^2+1}dx\\
&=& \displaystyle  x\arctan x - \int \frac{\frac{1}2d(x^2)}{x^2+1}=  x\arctan x - \int \frac{\frac{1}2d(x^2+1)}{x^2+1}\\
&=& x\arctan x - \frac12\ln (x^2+1) +C\quad .
\end{array}
\]
\end{solution}
\begin{problem}
Compute $\displaystyle \int x^n e^x dx$.
\end{problem}
\begin{solution}
\[
\begin{array}{rcl}
\displaystyle\int x^n e^x dx &=& \displaystyle\int x^n d e^x= x^n e^x- \int  e^x dx^n= x^n e^x- n\int  x^{n-1}e^x dx=\\
&=&\displaystyle x^ne^x- n\left(\int x^{n-1}de^x \right)= x^ne^x - n\left(x^{n-1}e^x- \int (n-1)x^{n-2}e^x dx\right)\\
&=&\displaystyle x^{n}e^x-nx^{n-1}xe^x+n(n-1)\int x^{n-2}e^x dx =\dots\\
&=&\displaystyle x^ne^{x}- nx^{n-1}e^x+n(n-1)x^{n-2}e^x+\dots \\
&&+(-1)^kn(n-1)(n-2)\dots(n-k+1)x^{n-k}e^x+\dots +(-1)^n n! e^x+C\\
&=&\displaystyle C+\sum_{k=0}^{n} (-1)^n\frac{n!}{(n-k)!} x^{n-k} e^x \quad .
\end{array}
\]
\end{solution}
\begin{problem}
Compute $\displaystyle\int \sin^n x dx$.

\end{problem}

\begin{problem}
Evaluate the indefinite integral. Illustrate the steps of your solutions. Check your answer using a computer algebra system.
\begin{multicols}{2}
\begin{enumerate}
\item 
$\int x^2\cos 2x dx$
\item 
$\int x^2e^{ax}dx$
\item 
$\int x^2e^{-ax}dx$
\item \label{eqProblemA}
$\int x^2\frac{(e^{ax}+e^{-ax})^2}4dx$ 
\item 
$\int \cos^2x dx$ 
\item 
$ \int x\ln x dx 
$
\item $\int \frac{x}{1+x^2} dx$ 
\item 
$ \int \arctan x dx
$
\item 
$ \int \arcsin x dx
$
\item $\int \frac{\ln x}{\sqrt{x}}dx $
\item $\int (\arcsin x)^2 dx $ \quad \quad (Hint: Try substituting $x=\sin y$)
\item $\int \frac{1}{\cos^2 x}dx$\quad \quad (Hint: What is the derivative of $\tan x$?)
\item $\int (\tan^2 x) dx $ \quad \quad (Hint: $\tan^2 x = \frac{1}{\cos^2x }-1$ )
\item $\int x(\tan^2 x) dx $ \quad \quad (Hint: $\tan^2 x dx= d(F(x))$, where $F(x)$ is the answer from the preceding problem)
\item 
$
\int\arctan (\frac{1}x)dx
$
\item $\int \sin (\ln (x)) dx $
\item
$\int x^2\cos^2x dx$ (Can you solve using complex numbers and Problem \ref{eqProblemA}?).
\end{enumerate}
\end{multicols}
\end{problem}

\section{Integration of rational functions}

\subsection{A few examples}

\subsection{The theory behind integrating rational functions}
Every rational function may be represented as a sum of a polynomial and a proper fraction. A fraction is proper if its numerator is a polynomial of smaller degree than the denominator. 

A non-proper function is transformed to a proper one through polynomial 
division.

\begin{theorem}(The fundamental theorem of algebra). Every polynomial $D(x)= a_0x^n+a_{1}x^{n-1}+\dots + a_n$ can be factored over the complex numbers into linear terms 
\begin{equation}\label{eqFundamentaTheoremAlgebra}
D(x)=a_0(x-x_1)(x-x_2)\dots (x-x_n),
\end{equation}
where $x_1,\dots, x_n$ are the roots of $D(x)$. The roots of $D(x)$ are complex numbers.
If all coefficients $a_i$ of the polynomial $D(x)= a_0x^n+a_{1}x^{n-1}+\dots + a_n$ are real numbers, and $z$ is a non-real root of $D(x)$, then the complex conjugate $\bar z$ of $z$ is also a root of $D(x)$. 
\end{theorem}

Therefore the terms  $(x-z)(x-\bar z)$ appear in the factorization \eqref{eqFundamentaTheoremAlgebra}. The terms $(x-z)(x-\bar z)$ may be combined to 
\begin{equation}\label{eqCombineConjugateRoots}
(x-z)(x-\bar z)= x^2- (z+\bar z)x + z\bar z = x^{2}+ (\underbrace{-2\Re z}_{c_j})x+ \underbrace{(\Re z)^2+(\Im z)^2}_{d_j}.
\end{equation}
The discriminant of the above quadratic polynomial is negative since it equals $4(\Re z)^2- 4((\Re z)^2+(\Im z)^2)= -4(\Im z)^2<0$. 
If all coefficients $a_i$ of the polynomial $D(x)= a_0x^n+a_{1}x^{n-1}+\dots + a_n$ are real numbers, then $D(x)$ can be factored in the form 
\begin{equation}\label{eqFactorOverReals}
D(x)= a_0(x-x_1)^{p_1}\dots (x-x_k)^{p_k} (x^2+c_1x+d_1)^{q_1}(x^2+c_2x+d_2)\dots (x^2+c_lx +d_l)^{q_k},
\end{equation}
where $x_1, \dots, x_k$ are real numbers, $c_1, \dots, c_l$ and $d_1,\dots, d_l$ are real numbers, $p_1,\dots, p_k, q_1, \dots, q_l$ are positive integers, and there are no repeating terms. Furthermore $c_j^2-4d_j<0$ and each of the terms $x^2+c_j x+d_j$ is formed by combining two  roots of $D(x)$ as indicated in \eqref{eqCombineConjugateRoots}. 

Let $\frac{N(x)}{D(x)}$ be a proper fraction, and let $D(x)$ be factorized as indicated in \eqref{eqFactorOverReals}. Then $\frac{N(x)}{D(x)}$ can be written in the form
\begin{equation}\label{eqSplitPF}
\begin{array}{rcl}
\displaystyle\frac{N(x)}{D(x)}&=&\displaystyle \left (\frac{A_{11}}{x-x_1}+\dots +\frac{A_{1p_1}}{(x-x_1)^{p_1}}\right) +\dots + \left (\frac{A_{k1}}{x-x_k}+\dots +\frac{A_{kp_k}}{(x-x_1)^{p_k}}\right) \\
&&\displaystyle+\left(\frac {B_{11}x+C_{11}}{x^2+c_1x+d_1}+\dots +\frac {B_{1q_1}x+C_{1q_1}}{(x^2+c_1x+d_1)^{q_1}} \right)+\dots 
\\&&\displaystyle +
\left(\frac {B_{l1}x+C_{l1}}{x^2+c_lx+d_l}+\dots +\frac {B_{lq_l}x+C_{lq_l}}{(x^2+c_lx+d_l)^{q_l}} \right)\\ \\
&&(\mathrm{in~}\sum\mathrm{~notation}):\\ \\
&=&\displaystyle \sum_{j=1}^{k}\sum_{s=1}^{p_k} \frac{A_{js}}{(x-x_j)^s}+\sum_{j=1}^{l}\sum_{s=1}^{q_j} \frac{B_{js}x+C_{js}}{(x^2+c_jx+d_j)^s}\quad .
\end{array}
\end{equation}
The constants $A_{js}, B_{js}, C_{js}$ are real numbers and may be extracted by clearing the denominators in the above identity and comparing the coefficients in front of $x$.
Using linear substitutions, integrals of the form 
\[\displaystyle\int \frac{A}{(x-x_j)^s}dx,
\] 
and
\[
\int \frac{Bx+C}{(x^2+c_jx+d_j)^s}dx
\] 
can be transformed to combinations of integrals of the forms: 
\[
\int \frac{1}{(y+a)^n}dy,
\]  
\[
\int \frac{1}{(y^2+1)^n}dy,
\] 
\[\int \frac{y}{(y^2+1)^n}dy.
\] 
Each of these ``building block'' integrals has different behavior for $n=1$ and for $n\geq 2$. We solve each of the ``building block'' integrals below. 

\subsubsection{Integrals of the form $\displaystyle\int \frac{1}{(x+a)^n}dx$.}
For $n=1$ we get
\begin{equation}\label{eqBuildingBlockInt1base}
\int \frac{1}{x+a}dx=\int {\frac{1}{x+a}d(x+a)}= \ln |x+a| +C\quad .
\end{equation}
Let $n> 1$. Then
\begin{equation}\label{eqBuildingBlockInt1N}
\int \frac{1}{(x+a)^n}dx= -\frac{1}{(n-1)(x+a)^{n-1}}+C \quad .
\end{equation}
\subsubsection{Integrals of the form $\displaystyle\int \frac{x}{(1+x ^2)^n} dx $.}
If we use the substitution $y=x^2$, we get $dy=2xdx $ and so $\displaystyle\int \frac{x}{(1+x ^2)^n} dx = \displaystyle\int \frac{\frac12 dy}{(y+1)^n} dy $, i.e., our integral is transformed to an integral of the form \eqref{eqBuildingBlockInt1base} or  \eqref{eqBuildingBlockInt1N}. In short form, this can be written as follows. 

For $n=1$ we have
\[
\int \frac{x}{x^2+1}dx= \int \frac{\frac{1}2 d(x^2+1)}{x^2+1}= \frac{1}{2}\ln (x^2+1) +C\quad .
\]
For $n>1, n\in \mathbb Z$ we have
\[
\int \frac{x}{(x^2+1)^n}dx= \int \frac{\frac{1}2d(x^2+1)}{(x^2+1)^n}= -\frac{1}{2(n-1)(x+a)^{n-1} } +C\quad .
\]
\subsubsection {Integrals of the form $\displaystyle\int \frac{1}{(1+x ^2)^n} dx $.}
For $n=1$ we can recall that the derivative of $\arctan x$ is $\displaystyle\frac{1}{1+x^2}$, in other words
\[
\int \frac{1}{x^2+1}dx= \arctan x+C\quad .
\]
For $n>1, n\in \mathbb Z$, we are solving the integral
\begin{equation}\label{eqBuildingBlock3N}
\int \frac{1}{(1+x ^2)^n}dx\quad .
\end{equation}
This ``building block'' integral and is considerably more challenging than the preceding ``building blocks''. There is a trick to solving \eqref{eqBuildingBlock3N}. Consider the integral $\displaystyle\int \frac{1}{(1+x ^2)^{n-1}}dx$ and integrate it by parts:
\[
\begin{array}{rcl}
\displaystyle \int \underbrace{\frac{1}{(1+x ^2)^{n-1}}}_{f(x)}d\underbrace{x}_{g(x)}&=&\displaystyle \frac{x}{(1+x ^2)^{n-1}}-\int x d\left(\frac{1}{(1+x ^2)^{n-1}}\right)\\
&=&\displaystyle\frac{x}{(1+x ^2)^{n-1}}- \int 2(-n+1)\frac{x^2}{(1+x^2)^n}dx\\
&=&\displaystyle\frac{x}{(1+x ^2)^{n-1}}+2(n-1) \int \frac{(1+x^2-1)}{(1+x^2)^n}dx\\
&=&\displaystyle\frac{x}{(1+x ^2)^{n-1}} +2(n-1)\int \frac{1}{(1+x^2)^{n-1}}dx \\&&\displaystyle -2(n-1) \int \frac{1}{(1+x^2)^n}dx\quad .
\end{array}
\]
Transfer the left-hand side term $\displaystyle\int \frac{1}{(1+x^2)^{n-1}}dx$ and the right-hand side term  $\displaystyle -2(n-1) \int \frac{1}{(1+x^2)^n}dx$ to the opposite sides to obtain that 
\[
\int \frac{1}{(1+x^2)^n}dx= \frac{x}{(2n-2)(1+x ^2)^{n-1}}+ \frac{2n-3}{2n-2}\int \frac{1}{(1+x^2)^{n-1}}dx\quad .
\]
In this way we expressed the integral $\displaystyle\int \frac{1}{(1+x^2)^n}dx$ using a simpler one, $\displaystyle\int \frac{1}{(1+x^2)^{n-1}}dx$. In a similar fashion, we may express $\displaystyle\int \frac{1}{(1+x^2)^{n-1}}dx$ using $\displaystyle\int \frac{1}{(1+x^2)^{n-2}}dx$, and so on, until we reach $\displaystyle\int \frac{1}{(1+x^2)}dx=\arctan x +C$. Our answer may be summarized as
\[
\begin{array}{rcl}
\displaystyle \int \frac{1}{(1+x^2)^n}dx&=& \displaystyle  \frac{x}{(1+x ^2)^{n-1}}+\frac{2n-3}{2n-2}\left(\frac{x}{(1+x^2)^{n-2}}+\frac{2n-5}{2n-4}\left(\dots\right)\right)\\ \\ &&\mathrm{using~}\Sigma \mathrm{~notation:}\\ \\
&=&\displaystyle \frac{(2n-3 )(2n-5)\dots 1}{(2n-2)(2n-4)\dots 2}\arctan x+  \frac{x}{(2n-2)(1+x ^2)^{n-1}} \\&&\displaystyle + \sum_{s=3}^{n} \frac{(2n-3 )(2n-5)\dots (2s-3)}{(2n-2)(2n-4)\dots (2s-2)} \frac{x}{(2s-4)(1+x^2)^{s-2}} \quad .
\end{array}
\]

\subsection{Algorithm for integrating rational functions}
In this section we summarize the algorithm for integrating rational functions $\int \frac{N(x)}{D(x)}dx$.
\begin{itemize}
\item If the degree of $N(x)$ is greater than the degree of $D(x)$, use polynomial division with remainder to transform to proper fraction. In other words, by the algorithm of long division, obtain $Q(x)$ and $R(x)$ such that $N(x)= Q(x)D(x)+R(x)$ and the degree of $R(x)$ is smaller than the degree of $D(x)$. Then use 
\[
\frac{N(x)}{D(x)}=Q(x) +\frac{R(x)}{D(x)}\quad .
\]
\item Factor $D(x)$, i.e., produce the decomposition \eqref{eqFactorOverReals}. In many problems, $D(x)$ will be presented to you in already factored form.
\item Split $\frac{R(x)}{D(x)}$ into partial fractions, i.e., write it in the form \eqref{eqSplitPF}. 
\item Using linear substitutions, transform each partial fraction into a multiple of one of the ``building block'' integrals  $\int \frac{1}{1+y^2}dy$, $\int \frac{1}{(1+y^2)^n}dy$, $n\geq 2$, $\int \frac{y}{(1+y^2)^n}dy$, $\int \frac{1}{y+C}dy$, $\int \frac{1}{(y+C)^n}dy$, $n\geq 2$.
\item Solve the ``building blocks'' integrals as we did earlier in this section.
\item Collect the ``building blocks'' to a single answer.
\item Check your answer by differentiation!!!
\end{itemize}
\optionalMaterial A few notes on factoring polynomials are in order. 
\begin{itemize}
\item There are algorithms for factoring a polynomial $D(x)$ over the rational numbers.
\item There is a ``closed form'' formula for finding the roots of an arbitrary polynomial of degree 4 or less. Here, ``closed form'' formula means it is written with finitely many radicals (second, third, \dots roots), addition, subtraction, division and multiplication.
\item A polynomial of degree 5 will generically have roots not of the above form (i.e., its roots cannot be written using finitely many arithmetic and radical operations). For concrete polynomials, there may be exceptions. All examples in our calculus course will fall under this exception.
\item One can solve (for tautological reasons) an arbitrary polynomial of degree 5 or more by allowing operations involving additional (transcedental) functions. These functions may be described using infinite power series, just as we will later come to describe the radical function $\sqrt{1+z}$ (see \eqref{eqNewtonBinomialGeneralized}). However, the properties of these transcedental functions (for example, whether one such function may be used for two different polynomials or how to systematically describe them using computer algebra systems) are not completely understood; this is an area of active mathematical research under the general title ``Galois Theory''.
\end{itemize}

\subsection{A large example illustrating the full algorithm }
\begin{problem}
Integrate 
\[
\int \frac{x^{6}-x^{5}+\frac{9}{2} x^{4}-4 x^{3}+\frac{13}{2} x^{2}-\frac{7}{2} x+\frac{11}{4}}{x^{5}-x^{4}+3 x^{3}-3 x^{2}+\frac{9}{4} x-\frac{9}{4}} dx\quad .
\]
\end{problem}

\begin{solution}

\noindent\textbf{Step 1.} Reduce the fraction so that numerator has smaller degree than the denominator. Using polynomial division we see that 
\[
\begin{array}{l}
x^{6}-x^{5}+\frac{9}{2} x^{4}-4 x^{3}+\frac{13}{2} x^{2}-\frac{7}{2} x + \frac{11}{4} \\ = (x^{5}-x^{4}+3 x^{3}-3 x^{2} + \frac{9}{4} x-\frac{9}{4}) x +\frac{3}{2} x^{4}-x^{3} +\frac{17}{4} x^{2}-\frac{5}{4} x +\frac{11}{4}\quad ,
\end{array}
\]
and therefore 
\[ 
\frac{x^{6}-x^{5}+\frac{9}{2} x^{4}-4 x^{3}+\frac{13}{2} x^{2}-\frac{7}{2} x+\frac{11}{4}}{x^{5}-x^{4}+3 x^{3}-3 x^{2}+\frac{9}{4} x-\frac{9}{4}} = x+\frac{6 x^{4}-4 x^{3}+17 x^{2}-5 x+11}{4x^{5}-4 x^{4}+12 x^{3}-12 x^{2}+9 x-9}
\]
Set 
\[
N(x)\eqdef 6 x^{4}-4 x^{3}+17 x^{2}-5 x+11
\]
and 
\[
D(x)\eqdef 4x^{5}-4 x^{4}+12 x^{3}-12 x^{2}+9 x-9\quad .
\]

\noindent\textbf{Step 2.} (Split into partial fractions). Factor the denominator $D(x)=4x^{5}-4 x^{4}+12 x^{3}-12 x^{2}+9 x-9$. In many high schools, one studies how to find all rational roots of $D(x)$, on condition $D(x)$ has integer coefficients. There, one is taught that if $\frac{p}{q}$ is a rational number, then $\pm \frac{p}{q}$ may be a root of the integer coefficient polynomial $D(x)$ only if $p$ is a divisor of the constant term of $D(x)$, and $q$ is a divisor of the leading coefficient of $D(x)$. Since in our case the leading coefficient is 4 and the constant term is -9, the only possible rational roots of $D(x)$ are $\pm 1, \pm 3, \pm 9, \pm 1/2, \pm 3/2, \pm 9/2, \pm 1/4, \pm 3/4, \pm 9/4$. A rational number $r$ is a root of $D(x)$ if and only if subtituting $x=r$ yields 0. Direct check shows that, for example,  $D(-1)=-50$. However, $D(1)=0$ and therefore using polynomial division we get that $D(x)=(x-1)(4x^{4}+12x^{2}+9)$. We recognize that the second multiplicand is an exact square and therefore $D(x)=(x-1)(2x^2+3)^2$.

\textbf{Note.} As explained in the theoretical part, finding the roots of polynomials is a non-trivial task and we cannot systematically give an algebraic solution beyond degree 4. Splitting rational functions to partial fractions in Calculus therefore relies on a certain amount of guesswork.

So far we got 
\[
\frac{N(x)}{D(x)}= \frac{6 x^{4}-4 x^{3}+17 x^{2}-5 x+11}{(x-1)(2x^2+3)^2}\quad .
\]
In order to split $\frac{N(x)}{D(x)}$ into partial fractions, we need to find numbers $A, B, C, D, E$ such that 
\[
\frac{6 x^{4}-4 x^{3}+17 x^{2}-5 x+11}{(x-1)(2x^2+3)^2}= \frac{A}{(x-1)}+\frac{Bx+C}{(2x^2+3)}+\frac{Dx+E}{(2x^2+3)^2}\quad .
\]
After clearing denominators, we see that this amounts to finding $A, B, C, D, E$ such that
\[
6 x^{4}-4 x^{3}+17 x^{2}-5 x+11= A(2x^2+3)^2+ (Bx+C)(2x^2+3)(x-1) + (Dx+E)(x-1)\quad .  
\]
Plugging in $x=1$ we see that $25=25A $ and so $A=1$. We may plug back $A=1$ and regroup to get
\[
2x^{4}-4x^{3}+5x^{2}-5x+2= (Bx+C)(2x^2+3)(x-1) + (Dx+E)(x-1)\quad .
\]
Dividing both sides by $(x-1)$ we get 
\[
2x^{3}-2x^{2}+3x-2= (Bx+C)(2x^2+3)+Dx+E\quad .
\]
Regrouping we get 
\[
x^{3}(2- 2B) + x^2(-2-2C)+x(3-3B-D)+(-2-3C-E)=0\quad.
\]
As $x$ is an indeterminate, the above expression may vanish only if all coefficients in the preceding expression vanish. Therefore we get the system
\[
\left| \begin{array}{rcl}
2-2B&=&0\\
-2-C&=&0\\
3-3B-D&=&0\\
-2-3C-E&=&0\quad .
\end{array}   \right.
\]
We may solve the above linear system using the standard algorithm for solving linear systems (the algorithm is called row reduction and is also known as Gaussian elimination). The latter algorithm is studied in any standard the Linear algebra course. Alternatively, we see from the first equations $B=1$, $C=-1$, and substituting in the remaining equations we see $D=0$, $E=1$. Finally, we check that 
\[
\frac{x^{6}-x^{5}+\frac{9}{2} x^{4}-4 x^{3}+\frac{13}{2} x^{2}-\frac{7}{2} x+\frac{11}{4}}{x^{5}-x^{4}+3 x^{3}-3 x^{2}+\frac{9}{4} x-\frac{9}{4}}
=x+\frac{1}{(x-1)}+\frac{x-1}{(2x^2+3)}+\frac{1}{(2x^2+3)^2}\quad .
\]
\textbf{Step 3.} (Find the integral of each partial fraction). 
\[
\begin{array}{rcl}
\displaystyle\int x dx &= &\frac{x ^2}2+C\\
\displaystyle\int \frac{1}{x-1} dx &=& \ln|x-1|+C\\
\displaystyle\int \frac{x-1}{2x^2+3} dx &=& \displaystyle \int\frac{x}{2x^2+3}dx -\frac{1}{3}\int \frac{1}{2/3x^2+1}dx\\
&=& \displaystyle  \int \frac{d(\frac{x^2}2)}{2x^2+3}dx-\frac{1}{3} \int \frac{1}{\left(\sqrt{2/3}x\right)^2+1} d x\\
&=&\frac{1}{4} \int \frac{d(2x^2+3)}{2x^2+3}dx- \frac{1}{3}\int \frac{\frac{d (\sqrt{2/3}x)} {\sqrt{2/3}}} {\left( \sqrt{2/3}x\right)^2+1} \\
&=&\frac{1}{4}\ln (2x^2+3)-\frac{\sqrt{6}}{6}\arctan (\sqrt{2/3}x)+C
\quad .
\end{array}
\]
The last integral is
\[
\int \frac{1}{(2x^2+3)^2}dx= \frac{1}3 \int \frac{d(\sqrt{2/3}x)/\sqrt{2/3}}{((\sqrt{2/3}x)^2+1)^2}= \frac{\sqrt{6}}{6}\int \frac{d(\sqrt{2/3}x)}{((\sqrt{2/3}x)^2+1)^2}= \frac{\sqrt{6}}{6}\int \frac{dy}{(y^2+1)^2}\quad,
\]
where for the last equality we have set $y=\sqrt{2/3}x$. We may solve $\displaystyle\int\frac{dy}{(y^2+1)^2}$ by integrating by parts. Indeed,
\[\begin{array}{rcl}
C+\arctan y &=&\displaystyle \int \frac{dy}{y^2+1}\\
&=&\displaystyle \frac{y}{y^2+1} +\int \frac{2y^2dy}{(y^2+1)^2}= \frac{y}{y^2+1}+\int \frac{2(y^2+1-1)dy}{(y^2+1)^2}\\
&=&\displaystyle \frac{y}{y^2+1} + 2\int \frac{dy}{(y^2+1)}- 2\int \frac{dy}{(y^2+1)^2}\quad.
\end{array}
\]
Transferring summands we get 
\[
\int \frac{dy}{(y^2+1)^2}= \frac{1}{2}\left(\frac{y}{y^2+1}+\arctan y\right) +C\quad .
\]
The final answer is 
\[
\frac{1}{2}\left(\frac{\sqrt{2/3}x}{2/3x^2+1}\right) +\frac{1}{4}\ln (2x^2+3)\left(\frac{1}2-\frac{\sqrt{6}}{6}\right)\arctan (\sqrt{2/3}x)+\ln|x-1|+\frac{x ^2}2+C\quad .
\]
\end{solution}
\subsubsection{Exercises}
\begin{problem}
Evaluate the indefinite integral. 
\begin{enumerate}
\item $\int \frac1{x^2+x+1}dx$
\item $\int \frac1{4x^2+4x+1}dx$
\item $\int \frac1{2x^2+5x+1}dx$
\item $\int \frac{15x^2-4x-81}{(x-3)(x+4)(x-1)}dx$
\item $\int \frac {x^{4}+10x^{3}+18x^{2}+2x-13}{x^{4}+4x^{3}+3x^{2}-4x-4}dx$ 
Check first that $(x-1)(x+2)^2(x+1)= x^{4}+4x^{3}+3x^{2}-4x-4$.
\end{enumerate}
\end{problem}
\section{Euler substitution, Integrals of the form $\int R(x, \sqrt{ax^2+bx+c})dx$}
Radicals of the form  $\sqrt{ay^2+by+c})$, $a\neq 0$, $b^2-4ac\neq 0$ can be transformed using linear substitutions to a multiple of one of the following radicals: $\sqrt{x^2+1}$, $\sqrt{-x^2+1}$,  $\sqrt{x^2-1}$.

In the present section we solve integrals of the form $\int R(x, \sqrt{x^2+1})dx$, $\int R(x, \sqrt{-x^2+1})dx$, $\int R(x, \sqrt{x^2-1})dx$ for $R$ - rational function. Such integrals can be converted to integrals of rational functions as using the Euler substitution.

\begin{enumerate}
\item Case $\sqrt{x^2+1}$. Substitute $\sqrt{x^2+1}= x+t $. Squaring both sides we see $x^2+1=x^2+2xt+t^2$, or $x=\frac12\left(\frac{1}{t}- t\right)$. Taking differential on both sides, we see that $dx=-\frac12(\frac{1}{t^2}+1) dt$. To summarize:
\begin{equation}\label{eqEulerSub1}
\begin{array}{rcl}
x&=&\frac12\left(\frac{1}{t}- t\right)\\
dx&=&-\frac12(\frac{1}{t^2}+1) dt\\
\sqrt{x^2+1}&=&x+t= \frac12 \left(\frac1t +t\right) \quad .
\end{array}
\end{equation}
\item Case $\sqrt{-x^2+1}$. Substitute $\sqrt{-x^2+1}=(x-1)t$. Squaring both sides and dividing out by $(x-1)$ we get $-x-1=(x-1)t^2$ or $x=1-\frac{2}{t^2+1}$. Finally $dx=\frac{4t}{(1+t^2)^2}dt$. To summarize:
\[
\begin{array}{rcl}
x&=&1-\frac{2}{t^2+1}\\
\sqrt{-x^2+1}&=&-\frac{2t}{t^2+1}\\
dx&=&\frac{4t}{(t^2+1)^2}dt\quad .
\end{array}
\] 
\item Case $\sqrt{x^2-1} $. Substitute $\sqrt{x^2-1}=(x-1)t$. Squaring both sides and dividing out by $(x-1)$ we get $x+1=(x-1)t^2$ or $x=\frac{t^2-1+2}{t^2-1}= 1+\frac{2}{t^2-1}$. Therefore $dx=\frac{-4t}{(t^2-1)^2}dt$. To summarize:
\[
\begin{array}{rcl}
dx&=&\frac{-4t}{(t^2-1)^2}dt\\
\sqrt{-x^2+1}&=&\frac{2t}{t^2-1} \\
x&=&1+\frac{2}{t^2-1}\quad .
\end{array}
\] 
\end{enumerate}
Integrals of the form $\int R(\sin y,\cos y )dy$ for $R$- rational function can be transformed to integrals of rational functions via the substitution $y= 2\arctan x$. Taking differentials on both sides, we see $dy=\frac{2}{1+x^2}dx$. Further, we have 
\[
\begin{array}{rcl}
\cos (2\arctan x) &=&\displaystyle \cos^2(\arctan x)- \sin^2(\arctan x)\\
&=&  \frac{\cos^2(\arctan x)- \sin^2(\arctan x)}{\cos^2(\arctan x)+ \sin^2(\arctan x)}\\
&=& \displaystyle \frac{1- \tan^2(\arctan x)}{\tan^2(\arctan x)+1}= \frac{1- x^2}{x^2+1}\quad .
\end{array}
\]
Therefore 
\[\sin (2\arctan x)=\pm \sqrt{1-\cos^2(2\arctan x)}= \pm\sqrt{ 1-\frac{(1- x^2)^2}{(x^2+1)^2} }= \frac{2x}{x^2+1}\quad .
\]
To summarize:
\[
\begin{array}{rcl}
dy&=&\frac{2}{x^2+1}dx\\
\cos y&=& \frac{1- x^2}{x^2+1} \\
\sin y&=&\frac{2x}{x^2+1}\quad .
\end{array}
\] 

\subsubsection{Examples of Euler integrals}
\begin{problem}
Integrate 
\[
\int \sqrt{x^2+1}dx\quad .
\]
\end{problem}
\begin{solution}
We carry out the substitution $t=\sqrt{x^2+1}-x$ described in \ref{eqEulerSub1}. We plug in $\sqrt{x^2+1}=\frac{1}2\left(\frac 1 t +t\right)$, $dx=-\frac12 \left(\frac1{t^2} +1\right)dt$ to get
\[
\begin{array}{rcl}
\displaystyle\int \sqrt{x^2+1}dx&=&\displaystyle-\int  \frac14 \left(\frac1t +t\right)\left(\frac 1 {t^2} +1\right)dt=-\frac 1 4\int (\frac{1}{t^3}+2\frac{1}t+t)dt\\
&=&\displaystyle -\frac{1}4 \left(-\frac{t^{-2}}{2}+2\ln|t|+\frac{t^2}2 \right)\\
&=&\displaystyle\frac{1}{8} \left(\frac{1}{(\sqrt{x^2+1}-x)^2}- (\sqrt{x^2+1}-x)^2\right)-\frac12 \ln (\sqrt{x^2+1}-x)
\end{array}
\]
The expression $\frac{1}{(\sqrt{x^2+1}-x)^2}- (\sqrt{x^2+1}-x)^2$ can be simplified further.  
\[\begin{array}{rcl}
\displaystyle\frac{1}{(\sqrt{x^2+1}-x)^2}- (\sqrt{x^2+1}-x)^2&=&\displaystyle
\frac{(\sqrt{x^2+1}+x)^2}{(\sqrt{x^2+1}-x)^2  (\sqrt{x^2+1}+x)^2 }- (\sqrt{x^2+1}-x)^2 \\&=&\displaystyle \frac{(\sqrt{x^2+1}+x)^2}{\underbrace{((\sqrt{x^2+1})^2-x^2)^2}_{=1} }- (\sqrt{x^2+1}-x)^2 \\
&=&\displaystyle 4x\sqrt{x^2+1}
\end{array}
\]
The expression $\ln (\sqrt{x^2+1}-x)$ can be transformed to 
\[
\ln \left(\frac{(\sqrt{x^2+1}-x)(\sqrt{x^2+1}+x)}{\sqrt{x^2+1}+x}\right)=\ln \left(\frac{1}{\sqrt{x^2+1}+x}\right)= -\ln (\sqrt{x^2+1}+x)
\]
Finally, we get 
\[
\begin{array}{rcl}
\displaystyle \int \sqrt{x^2+1}dx&=&\frac{1}2\left(x\sqrt{x^2+1}+\ln (\sqrt{x^2+1}+x) \right)\quad .
\end{array}
\]
\end{solution}
\begin{problem}
Integrate 
\[
\int \frac{dx}{x+\sqrt{x^2+x+1}}\quad .
\]
\end{problem}
%\begin{solution}
%Substitute first  $x=t-1/2$, $dx=dt$, and then $t=\frac{\sqrt{3}}2 u$, $dt=\frac{\sqrt{3}}2 du$:
%\[
%\begin{array}{rcl}
%\displaystyle\int \frac{dx}{x+\sqrt{x^2+x+1}}&=&\displaystyle \int \frac{dt}{t-1/2+\sqrt{t^2+3/4}} =\\
%&=&\displaystyle \int \frac{\frac{\sqrt{3}}2du}{ \frac{\sqrt{3}}{2}u-1/2+\frac{\sqrt{3}}{2}\sqrt{u^2+1}}= \int \frac{du}{ u-\frac{\sqrt{3}}{3}+\sqrt{u^2+1}}\\ \quad .
%\end{array}
%\]
%According to recipe, we substitute $\sqrt{u^2+1}= u+s$ as computed in \ref{eqEulerSub1}, to get $\displaystyle du=\left(-\frac{1}{2s^2}-2\right) ds$, $\displaystyle \sqrt{u^2+1}=\frac{1}{2s}-s$, $u=\displaystyle\frac{1}{2s}-2s$:
%\[
%\begin{array}{rcl}
%\displaystyle
%\int \frac{dx}{x+\sqrt{x^2+x+1}}
%&=&\displaystyle \int \frac{\left(-\frac{1}{2s^2}-2\right) ds}{ \frac{1}{2s}-2s-\frac{\sqrt{3}}{3}+\frac{1}{2s}-s}\\
\quad .
%\end{array}
%\]
%\end{solution}
\begin{problem}
Integrate 
\[
\int \frac{xdx}{(1-x^3)\sqrt{1-x^2}}\quad .
\]
\end{problem}
\subsubsection{Examples of trigonometric integrals}
Integrate 
\[
\int \frac{dx}{2\sin x-\cos x +5}\quad .
\]
\begin{solution}
Set $x=2\arctan t$. Therefore 
\[
\begin{array}{rcl}
\displaystyle \int \frac{dx}{2\sin x-\cos x +5}&=&\displaystyle \int \frac{2dt}{(1+t^2)\left(2\frac{2t}{t^2+1}- \frac{(-t^2+1) }{t^2+1}+5\right)}= \int \frac{dt}{3t^2+2t+2}
\end{array}
\]
We substitute $t=z-1/3$. Therefore $dt=dz$ and $3t^2+2t+2=3z^2-2z+1/3+2z-2/3+2=3z^2+5/3= 5/3(9/5z^2+1)= 5/3((3/\sqrt{5}z)^2+1)$. We substitute $3/\sqrt{5}z=w$, $dt=dz=\sqrt{5}/3dw$. Finally we get 
\[
\begin{array}{rcl}
\displaystyle \int \frac{dt}{3t^2+2t+2}&=& \displaystyle \int \frac{\frac{\sqrt{5}}{3}dw}{\frac{5}{3}(w^2+1)}= \frac{\sqrt{5}}{5}\arctan w+C\\
&=&\displaystyle \frac{\sqrt{5}}{5}\arctan\left( \frac{\sqrt{5}}{5}(3t+1) \right)+C\\
&=&\displaystyle \frac{\sqrt{5}}{5}\arctan \left(\frac{3\tan \left(\frac{x}{2}\right)+1}{\sqrt{5}}\right)+C\quad .
\end{array}
\]
\end{solution}
\subsubsection{Exercises}
\begin{problem}
Solve the integral. Illustrate each step of your solution.
\[
\int \frac{1}{\cos x}dx\quad .
\]
Hint: you may  1) solve this problem with the standard method from class: substitute $x=2 \arctan t$ 2)  multiply top and bottom by $\cos x$, use $\cos x dx = (\sin x)' dx = d\sin x$ 3) use any other method you like to solve this problem.
\end{problem}
\begin{problem}~
\begin{enumerate}
\item
\[
\int \frac{1}{\sqrt{x^2+1}}dx\quad .
\]
Carry out the Euler substitution: $x=\frac12\left(\frac{1}{t}- t\right)$. You do not need to solve the integral.
\item
\[
\int \frac{1}{\sqrt{x^2+1}}dx\quad .
\]
Carry out the substitution: $x=\frac12\left(e^s- e^{-s}\right)$. You do not need to solve the integral.
\item Solve the integral 
\[
\int \frac{1}{\sqrt{x^2+1}}dx\quad .
\]
\end{enumerate}
\end{problem}

\begin{problem}
Solve the integral. 
\[
\int \frac{1}{(1+x^2)^2}dx
\]
\end{problem}

\begin{problem}
Solve the integral. Illustrate each step of your solution.
\begin{enumerate}
\item
\[
\int \frac{1}{(1 + \cos x)^2}dx
\]
\item
\[
\int \frac{1}{(2+\cos x)^2}dx
\]
\end{enumerate}
\end{problem}
\begin{problem}
Solve the integral. Illustrate each step of your solution.
\begin{enumerate}
\item
\[
\int \frac{1}{(1 + \sqrt{2x^2+x+1})}dx
\]
\item
\[
\int \frac{1}{(1 + \sqrt{2x^2+x+1})^2}dx
\]
\end{enumerate}
\end{problem}
\section{Application of integration.}
\subsection{Volumes of solids of revolution.}
A solid of revolution is the set of points obtained by rotating a 2-dimensional set of points lying inside a plane $P$ around an axis lying in the plane $P$.

Example. \label{itemExampleFigureRotationalSolid}
%\multido{\rA=\deltaX+\deltaX}{10}{%
%\pstIIIDCylinder[fillstyle=none, linecolor=gray, increment=20, fillcolor=gray](0,0,0){\rA\space \deltaX\space add}{\rA\space sqrt}
%\pstIIIDCylinder[fillstyle=none, linecolor=gray, increment=20, fillcolor=gray](0,0,0){\rA}{\rA\space sqrt}
%}%

%\psSolid[action=draw*,h={\rA\space\rA\space mul},r1={\rA\space\deltaX\space add}, r0={\rA},object=anneau](0,0,0)

\optionalDisplay{

\psset{xunit=1.5cm,yunit=1.5cm}
\begin{pspicture*}(-0.3,-0.5)(4,4.5)
%\psset{lightsrc=80 30 30,viewpoint=100 45 30 rtp2xyz,Decran=110,linewidth=0.2pt}
\psline[linecolor=gray]{->}(0,0)(3.5,0) % x-axis
\psline[linecolor=gray]{->}(0,0)(0,3.5) % z-axis
\rput[l](3.5,0){$x$}
\rput[b](0,3.5){$z$}
\pscurve[linecolor=red]{->}(0.1,3)(0.2, 3.1)(0, 3.2)(-0.2,3.1 )(-0.05,3)
\pscustom[linewidth=2pt, linecolor=blue, fillstyle=solid,fillcolor=lightgray]{
\psplot{3}{0.3}{1 x div} 
\psplot{0.3}{3}{2.71828 -1 x mul exp}
\closepath
%\pscurve(0.3, 3.33) (0.3, 1.349858808) 
}
\pscustom[linewidth=0.5pt, linecolor=blue, fillstyle=solid,fillcolor=red]{
\psline(2, 0.135335283)(2.1,0.135335283)(2.1,0.5)(2,0.5)
\closepath
%\pscurve(0.3, 3.33) (0.3, 1.349858808) 
}
\rput(0.3,0){\psline(0,-0.03)(0,0.03)}
\rput(3,0){\psline(0,-0.03)(0,0.03)}
\rput[t](0.3,-0.1){$a$}
\rput[t](3,-0.1){$b$}
\rput(1.6,1){$z=f_2(x)$}
\rput[t](0.6,0.3){$z=f_1(x)$}
\end{pspicture*}
\psset{xunit=1cm,yunit=1cm}
\begin{pspicture*}(-4,-3)(4,4.5)
%\psset{lightsrc=80 30 30,viewpoint=100 45 30 rtp2xyz,Decran=110,linewidth=0.2pt}
\psset{lightsrc=50 20 20,viewpoint=100 45 30 rtp2xyz,Decran=110,linewidth=0.2pt}
\newcommand{\deltaXcustom}{0.1}
\newcommand{\rB}{3.3\space\rA\space sub}
\newcommand{\innerCycleX}{
%\pstIIIDCylinder[fillstyle=solid, linecolor=gray, increment=30, fillcolor=green]
\pstThreeDEllipse[fillstyle=solid, fillcolor=lightgray](0,0,{1\space\rB\space div})(\rB\space\deltaXcustom\space add,0,0)(0,\rB\space\deltaXcustom\space add,0)
\pstThreeDEllipse[fillstyle=solid](0,0,{1\space\rB\space div})(\rB,0,0)(0,\rB,0) 
%
\pstIIIDCylinder[fillstyle=none, linecolor=gray, increment=30, fillcolor=green]
(0,0,{2.71828\space -1\space \rB\space mul\space exp})
{\rB\space \deltaXcustom\space add}
{1 \space \rB\space div\space 2.71828\space -1\space \rB\space mul\space exp \space sub }
\pstIIIDCylinder[fillstyle=none, linecolor=gray, increment=30, fillcolor=white]
(0,0,{2.71828\space -1\space \rB\space mul\space exp})
{\rB}
{1 \space \rB\space div\space 2.71828\space -1\space \rB\space mul\space exp \space sub }
}
\multido{\rA=0.3+\deltaXcustom}{14}{%
\innerCycleX
}%
\pstThreeDEllipse[fillstyle=solid, fillcolor=red](0,0,{1\space 2\space div})( 2\space\deltaXcustom\space add,0,0)(0,2\space\deltaXcustom\space add,0)
\pstThreeDEllipse[fillstyle=solid, fillcolor=white](0,0,{1\space 2\space div})(2,0,0)(0,2,0) 
\pstIIIDCylinder[fillstyle=none, linecolor=red, increment=1, fillcolor=green]
(0,0,{2.71828\space -1\space 2\space mul\space exp})
{2\space \deltaXcustom\space add}
{1 \space 2\space div\space 2.71828\space -1\space 2\space mul\space exp \space sub }
\pstIIIDCylinder[fillstyle=none, linecolor=red, increment=3, fillcolor=white]
(0,0,{2.71828\space -1\space 2\space mul\space exp})
{2}
{1 \space 2\space div\space 2.71828\space -1\space 2\space mul\space exp \space sub }
%
\multido{\rA=1.4+\deltaXcustom}{17}{%
\innerCycleX
}%
\defFunction[algebraic]%
{etoMinusT}(t)
{t}{0 }{e^(-t)}
\psSolid[object=courbe,r=0,
range=0.3 2.8,
linecolor=blue,
linewidth=0.03,
resolution=360,
function=etoMinusT]%
\defFunction[algebraic]%
{oneOverT}(t)
{t}{0 }{1/t}
\psSolid[object=courbe,r=0,
range=0.3 2.8,
linecolor=blue,
linewidth=0.03,
resolution=360,
function=oneOverT]%

\psSolid[object=line, linecolor=blue,
args=0.3 0 0.740818221 0.3 0 3.33]
\psSolid[object=line, linecolor=blue,
args=2.8 0 0.060810063 2.8 0 0.357142857]
\axesIIID[](-1,-1,-1)(4,4,4)
\end{pspicture*}
} %optionalDisplay end
For the purposes of these lectures, we introduce the notion of a ``curved trapezoid''. A figure in the $x,z$ plane is called a ``curved trapezoid'' if it is enclosed by two functions $f_1(x), f_2(x)$ in the interval $[a,b]$ with $f_1(x)<f_2(x)$ as $x$ varies in the interval $[a,b]$. 

In other words, a ``curved trapezoid'' is a figure that can be given as the set of points $(x,z)$ for which $x\in [a, b]$ , $z\in [f_1(x), f_2(x)]$. A curved trapezoid is drawn in the preceding item.

The term ``curved trapezoid'' is not standard, and we use it only for the purpose of the current section.

Every ``well behaved'' figure can be subdivided into curved trapezoids. In our course, we shall only consider such ``well-behaved'' figures.

Let $F$ be the solid of revolution obtained by rotating a curved trapezoid $x\in[a,b], z\in [f_1(x), f_2(x)]$, $f_1(x)<f_2(x)$ around the $z$ axis, as shown on the preceding figure. Then the volume of $F$ is computed by

\importantFormula{\label{eqVolumeRotationalSolid}
volume(F) = \int\limits_{a}^b 2\pi x (\underbrace{f_1(x)-f_2(x)}_{=: h(x)}) dx=  \int\limits_{a}^b 2\pi x h(x) dx\quad .
}

A motivation for the above computation may be given as follows. The volume of the ``cylindrical shell'' figure enclosed between an inner cylinder of radius $r$ and an outer cylinder of radius $r+\Delta$ and height $h$ equals the difference of the volumes of the two cylinders 
\[
h(\pi (r+\Delta)^2-\pi r^2)=h\pi (\cancel{r^2}+2r\Delta+\Delta^2-\cancel{r^2})=h\pi (2r\Delta+\Delta^2)\quad .
\]

\optionalDisplay{
\psset{xunit=2cm,yunit=2cm}
\psset{lightsrc=50 20 20,viewpoint=100 45 30 rtp2xyz,Decran=110,linewidth=0.2pt}

\begin{pspicture*}(-1,-1)(1,1)
\defFunction[algebraic]{innerC}(t)
{cos(t)}{sin(t)}{}
\defFunction[algebraic]{outerC}(t)
{1.1*cos(t)}{1.1*sin(t)}{}
\psSolid[object=prisme,h=0.6,
fillcolor=red,
resolution=30,
base=0 6.3 {outerC} CourbeR2+
6.3  0 {innerC} CourbeR2+](0,0,-0.6)
\psline[linecolor=black, linewidth=0.3pt, arrows={|-|}] (0,0.4) (0.55,0.4)
\psline[linecolor=black, linewidth=0.3pt, arrows={|-|}] (0,0.7) (0.607,0.7)
\rput(0.25, 0.52 ){\tiny $r$}
\rput(0.30, 0.82 ){\tiny $r+\Delta$}
\psline[linecolor=black, linewidth=0.3pt, arrows={|-|}] (0.7,0) (0.7,-0.3)
\rput[l](0.75, -0.15 ){\tiny $h$}
\end{pspicture*}
}

Let $N$ be a large integer and set $\Delta:= \frac{b-a}{N}$. Split the interval $[a,b]$ into $N$ equal segments $[a,a+\Delta],[a+\Delta(x),a+2\Delta],\dots, [b-\Delta, b]$. The length of each segment is $\Delta$. Then the volume of our solid of revolution is approximated by the sum of the volumes of the cylindrical shells of inner radius $a+ k\Delta $, outer radius $a+(k+1)\Delta$, and height $h(a+k\Delta)$, where we have set 
\[
h(x)\eqdef f_2(x)-f_1(x)\quad .
\]
In other words, the volume of a solid of revolution is approximated by
\[
\begin{array}{l}
\displaystyle\sum_{k=0}^{k=N-1} \underbrace{ \underbrace{h(a+k\Delta)}_{\mathrm{height}}(\underbrace{\pi (\underbrace{(a+k\Delta)+\Delta}_{ \mathrm{rad.~outer~cylinder}} ) ^2}_{\mathrm{area~base~outer~cyl.}}-\underbrace{\pi (\underbrace{a+k \Delta}_{\mathrm{rad.~inner~c.}})^2}_{\mathrm{area~base~inner~c.}})}_{\mathrm{volume~k^{th}~cylindrical~shell}} \\
=\displaystyle \underbrace{\sum_{k=0}^{k=N-1} 2\pi(a+k\Delta)  h(a+k\Delta) \Delta }_{\mathrm{approximates~} \int\limits_{a}^b 2\pi x h(x) dx }  + \Delta \left(\underbrace{\sum_{k=0}^{k=N-1} \pi  h(a+k\Delta)\Delta}_{\mathrm{approximates~} \int\limits_{a}^b h(x) dx }\right)
\end{array}
\]
As we subdivide into finer intervals, i.e., $N$ tends to zero, $\Delta$ (as a function of $N$) tends to zero, and the first summands tends to $\int\limits_{a}^b 2\pi x h(x) dx $. On the other hand, the second multiplicand of the second summand approximates $\int\limits_{a}^b h(x) dx $, in particular it is bounded. Therefore the entire second summand equals $\Delta$ times a bounded quantity, and therefore approaches zero as $\Delta$ tends to zero.


\textbf{Remark.} The above motivation may be regarded as a proof of the volume formula. The main reason we do not call the above motivation ``a proof'' is that we have not given a definition of volume. \textbf{Optional material, (!).} Here is an informal brief sketch  of a definition of volume. If a figure $X$ can be represented as a set of the form $X= \{(x,y,z)| x\in [a,b], y\in [f_1(x), f_2(x)], z\in [g_1(x,y),  g_2(x,y)]\}$, its volume can be defined as 
\[
\int\limits_a^b \left(\int\limits_{f_1(x)}^{f_2(x)}\left(\int\limits_{g_1(x,y)}^{g_2(x,y)} 1 ~ ds \right)dt \right)dv \quad .
\]
By allowing finite sums of figures of the above kind, and by allowing subdivision of figures into components of the above kind, we can define volume of a much wider set of figures. This set of figures includes all 3-dimensional figures of practical significance to  the natural sciences. 

However, there are a lot of important details to take care of - for example, what do we do if the figures of the above form intersect along a (2-dimensional) wall, and how do these integrals behave when we re-parametrize the coordinates system. In order to handle these details smoothly, as well as to define volume of figures that cannot be represented with finitely many operations by  figures of the above form,  we need to define volume as follows. (Disclaimer: non-defined mathematical terms follow. The terms are given as starting keywords for students who have additional interest in the subject and would like to google them.) The volume of a \emph{measurable} figure $X$ equals the \emph{infimium of the of the sums of volumes of (possibly infinitely many) rectangular parallelepipeds whose union covers $X$}. The words in the preceding sentence express the simplest idea of measuring volume: namely, approximate volume by increasingly finer rectangular parallelepipeds. However, molding this simple idea into a precise mathematical definition, although not difficult or conceptually hard (however somewhat lengthy, as the preceding ``complicated'' words indicate), is outside of the scope of the current course. We note that one area of mathematics in which a precise definition of volume is given is ``Measure Theory''.
\subsubsection{Strategy for computing the volume of a solid of revolution}

A typical problem for finding the volume of a solid of revolution will ask you to find the volume of a figure obtained by rotating a figure in the $(x,z)$ plane around an axis. The actual letters in the problem might be different from $(x,z)$. The axis of rotation might be the $x$-axis, the $z$-axis, or any other axis given by the equation of any line. The figure in the $(x,z)$ plane will usually be given as the figure enclosed by two or more curves in the plane.

If the axis of rotation does not pass through $(0,0)$, we should change the coordinate system so that the axis of rotation passes through zero. This is easy to do. Suppose the axis of rotation passes through some point $(s,t)$. Then if the change the coordinate system by setting $u\eqdef x-s, v\eqdef z-t$, then when $(x,z)=(s,t)$, we would have that $(u,v)=(0,0)$. In other words, we may replace everywhere $x=u+s$, $z=v+t$, and work with the new coordinates $(u,v)$.

Suppose now the axis of rotation passes through $(0,0)$, but is neither the $x$ nor the $z$ axis. Suppose in addition the axis of rotation passes through the point $(a,b)$, $a\neq 0$. Then the axis of rotation passes through the point $(\frac{a}{\sqrt{a^2+b^2}},\frac{b}{\sqrt{a^2+b^2}})$ as well (this is one of the two points on the line whose distance to $(0,0)$ is one). Then we can change the coordinate system by 
\begin{equation}\label{eqRotateXZtoUV}
\begin{array}{rcl}
u &=&\frac{1}{\sqrt{a^2+b^2}}\left( bx-az\right)\\
v &=& \frac{1}{\sqrt{a^2+b^2}}\left( ax+bz\right)\quad .
\end{array}
\end{equation}
If $(x,y)= (\frac{a}{\sqrt{a^2+b^2}}, \frac{b}{ \sqrt{a^2 +b^2}})$, it follows that $(u,v)=(0,1)$, and so the axis of rotation passes through the point $(0,1)$ in the $(u,v)$-coordinates, i.e., the axis of rotation is the $v$-axis. It is important that the substitution \eqref{eqRotateXZtoUV} preserves volumes. This does not happen for all linear substitutions (transformations); the linear transformations for which this happens are called ``volume preserving''. 

One way to motivate the fact that substitution \eqref{eqRotateXZtoUV} preserves volumes (without actually proving it), is to show that the transformation \eqref{eqRotateXZtoUV} is a rotation. The lengths $\sqrt{a^2+b^2}$, $|a|$ and $|b|$ form a right-angle triangle, and therefore $\frac{a}{\sqrt{a^2+b^2}}$ is the cosine of the angle $\varphi$ indicated on the figure below and $\frac{b}{\sqrt{a^2+b^2}}$ is the sine. If we set $\mathbf e_1\eqdef(1,0)$, $\mathbf e_2\eqdef(0,1)$, then a vector in the $(x,z)$ plane is given by $x\mathbf e_1+z\mathbf e_2$. If we set $\mathbf v_1\eqdef (\cos\varphi, -\sin \varphi), \mathbf v_2\eqdef (\sin \varphi, \cos \varphi)$, then a vector in the plane may be alternatively given as $u\mathbf v_1+ v\mathbf v_2$. Therefore if we want to change coordinate system so that the axes are aligned along $\mathbf v_1, \mathbf v_2$, we need to have that 
\[
(x, z)=x\mathbf e_1+z\mathbf e_2= u \mathbf v_1 +v \mathbf v_2=( u\cos \varphi+ v\sin\varphi, -u\sin \varphi+v\cos\varphi)\quad , 
\]
and therefore 
\[
\left|\begin{array}{rcl}
x&=& u\cos \varphi+ v\sin\varphi\\
z&=&  -u\sin \varphi+v\cos\varphi\quad .
\end{array}\right.
\]
Solving this system for $u,v$, we get that 
\[
\left|\begin{array}{rcl}
u&=&  x\cos \varphi-z\sin\varphi\\
v&=& x\sin \varphi+z\cos\varphi\quad .
\end{array}\right.
\]
\optionalDisplay{
\psset{xunit=3cm,yunit=3cm}
\begin{pspicture*}(-2,-2)(3,2)
\psline[linecolor=gray]{->}(-1.5,0)(1.5,0) % x-axis
\psline[linecolor=gray]{->}(0,-1.5)(0,1.5) % y-axis
\rput[l](1.5,0){$x$}
\rput[b](0,1.5){$z$}
\rput[bl](0.03,1.03){$(0,1)$}
\rput[bl](1,-0.15){$(1,0)$}
\rput[c](0,1){$\bullet$}
\rput[c](1,0){$\bullet$}

\psplot[linestyle=dotted, arrows={<-}]{0}{0.5}{1 x x mul sub sqrt} 
\psplot[linestyle=dotted, arrows={->}]{0.866}{1}{1 x x mul sub sqrt -1 mul} 

\psplot[linecolor=red]{0}{0.05}{0.01000001 x x mul sub sqrt} 

\rput(0.5, 0){\psplot{-0.1}{0}{ 0.01001 x x mul sub sqrt 1 mul}}
\rput[l](0.53, 0.34){$\cos\varphi$ $= \frac{a}{\sqrt{a^2+b^2}}$}
\rput[t](0.34, -0.03){$\sin\varphi$}

\psline(0,0)(0.5,0.866)
\psline(0,0)(0.866, -0.5)
\rput[c](0.5,0.866){$\bullet$}
\rput[c](0.866,-0.5){$\bullet$}

\rput(1.3,1){$( \sin \varphi, \cos \varphi) = \left( \frac{a}{\sqrt{a^2+b^2}}, \frac{b}{\sqrt{a^2+b^2}}\right)$}
\rput(1.3,-0.6){$(\cos\varphi, -\sin \varphi)= \left( \frac{b}{\sqrt{a^2+b^2}}, \frac{-a}{\sqrt{a^2+b^2}}\right)$}
\psline[linestyle=dotted](0.5,0.866)(1,1.74)

\rput(1.1,1.81){$(a,b)$}
\rput[c](1,1.74){$\bullet$}
\rput(0.8, 1.392){\pscurve[linecolor=red]{->}(0.0866, -0.05) (0.224, -0.014)(0.1, 0.174)(-0.124, 0.1866 )(-0.0866, 0.05)}

\psline(0.5,0)(0.5,0.866)
\rput[l](0.01,0.2){$\varphi$}
\end{pspicture*}
} %end optionalDisplay

Substituting $\sin \varphi = \frac{a}{\sqrt{a^2+b^2}}, \sin \varphi = \frac{b}{\sqrt{a^2+b^2}}$ we get the substitution \eqref{eqRotateXZtoUV}. Observation of the figure shows that this transformation in fact corresponds to rotation. As rotations preserve volumes, we have transformed the problem of rotating around an arbitrary axis to a problem of rotating around the vertical axis $u=0$.

A formal definition of rotation is given in the subject of linear algebra, or in more advanced Calculus I courses. Alternatively, a very good explanation may be found at the Wikipedia page

\url{http://en.wikipedia.org/wiki/Rotation_(mathematics)} .

Suppose now we are rotating the figure around the $z$ axis. The next task is to plot the curves in the $(x,z)$-plane that determine the figure.

Once we have plotted the figure, we use vertical lines to split it into ``curved trapezoids'', i.e.,  figures bounded by a line $x=a$, a curve $z=f_1(x)$, a line $x=b$ and a curve $z=f_2(x)$ with $a< b$ and $f_1(x)\leq f_2(x)$ for $x\in [a,b]$. One such figure is illustrated in \ref{itemExampleFigureRotationalSolid}.

If the figure lies entirely on one side of the $z$ axis, we simply apply the volume formula  \eqref{eqVolumeRotationalSolid}. 
\subsubsection{Example problem}
\begin{problem}
Compute the volume of the rotational solid obtained by rotating the figure enclosed between the curves $z=-x+3$, $z= \frac{1}x$.
\end{problem}
\begin{solution}
First we plot the figure and the axis of rotation.

\optionalDisplay{
\psset{xunit=1.5cm,yunit=1.5cm}
\begin{pspicture*}(-3,-2)(4,4.5)
%\psset{lightsrc=80 30 30,viewpoint=100 45 30 rtp2xyz,Decran=110,linewidth=0.2pt}
\psline[linecolor=gray]{->}(-2,0)(3.5,0) % x-axis
\psline[linecolor=gray]{->}(0,-2)(0,3.5) % z-axis
\rput[l](3.5,0){$x$}
\rput[b](0,3.5){$z$}

\psline[linestyle=dotted](-4,3)(3,-4)
\psline[linestyle=dotted](4,3)(-3,-4)

\rput{45}(3,2.1){$u$}
\rput{45}(-2,1.1){$v$}
\rput{45}(-1,0){
\pscurve[linecolor=red]{->}(0,0)(0.2, 0.1)(0, 0.2)(-0.2,0.1 )(-0.05,0)
}
\pscustom[linewidth=2pt, linecolor=blue, fillstyle=solid,fillcolor=lightgray]{
\psplot{0.381966011}{2.618033989}{1 x div} 
\psline(2.618033989, 0.381966011)(0.381966011,2.618033989)
\closepath
%\pscurve(0.3, 3.33) (0.3, 1.349858808) 
}
\rput(1, 1){$\bullet$}
\rput(-0.5, -0.5){$\bullet$}
\psline[linestyle=dotted](-0.5, -0.5)(1,1)
\rput(1.6,1){$z=\frac{1}x$}
\end{pspicture*}
}

Next we change coordinates so that the axis of rotation becomes the $z$ axis. First, we need to change coordinate system so the axis of rotation passes through the origin. The axis of rotation passes through $(x,z)=(-1,0)$ and therefore we can change basis by $y=x+1$, i.e., $x=y-1$. In the new variables, the axis of rotation has equation $z+y=0$. $z+y$ passes through $(-1,1)$, and therefore we may set, according to \eqref{eqRotateXZtoUV}, 
\[
u=\frac{\sqrt{2}}{2}(y+z)\quad,\quad v=\frac{\sqrt{2}}{2}(-y+z)\quad .
\]
Solving for $x$ and $z$, we get that $z=\frac{\sqrt{2}}{2}(u+v)$, $x=y-1=\frac{\sqrt{2}}{2}(u-v)-1$. Our axis of revolution becomes $u=0$. Therefore we would like to express the bounding curves in the form $v=f(u)$ for some function $f$. Therefore the curve $z=\frac{1}{x}$ is expressed in $(u,v)$ as 
\[
\frac{\sqrt{2}}{2}(u+v)= \frac{1}{\frac{\sqrt{2}}{2}(u-v)-1}
\]
We solve by expressing $v$ in terms of $u$:
\[
\begin{array}{l}
\frac{1}{2}(u^2-v^2)-\frac{\sqrt{2}}{2}(u+v)=1 \\
v^2+\sqrt{2}v+2-\sqrt{2}u- u^2=0 \\
v_{1,2}=\frac{-\sqrt{2}\pm \sqrt{2-4(2-\sqrt{2}u- u^2)} }2=
\frac{-\sqrt{2}\pm \sqrt{-6+\sqrt{2}u+ u^2} }2\quad .
\\
\end{array}
\]
The smallest possible value of $u$ is $\frac{3\sqrt{2}}2$, which is also seen on our picture.  The other bounding curve, $x+z-3=0$, becomes $y-1+z-3=\sqrt{2}u -4=0 $,i.e., $u=2\sqrt{2}$. We see that the figure in the $(u,v)$-plane is enclosed by the two functions $\frac{-\sqrt{2}+ \sqrt{-6+\sqrt{2}u+ u^2} }2>\frac{-\sqrt{2}- \sqrt{-6+\sqrt{2}u+ u^2} }2$ and $u\in [\frac{3\sqrt{2}}2, 2\sqrt{2}]$.  Applying the formula \eqref{eqVolumeRotationalSolid}, we get the integral 
\[
\int\limits_{u=\frac{3\sqrt{2}}2}^{2\sqrt{2}} 2\pi u\left(\sqrt{-6+\sqrt{2}u+ u^2}\right)du\quad .
\]
We can solve this integral using Euler substitution, Section \ref{secEulerSubTrigIntegrals}. %This will be done in later versions of the notes.
\end{solution}

\subsection{Length of curves.} \label{secCurveLength}
\index{curve}\index{curve!parametrization} A curve parametrization (or simply curve) is defined as a function from a given interval $[a,b]$ to a vector space over the real numbers. For the purposes of our course, a curve will map the interval $[a,b]$ onto either a two dimensional space (denoted by $\mathbb R^2$), or a 3-dimensional space (denoted by $\mathbb R^3$). Thus, a curve $f:[a,b]\to \mathbb R^2$ is given by the two functions (one for each coordinate):
\[
\begin{array}{rcl}
\gamma:[a,b]&\to& \mathbb R^2\\
\gamma(t)&\eqdef& (f_1(t), f_2(t))
\end{array}\quad .
\]
An alternative and less formal notation for a curve is
\[
\gamma:\left| 
\begin{array}{rcl}
x(t)&=&f_1(t)\\
y(t)&=&f_2(t)\\
\end{array}\right., t\in [a,b]\quad .
\]
In a similar fashion, a curve in the 3-dimensional space is given by 
\[
\gamma:\left| 
\begin{array}{rcl}
x(t)&=&f_1(t)\\
y(t)&=&f_2(t)\\
z(t)&=&f_3(t)\\
\end{array}\right., t\in [a,b]\quad .
\]
This definition is extended to an arbitrary number of variables in the obvious way.

\index{curve!image}The \emph{image of a curve} $f:[a,b]\to \mathbb R^2$ is the set of points in $\mathbb R^2$ of the form $f(t)$ as $t$ runs over all points in the interval $[a,b]$.

Colloquially, as often happens during lectures and scientific talks, we call both the curve parametrizations (which are functions) and their  and images (which are sets of points in the plane) using the word ``curve''. We try to avoid this informality in the present notes.

It is important to distinguish between a curve parametrization and the image of the curve. For example, the curves 
\[
\gamma_1:\left| 
\begin{array}{rcl}
x&=&t^2\\
y&=&t^2\\
\end{array}\right., t\in [0,1]\quad .
\]
\[\gamma_2:\left| 
\begin{array}{rcl}
x(t)&=&t\\
y(t)&=&t\\
\end{array}\right., t\in [0,1]\quad .
\]
both represent the straight segment connecting $(0,0)$ and $(1,1)$, but ``traverse'' the segment at different ``speeds''. Another example is given by
\[
\gamma_1:\left| 
\begin{array}{rcl}
x(t)&=&t\\
y(t)&=&\sqrt{1-t^2}\\
\end{array}\right., t\in [0,1]
\]
and 
\[
\gamma_2:\left| 
\begin{array}{rcl}
x(t)&=&\cos t\\
y(t)&=&\sin t\\
\end{array}\right., t\in [0, \frac{\pi}{2}]\quad .
\]
These two curve parametrizations correspond to a quarter of a circle (which quarter?), however the two curve parametrizations traverse the quarter circle in different directions (which ones?) and at different speeds. 

Images of curves are often given implicitly with a relation between $x$ and $y$. For example, the unit circle is given by 
\[
x^2+y^2=1\quad .
\]
In the previous points, we showed two different curve parametrizations of a quarter circle.

Parametrizing a curve whose image is given by a relation between $x$ and $y$ is a highly non-trivial task. For example, if the relation is of the form $y^2=x^3+px+q$ for parameters $p,q$, the problem of deciding whether there is a point with rational coordinates $(x,y)$ that satisfies the equation is an active research area. There are cryptographic schemes based on results in this area.

In fact, the Euler substitution in Section \ref{secEulerSubTrigIntegrals} is an example of a parametrization of curves of the form $y^2=\pm x^2\pm 1$.

\index{curve!length}Definition of curve length. Let $\gamma$ be given by
\[
\gamma:\left| 
\begin{array}{rcl}
x(t)&=&f_1(t)\\
y(t)&=&f_2(t)\\
\end{array}\right., t\in [a,b]\quad .
\]
Suppose in addition $f_1(t), f_2(t)$ are differentiable. Then the \emph{length} of $\gamma$ is defined as 
\importantFormula{
\label{eqDefLengthCurve}
\int\limits_{a}^b \sqrt{f_1'(t)^2+f_2'(t)^2}dt \quad . 
}
Similarly, the length of a curve sitting in 3-dimensional space is given by 
\[
\int\limits_{a}^b \sqrt{f_1'(t)^2+f_2'(t)^2+f_3'(t)^2}dt \quad . 
\] 

Suppose an image of a curve $C\subset \mathbb R^2$ has two different parametrizations 
\begin{equation}\label{eqDefCurve1}
\gamma_1:\left| 
\begin{array}{rcl}
x(t)&=&f_1(t)\\
y(t)&=&f_2(t)\\
\end{array}\right., t\in [a,b]\quad ,
\end{equation}
\begin{equation}\label{eqDefCurve2}
\gamma_2:\left| 
\begin{array}{rcl}
x(s)&=&g_1(s)\\
y(s)&=&g_2(s)\\
\end{array}\right. , s\in [p,q]\quad ,
\end{equation}
both differentiable, such that both are injective (i.e., every point $(x(t), y(t))$ has exactly one preimage, see Section \ref{secInverseFunctionBasics}). 
\importantFormula{
\begin{array}{c}
\mathrm{If~the~curve~parametrization~is~differentiable~and~injective~~(one~to~one),}\\
\mathrm{~the~length~of~the~image~of~a~curve~}C
\mathrm{~does~not~depend~on~the~curve~parametrization.}
\end{array}
}
We have that $t$ can be expressed as function of $s$, namely, $t(s)= \gamma_1(\gamma_2^{-1}(s))$. Furthermore, as a function of $s$ from $[a,b]$ to $[p,q]$, $t$ is bijective (this follows from the fact that both $\gamma_1$ and $\gamma_2$ are injective).

We will prove the preceding item if $t$ as a function of $s$ is  continuously differentiable.

\begin{proof} (on condition $t(s)$ is continuously differentiable). In order to avoid confusion throughout this proof, we will denote derivatives with respect to $s$ by $\frac{d}{ds}$, and derivatives with respect to $t$ by $\frac{d}{dt}$. 

We claim that $\frac{dt}{ds}(s)$ does not change sign. Suppose the contrary. The endpoints of the interval $[a,b]$ (where $t$ lives) must be mapped to the endpoints $p,q$ of the interval where $s$ lives. Suppose first $t(a)=p<q=t(b)$.  Then there exist points $s_0$ and $s_1$ such that  $\frac{dt}{ds}(s_0)<0$. As $t(s)$ is continuously differentiable, there exists a small interval  $s_0\in [w, z]$ such that $\frac{dt}{ds}(s)<0$. Then by the fundamental theorem of Calculus $t(w)>t(z)$ but $w<z$. By the intermediate value theorem, in the interval $[z,b]$, the function $t(s)$ achieves all values between $t(z)$ and $t(b)=q$. In particular, it achieves the value $t(z)<t(w)<q$. But then $t$ as a function of $s$ achieves the value $t(w)$ at least twice (once at $w$ and once at a point in $[z,q]$). On the other hand, we already saw that $t$ is bijective, and cannot achieve the same value twice. Contradiction. The other case, in which $t(a)=q>p=t(b)$, is treated in a similar fashion.

The length of $C$ is by definition 
\[
\int\limits_{a}^b \sqrt{f_1'(t)^2+f_2'(t)^2}dt \quad . 
\]
Unlike computations of indefinite integrals, where we make variable changes on the fly, when dealing with definite integrals, we may make variable changes only when they are bijective (one-to-one and onto, Section \ref{secInverseFunctionBasics}). By the conditions on the parametrization of the curve we may make a variable change under the definite integral. Suppose first $\frac{dt}{ds}(s)\geq 0$ and therefore $t(a)=p<q=t(b)$. Therefore
\[
\begin{array}{l}
\phantom{=}\displaystyle \int\limits_{t(s)=a}^b \sqrt{\left(\frac{df_1}{dt}(t(s))\right)^2+\left(\frac{df_2}{dt}(t(s))\right)^2}d(t(s)) \\
\displaystyle =  \int \limits_{s=p}^q \sqrt{\frac{df_1}{dt}( t(s))^2+ \frac{df_2}{dt}(t(s))^2}d(t(s))\\
= \displaystyle \int\limits_{s=p}^q \sqrt{\frac{df_1}{dt}( t(s))^2+\frac{df_2}{dt}(t(s))^2} \frac{dt}{ds}ds \\
\displaystyle
\stackrel{\frac{dt}{ds}(s)\geq 0}{=}  \int\limits_{s=p}^q \sqrt{ \left(\underbrace{\frac{dt}{ds}(s) f_1( t(s)) }_{ \mathrm{chain~rule}} \right)^2+ \left( \underbrace{ \frac{ dt}{ds}(s) \frac{df_2}{dt} (t(s))}_{ \mathrm{chain~rule}} \right)^2}ds \\
=\displaystyle \int\limits_{s=p}^q \sqrt{ \left(\frac{d}{ds}\left(f_1(t(s))\right)\right)^2+ \left(\frac{d}{ds}\left(f_2(t(s))\right)\right)^2}ds \\
=\displaystyle \int\limits_{s=p}^q \sqrt{ \frac{ dg_1}{ ds}( s)^2 + \frac{dg_2}{ds}(s)^2}ds,
\quad
\end{array}
\]
where in the very last equality we use the definition of $t$ as a function of $s$ and \eqref{eqDefCurve2}. This is exactly what we wanted to prove. The case when $t(a)=q>p=t(b)$ is handled similarly.
\end{proof}

A motivation for the definition may be given as follows. Let $[a,b]$ be subdivided into $N$ intervals $[a,a+\Delta ], \dots, [a+k\Delta, a+(k+1)\Delta],\dots, [b-\Delta, b]$, $\Delta=\frac{b-a}{N}$. Then the straight segment connecting the point $(x,y)=(f_1(t), f_2(t))$ with $(f_1(t+\Delta), f_2(t+\Delta))$ has length $\sqrt{(f_1(t)-f_1(t+\Delta))^2 + (f_2(t)-f_2(t+\Delta))^2}= \sqrt{\left(\frac{(f_1(t)-f_1(t+\Delta)}{\Delta}\right)^2 + \left(\frac{(f_2(t)- f_2(t+\Delta)}{\Delta} \right)^2}\Delta$. Summing up over all intervals, we get that the piecewise linear curve approximating our smooth curve has length
\[
\sum_{k=1}^{N-1} \sqrt{\left(\frac{(f_1(a+k\Delta)-f_1(a+(k-1)\Delta)}{\Delta}\right)^2 + \left(\frac{(f_2(a+k\Delta)- f_2(a+(k-1)\Delta)}{\Delta} \right)^2}\Delta\quad .
\]
As the number of intervals $N$ tends to $\infty$, $\Delta$ approaches 0, and by the definition of derivative, the radical term tends uniformly to $\sqrt{f_1'(t)^2+f_2'(t)^2}$. Therefore the entire sum approximates the integral \eqref{eqDefLengthCurve}.

\optionalDisplay{
\psset{xunit=1cm,yunit=1cm}
\begin{pspicture}(-1,-1)(4,4.3)
\psaxes[ ticks=none, arrows={->}](0,0)(4,4)
\rput[b](0,4.1){\tiny$y$}
\rput[b](4.1,0){\tiny$x$}

\psplot[linecolor=blue, plotstyle=curve]{0.2}{4} { x 2 div x 180 mul  sin 1 add  mul 1 add x 3 div  sub   }  %{ x x mul 4 div x 180 mul  sin 1 add  mul   } 
\psplot[linecolor=black!50, plotpoints=11,plotstyle=line]{0.2}{4}{ x 2 div x 180 mul  sin 1 add  mul 1 add x 3 div  sub   } 
\psline[linecolor=red!50](2.1,1.674467844)(2.1,2.650886477)(2.48,2.650886477)
\rput[t](1.9,2.3){\tiny$dy$}
\rput[t](2.28,2.85){\tiny$dx$}


\rput[t](1,1.8){\tiny$(x(t), y(t))$}
\pscurve[arrows={->}, linestyle=dotted](1,1.8)(1.3, 1.5)(2.1,1.674467844)
\pscurve[arrows={->}, linestyle=dotted](2,4)(2.5, 3.55)(2.48,2.650886477)

\rput[b](2,4){\tiny$(x(t+\Delta), y(t+\Delta))$}
%\pscurve[linecolor=blue](0.2,1)(0.4,0.9)(0.8,0.5)(1.2,0.3)(1.6,1.5)(2, 0.5)(2.4, 2)(3.2, 1.5)(3.6, 1.4)(4, 2)

\end{pspicture}
}

\section{A bit of differential equations}
\subsection{Some terminology and background}
A differential equation is an equation that contains at least one derivative (first, second, third,...) of an unknown function. 

The \emph{order} of a differential equation is the order of the highest derivative that appears in the equation.

An example of a differential equation is
\begin{equation}\label{eqDFQsimplestExample}
\frac{dP}{dt} = k P\quad ,
\end{equation}
where $t$ is a variable, $k$ is a constant and $P(t)$ is a function.

Let us interpret $P$ as population size as a function of the time $t$, and $k$ as some natural rate of growth constant. If $k$ is positive (and the population $P$ is positive), so is $\frac{d P}{dt}$. As the first derivative of $P$ is positive, $P$ will increase with time, at an ever growing rate. We can plug in the function $P(t)= e^{k t}$ and verify that it satisfies the equation.

Differential equations are used to model processes in the natural sciences. For example, \ref{eqDFQsimplestExample} models ideal population growth with infinite resources and unlimited environment - the increase in population is proportional to the size of the population.

A more complicated example is the logistic differential equation 
\[
\frac{d P }{dt} = k P(1- \frac{P}{M})\quad.
\]
Here, $k$, $t$, $P$ have the same interpretation as before, and $M$ is the carrying capacity of the environment. In this model, while $P$ is smaller than the carrying capacity $M$,  $\frac{dP}{dt}$ will be positive, and the population will increase with time. However, if the population is larger than the carrying capacity $M$, $\frac{d P }{dt}$ will be negative and the population will decrease.

Another example of a differential equation is the motion of a spring
\[
m\frac{d^2 y (t)}{dt^2}= -k y,
\]
where $m$ and $k$ are constants ($m$ stands for mass and $k$ for the spring's constant). Here, we recall the notation $\frac{d^2 y (t)}{dt^2}= y''(t)$ means second derivative.


Differential equations may have \emph{initial conditions}, i.e., we may be given the value of the unknown function or its derivatives in one or more points. For example, in \eqref{eqDFQsimplestExample}, we may add the initial condition $P(t_0)= 10$, to say that at time $t=t_0$, $P(t)=10$. 

If we are given initial conditions $y(x_0)=y_0$, we say that we have  an \emph{initial condition differential equation}.

Initial conditions are critical for a solution of a differential equation. For example, if $P(t_0)=0$, \eqref{eqDFQsimplestExample} has only solution $P(t)=0$, buf if $P(t_0)=10$, \eqref{eqDFQsimplestExample} has only solution $P(t)=10  e^{k(t-t_0)}$.
\subsection{Direction fields}
Suppose we have a differential equation of the form 
\begin{equation}\label{eqDFQwithDirectionFlow}
\frac{dy}{dx}= F(x,y)\quad.  
\end{equation}

At each point $(x_0,y_0)$, let us plot a segment of the line with slope $F(x_0,y_0)$ passing through $(x_0,y_0)$. The resulting picture is called a direction field. 

Examples.

\optionalDisplay{
\psset{xunit=1cm,yunit=1cm}
\SpecialCoor
\begin{pspicture}(-5,-5)(5,5)
\psaxes{<->}(0,0)(-5,-5)(5,5)
\rput (5,5){The direction field $\frac{dy}{dx}=xy$}
  \psset{arrows=->}
  \multido{\ra=-4+0.5}{17}{%
    \multido{\rb=-4+0.5}{17}{%
      \pstVerb{/xC \ra\space def
               /yC \rb\space def
               /F xC  yC mul \space def
}
\psdot[linecolor=red!60](! xC yC)
\psline[linecolor=blue](! xC F ATAN 57.295 mul cos 0.2 mul sub yC F ATAN 57.295 mul sin 0.2 mul sub)(! xC F ATAN 57.295 mul cos 0.2 mul add yC F ATAN 57.295 mul sin 0.2 mul add )
}}
\end{pspicture}

\begin{pspicture}(-5,-5)(5,5)
\psaxes{<->}(0,0)(-5,-6)(5,5)
\rput (5,5){The direction field $\frac{dy}{dx}=x+y$}
  \psset{arrows=->}
  \multido{\ra=-4+0.5}{17}{%
    \multido{\rb=-4+0.5}{17}{%
      \pstVerb{/xC \ra\space def
               /yC \rb\space def
               /F xC  yC add \space def
}
%\psline[linecolor=blue](! xC  yC )(! xC yC)
\psdot[linecolor=red!60](! xC yC)
\psline[linecolor=blue](! xC F ATAN 57.295 mul cos 0.2 mul sub yC F ATAN 57.295 mul sin 0.2 mul sub)(! xC F ATAN 57.295 mul cos 0.2 mul add yC F ATAN 57.295 mul sin 0.2 mul add )
}}
\end{pspicture}
}%optionalDisplay

$\frac{dy}{dx}= F(x,y)$ has different solutions depending on the initial condition given by the $y(x_0)=y_0$ starting point $(x_0, y_0)$. A solution of a differential equation of the form \eqref{eqDFQwithDirectionFlow} corresponds to the trajectory of a point ``flowing'' along the direction field. If drawn with increasing precision, the blue segments representing the direction field begin to resemble smooth curves; those curves indeed approximate solutions of the differential equation.

Euler's method for approximating solutions. Fix a small ``approximation step'' $h$. Let $x_n:=x_0+nh$ be the point at distance $n$ times our small approximation step from $x_0$. Suppose the initial condition differential equation problem
\[
\frac{dy}{dx}= F(x,y)\quad.  
\]
with initial conditions $y(x_0)=y_0$  has a solution $y(x)$. Then approximate values for $y_n\approx y(x_n)$ are given by
\begin{equation}\label{eqEulerApproxDFQ}
y_n \eqdef y_{n-1} + h F(x_{n-1}, y_{n-1})\quad .
\end{equation}
A motivation for Euler's method can be given as follows. At a point $x$, for a very small interval $(x,x+h)$, the function $y(x)$ is approximately linear, and is approximated by $y(x+h)\approx y(x)+ h y'(x) $. On the other hand, $y'(x)$ equals $F(x, y(x))$. Now if we have already approximated $y(x_{n-1})\approx y_{n-1}$, we known $y'(x_{n-1}) \approx F(x_{n-1}, y_{n-1})$ and  we can approximate $y(x_{n-1}+h)\approx y_n $ by \eqref{eqEulerApproxDFQ}.

Euler's method has a number of drawbacks, both computationally and algebraically, and is not used in practice. However, it illustrates the main idea of approximating solutions to differential equations: approximate at a point $x$ the value of $y(x+h)$ (as a function of $h$) by a well-known function in $h$ (linear, quadratic, third power, fourth power,\dots), move along the approximation for a very short distance $h$, and recompute. Without being an expert in differential equations, I may speculate that modern computer algorithms  are still based on (some complicated variation of) this simple idea.

The algebraic problems arising form Euler's method stem from the fact that $F(y,x)$ may tend to $\pm\infty$. For example, consider the direction field given by $\frac{dy}{dx}= -\frac{x}{y}$. Euler's method will not tell you what to do as $y$ approaches $0$. In fact, we will soon be able to compute that solutions of the differential equation  $\frac{dy}{dx}= -\frac{x}{y}$ for $y>0$ behave locally likes arcs of circles whose radii depending on the starting point $(x_0, y_0)$.

\optionalDisplay{
\begin{pspicture}(-6,-1)(6,6)
\psaxes{<->}(0,0)(-6,-1)(6,6)
\rput (5,5){The direction field $\frac{dy}{dx}=-\frac{x}{y}$}
  \psset{arrows=->}
  \multido{\ra=-4+0.5}{17}{%
    \multido{\rb=0.001+0.5}{8}{%
      \pstVerb{/xC \ra\space def
               /yC \rb\space def
               /F  xC yC div -1 mul \space def
}
%\psline[linecolor=blue](! xC  yC )(! xC yC)
\psdot[linecolor=red!60](! xC yC)
\psline[linecolor=blue](! xC F ATAN 57.295 mul cos 0.2 mul sub yC F ATAN 57.295 mul sin 0.2 mul sub)(! xC F ATAN 57.295 mul cos 0.2 mul add yC F ATAN 57.295 mul sin 0.2 mul add )
}}
\end{pspicture}
} %optionalDisplay
%\begin{pspicture}
%\end{pspicture}

\subsection{Separable equations}
A differential equation is separable if it can be written in the form 
\begin{equation}\label{eqDFQSeparable}
\frac{d y}{dx}= f(x)g(y)\quad .
\end{equation}
Here, we recall that $y$ is an abbreviation for the function $y(x)$. 

A separable differential equation can be solved as follows. Replace for convenience $x$ by $t$:
\[
\frac{d y}{dt}= f(t)g(y)\quad .
\]
Divide out by $g(y)$
\[
\frac{d y}{dt} \frac{1}{g(y)}= f(t)\quad .
\]
We integrate both sides from $t=x_0$ to $t=x$. Again, we use the abbreviation $y=y(t)$.
\begin{equation}\label{eqSeparableDFQSolution1}
\begin{array}{rcl}
\displaystyle \int\limits_{t=x_0}^{t=x}\frac{1}{g(y(t))} \underbrace{\frac{d y}{dt}dt}_{= d y(t)}&=&\displaystyle \int\limits_{t=x_0}^{t=x}  f(t)dt
\\ 
\displaystyle \int\limits_{t=x_0}^{t=x}\frac{1}{g(y(t))} d(y(t))&=&\displaystyle \int\limits_{t=x_0}^{t=x}  f(t)dt
\quad .
\end{array}
\end{equation}
Suppose now $\frac{dy }{dt}$ is non-zero and does not change sign for $t\in [x_0, x]$. Without loss of generality assume $\frac{dy}{dt}>0$. Then $y$, as a function of $t$, is a bijection (for definition of bijection see Section \ref{secInverseFunctionBasics}) between $t\in [x_0, x]$ and $[y(x_0), y(x)]$. Therefore we may change the interval over which we integrate from $t\in  [x_0, x]$ to $y(t)\in [y(x_0), y(x)]$, to get 
\begin{equation}\label{eqSeparableDFQSolution2}
\displaystyle \int\limits_{y=y(x_0)}^{y=y(x)}\frac{1}{g(y)} dy=\displaystyle \int\limits_{t=x_0}^{t=x}  f(t)dt\quad .
\end{equation}
Here, we have introduced a new dummy variable $y$, with respect to which we integrate. We ask the reader to distinguish  the function $y(x)$ that appears in the bounds of integration from the newly introduced dummy variable $y$ with respect to which we integrate. We ask the reader to forgive this slight abuse of notation\footnote{In more formal expositions, e.g., scientific papers, such abuse of notation should be avoided.}.

Suppose we can solve $\displaystyle \int \frac{1}{g(y)}dy = G(y)+C_1$ and $\displaystyle \int  f(t)dt=F(t)+C_2$. Using the Fundamental theorem of Calculus we can rewrite \eqref{eqSeparableDFQSolution2} as
\[
G(y(x))-G(y(x_0)) = \left. G(y)\right|_{y=y(x_0)}^{y=y(x)} = \left. F(x) \right|_{t=x_0}^{t=x}= F(x)-F(x_0)
\]
or finally as 
\begin{equation}\label{eqDFQSeparableSolution}
G(y)-F(x)-G(y(x_0))+ F(x_0)=0\quad .
\end{equation}
If it is possible to solve the above equation for $y$ as a function of $x$, we get the desired solution of our differential equation. If not possible to solve for $y$ explicitly, we may leave the solution of our differential equation in the form \eqref{eqDFQSeparableSolution}.
\subsubsection{Examples of separable equations}
(a) Find the general solution of
\[
\frac{dy}{dx}= y^2-1\quad .
\]
(b)Find a solution for which $y(0)=-3/5$.

\begin{solution}
(a) Proceeding as explained in the theoretical section,
\[
\begin{array}{rcl}
\frac{\frac{dy}{dx}}{y^2-1}&=&1 \\
\displaystyle\int\limits_{t=x_0}^{t=x} \frac{1}{ y(t)^2-1}\underbrace{\frac{dy}{dt}dt}_{=d(y(t))}&=&\displaystyle\int\limits_{t=x_0}^{x}dt \\
\displaystyle\int\limits_{y=y(x_0) }^{y=y(x)} \frac{dy}{ y(t)^2-1}& =& \displaystyle\left.t \right|_{ t=x_0}^{t=x} \\
\displaystyle\int\limits_{y=y(x_0)}^{y=y(x)} \left(\frac{1/2 }{y-1}- \frac{1/2}{y+1}\right)dy&=& x-x_0\\
\displaystyle\left .\frac{1}2 \ln \left|\frac{y-1}{y+1}\right|~~~~~~~~~~\right|_{y=y(x_0)}^{y(x)}&=& x-x_0\\
\displaystyle\ln \left|\frac{y(x)-1}{y(x)+1}\right|&=& 2x - C\quad ,
\end{array}
\]
where we have set $C:=2x_0-  \ln \left|\frac{y(x_0)-1}{y(x_0)+1}\right|$. 

Case 1. Suppose $\frac{y(x)-1}{y(x)+1}>0$. Set $D:=e^{-C}$. Exponentiate both sides to get 
\[
\begin{array}{rcl}
\frac{y(x)-1}{y(x)+1}&=& D e^{2x}\\
y(x)-1&=& De^{2x}(y(x)+1)\\
y(x)(1- De^{2x})&=& De^{2x}+1\\
y(x)&=&\frac{ 1+De^{2x}}{1- De^{2x}}\quad .\\
\end{array}
\]

Case 2.  Suppose $\frac{y(x)-1}{y(x)+1}=0$. Then $y(x)=\pm 1$, and we can plug back $y(x)=\pm 1$ in the original equation to get that $\frac{dy}{dx}= 0$, and therefore $y(x)= \pm 1$ (does not depend on $x$) are two solutions.

Case 3. Suppose  $\frac{y(x)-1}{y(x)+1}<0$. Then $\ln \left|\frac{y(x)-1}{y(x)+1}\right|= \ln \left(\frac{1-y(x)}{y(x)+1}\right)$ and, similarly to Case 1, we get 
\[
\begin{array}{rcl}
\frac{1-y(x)}{y(x)+1}&=& D e^{2x}\\
1-y(x)&=& De^{2x}(y(x)+1)\\
y(x)(1+ De^{2x})&=& 1-De^{2x}\\
y(x)&=&\frac{1- De^{2x}}{1+ De^{2x}}\quad .
\end{array}
\]
We may plot solutions for a few values of $D$ as follows. We overlay the solutions on top of the direction field of the differential equation. The picture tells us a lot about the properties of the solutions of the differential equations. 
\optionalDisplay{
\begin{pspicture}(-6,-6)(6,6)
\newcommand{\Dconst}{1}
\psplot[linecolor=green]{-4}{4}{1 \Dconst\space 2.718281828 2 x mul exp mul sub 1 \Dconst\space 2.718281828 2 x mul exp mul add div} 
\renewcommand{\Dconst}{0.25}
\psplot[linecolor=green]{-4}{4}{1 \Dconst\space 2.718281828 2 x mul exp mul sub 1 \Dconst\space 2.718281828 2 x mul exp mul add div} 
\renewcommand{\Dconst}{4}
\psplot[linecolor=green]{-4}{4}{1 \Dconst\space 2.718281828 2 x mul exp mul sub 1 \Dconst\space 2.718281828 2 x mul exp mul add div} 
\rput[l](5,2 ){$\frac{1- \frac{1}4 e^{2x}}{1+\frac 14 e^{2x}}$ }
\rput[l](5,0.5 ){$\frac{1- e^{2x}}{1+e^{2x}}$ }
\rput[l](5,-2 ){$\frac{1- 4e^{2x}}{1+4e^{2x}}$ }
\psline[arrows=->, linestyle=dotted](5,2)(0,0.6)
\psline[arrows=->, linestyle=dotted](5,0.5)(0,0)
\psline[arrows=->, linestyle=dotted](5,-2)(0,-0.6)

\renewcommand{\Dconst}{1}
\psplot[linecolor=green]{-4}{-0.17}{1 \Dconst\space 2.718281828 2 x mul exp mul add 1 \Dconst\space 2.718281828 2 x mul exp mul sub  div} 
\psplot[linecolor=green]{0.17}{4}{1 \Dconst\space 2.718281828 2 x mul exp mul add 1 \Dconst\space 2.718281828 2 x mul exp mul sub  div} 
\renewcommand{\Dconst}{4}

\psplot[linecolor=green]{-4}{-0.863147181}{1 \Dconst\space 2.718281828 2 x mul exp mul add 1 \Dconst\space 2.718281828 2 x mul exp mul sub  div} 
\psplot[linecolor=green]{-0.523147181}{4}{1 \Dconst\space 2.718281828 2 x mul exp mul add 1 \Dconst\space 2.718281828 2 x mul exp mul sub  div} 

\renewcommand{\Dconst}{0.25}
\psplot[linecolor=green]{-4}{0.523147181}{1 \Dconst\space 2.718281828 2 x mul exp mul add 1 \Dconst\space 2.718281828 2 x mul exp mul sub  div} 
\psplot[linecolor=green]{0.863147181}{4}{1 \Dconst\space 2.718281828 2 x mul exp mul add 1 \Dconst\space 2.718281828 2 x mul exp mul sub  div} 
\rput[r](-5,0.5 ){$\frac{1+\frac 14 e^{2x}}{1- \frac{1}4 e^{2x}}$ }
\rput[r](-5,2 ){$\frac{1+e^{2x}}{1- e^{2x}}$ }
\rput[r](-5,-2 ){$\frac{1+4e^{2x}}{1- 4e^{2x}}$ }
\psline[arrows=->, linestyle=dotted](-5,0.5)(0,1.6667)
\psline[arrows=->, linestyle=dotted](-5,0.5)(1,-3.360539267)
\psline[arrows=->, linestyle=dotted](-5,2)(-0.2,5.066489563)
\psline[arrows=->, linestyle=dotted](-5,2)(0.2,-5.066489563)
\psline[arrows=->, linestyle=dotted](-5,-2)(0,-1.6667)
\psline[arrows=->, linestyle=dotted](-5,-2)(-1,3.360539267)
\psaxes{<->}(0,0)(-4.5,-4.5)(4.5,4.5)
\rput (5,5){The direction field  $\frac{dy}{dx}=y^2-1$}
  \psset{arrows=->}
  \multido{\ra=-4+0.5}{17}{%
    \multido{\rb=-4+0.5}{17}{%
      \pstVerb{/xC \ra\space def
               /yC \rb\space def
               /F  yC yC mul 1 sub \space def
}
%\psline[linecolor=blue](! xC  yC )(! xC yC)
\psdot[linecolor=red!60](! xC yC)
\psline[linecolor=blue](! xC F ATAN 57.295 mul cos 0.2 mul sub yC F ATAN 57.295 mul sin 0.2 mul sub)(! xC F ATAN 57.295 mul cos 0.2 mul add yC F ATAN 57.295 mul sin 0.2 mul add )
}}
\end{pspicture}
} %optionalDisplay

(b) From the computer generated picture above, we may visually estimate that $y(x)=\frac{1-4 e^{2x} }{1+4 e^{2x} }$ intersects the $x$-axis at $(0, -\frac 35)$. Furthermore, we may and check directly that for $y(x)=\frac{1-4 e^{2x} }{1+4 e^{2x} }$ we have $y(0)= \frac{1-4}{1+5}= \frac{-3}{5}= -\frac 35$ and that is a solution to our problem (this however does not prove the solution is unique). 

Alternatively, let us give an algebraic solution. As we are given that $y(0)=-3/5$, the solution we are looking for cannot be given by Case 2. Suppose the solution is of the form given by Case 1. Then $-3/5=y(0)= \frac{1+De^{2*0}}{1- De^{2*0}}= \frac{1+D}{1-D}$ and therefore $1+D=\frac{3}{5}(D-1)$, and so $\frac{2}{5}D= -\frac{8}{3}$, and finally $D=-\frac{20}3$. However, $D=e^{-C}>0$, so negative values for $D$ are not possible.

Finally, suppose the solution is of the form given by Case 3. We see that then $-3/5=y(0)= \frac{1-De^{2*0}}{1+ De^{2*0}}= \frac{1-D}{1+D}$, and we can compute that $D=4$ is the only solution. Finally, we get the answer
\[
y(x)= \frac{1- 4e^{2x}}{1+4e^{2x}}\quad .
\]

\end{solution}
The mixing problem. 
A tank contains 30 kg of salt dissolved in 10000 liters of water and salt solution. Brine that contains 0.05 kg of salt per liter enters the tank at a rate of 10 liters per minute. The solution is kept thoroughly mixed and drains from the tank at the same rate (10 liters per minute). Determine how much salt remains in the tank after half an hour.

\begin{solution}
Let $y(t)$ be the amount of salt (in kilograms) contained in the tank after $t$ minutes. We are given $y(0)= 30kg$. In addition, we have that 
\[
\frac{dy}{dt}= \mathrm{(rate~in)}-\mathrm{(rate~out)} \quad .
\]
The rate of salt entering the tank is constant, $0.05 kg/L *10 L/min= 0.5 kg/min$. As the solution is thoroughly mixed, at any time the concentration of salt in the tank is $\frac{y(t)}{10000} kg/L$. Therefore the rate of salt going out of the tank is $\frac{y(t)}{10000} kg/L * 10 L/min= \frac{y(t)}{1000} kg/min$. Finally, the differential equation for the amount of salt in the tank is
\[
\frac{dy}{dt}= 0.5-\frac{y(t)}{1000} \quad .
\]
We can integrate as explained in the theoretical part, to get 
\[
\begin{array}{rcl}
\int\limits_{t=0}^{30} \frac{1000}{500-y(t)}\underbrace{\frac{dy}{dt} dt}_{d(y(t))} &=& \int\limits_{t=0}^{30} dt \\
\int\limits_{y=y(0)}^{y=y(30)} \frac{1000}{500-y}dy&=& 30 \\
-1000\int\limits_{y=y(0)=30}^{y=y(30)} \frac{1}{500-y}d(500-y)&=& 30 \\
\left. -1000 \ln |500-y|~~~~~~~~~~~ \right|_{30}^{y(30)}&=& 30 \\
-1000 \left(\ln |500-y(30)| -\ln |500- 30|  \right)&=& 30 \\
\ln\underbrace{\left| \frac{470}{500-y(30)} \right|}_{=\frac{470}{500-y(30)}\mathrm{,~see~below} } &=& \frac{30}{1000} \\
\frac{470}{500-y(30)}&=&e^{3/100}\\
500-y(30)&=& 470e^{-3/100}\\
y(30)&=&500-470e^{-3/100}\\
&\approx& 500-470*0.970445534 \\
&\approx& 43.89 \quad ,
\end{array}
\]
where we have used that $ \frac{470}{500-y(t)}>0 $. The fact that $ \frac{470}{500-y(t)}>0 $ can be seen as follows. As $500-y(0)=470>0$ and $y(t)$ is continuous, in order to have $500-y(t)<0$ there must exist some $x_1$ for which $y(x_1)=500$. However this is impossible since $x=\ln \left|\frac{470}{500-y(x)}\right|  $. 

As the unit of measurement is $kg$, the answer to the problem is $\approx 43.89 kg$ salt.

\end{solution}
\subsubsection{Exercises}
\begin{problem} ~
\begin{itemize}
\item Find the general solution to the differential equation 
\[
\frac{dy}{dx}= y^2-4\quad .
\]
The drawing below is a computer-generated plot of the direction field  $\frac{dy}{dx}=y^2-4$, you may use it to get a feeling for what your answer should look like.
\item Find a solution of the above equation for which $ y(0)= -\frac{6}{5}$. 
\end{itemize}

\optionalDisplay{
\begin{pspicture}(-6,-6)(6,6)
\newcommand{\Dconst}{4}
\psplot[linecolor=green]{-4}{4}{1 \Dconst\space 2.718281828 4 x mul exp mul sub 1 \Dconst\space 2.718281828 4 x mul exp mul add div 2 mul} 

\psaxes{<->}(0,0)(-6,-2)(6,6)
\rput (5,5){The direction field  $\frac{dy}{dx}=y^2-4$}
  \psset{arrows=->}
  \multido{\ra=-4+0.5}{17}{%
    \multido{\rb=-4+0.5}{17}{%
      \pstVerb{/xC \ra\space def
               /yC \rb\space def
               /F  yC yC mul 4 sub \space def
}
%\psline[linecolor=blue](! xC  yC )(! xC yC)
\psdot[linecolor=red!60](! xC yC)
\psline[linecolor=blue](! xC F ATAN 57.295 mul cos 0.2 mul sub yC F ATAN 57.295 mul sin 0.2 mul sub)(! xC F ATAN 57.295 mul cos 0.2 mul add yC F ATAN 57.295 mul sin 0.2 mul add )
}}
\end{pspicture}
} %optionalDisplay
\end{problem}
\begin{problem}
Mixing problem. A tank contains 1000 kg of salt dissolved in 10000 liters of water. Brine that contains 0.05 kg of salt per liter of water enters the tank at a rate of 30 liters per minute. The solution is kept thoroughly mixed and drains from the tank at the same rate (30 liters per minute). Determine how much salt remains in the tank after an hour.
\end{problem}
\subsection{Linear differential equations}
A differential equation is \emph{linear} if it is of the form 
\begin{equation}\label{eqDFQlinear}
\frac{dy}{dx} + P(x)y=Q(x)
\end{equation}
for some functions $P(x),Q(x)$.

A linear differential equation \eqref{eqDFQlinear} can be solved as follows. 

Let us make the problem slightly more general by considering an equation of the form 
\begin{equation}\label{eqDFQlinearStep1}
R(x)\frac{dy}{dx} + S(x)y=Q(x)\quad .
\end{equation}
Suppose that it so happened that $R'(x)=S(x)$. Then \eqref{eqDFQlinearStep1} becomes
\begin{equation}\label{eqDFQlinearStep2}
\begin{array}{rcl}
\underbrace{R(x)\frac{dy}{dx} + R'(x)y}_{\mathrm{product~rule~}(fg)'=f'g+g'f}&=&Q(x)\\
\frac{d}{dx} \left(R(x)y \right)&=& Q(x)\quad .
\end{array}
\end{equation}
In the above we can introduce a new function $z(x)= y(x)R(x)$, and the equation becomes 
\[
\frac{dz}{dx}= Q(x)\quad ,
\]
which we already can solve by integrating both sides to obtain $z(x)= \int\limits_{t=x_0}^x Q(t)dt $, or finally, if $R(x)\neq 0$, we get $y(x)= \frac{1}{R(x)}\int\limits_{t=x_0}^x Q(t)dt $.
Let us now try to solve \eqref{eqDFQlinear} by finding a multiplier function $T(x)$ that does not vanish, such that when we multiply \eqref{eqDFQlinear}, we obtain an equation of the form \eqref{eqDFQlinearStep2}. Indeed, in order to get an equation of the form \eqref{eqDFQlinearStep2}, we need that $\frac{dT}{dx}= (TP(x))$. The latter is a separable differential equation for $T(x)$.  We solve the equation as studied in the preceding section.
\begin{equation}\label{eqDFQlinearStep3}
\begin{array}{rcl}
\frac{dT}{dx}\frac{1}{T}&=&P(x)\\
\int\frac{dT}{T}&=&\int P(x)dx\\
\ln |T|&=&  \int P(x)dx+C\\
T&=&De^{\int P(x)dx}, 
\end{array}
\end{equation}
where $C$ is an arbitrary constant and $D=e^C$. As we have no initial conditions on $\frac{dT}{dx}= (TP(x))$ (in fact, we made up that equation ourselves), we are free to choose arbitrary initial conditions. This is reflected in the fact that, somewhat informally, we used indefinite integral instead of definite in the solution of \eqref{eqDFQlinearStep3}.

Motivated by the preceding discussion, define $T(x)\eqdef e^{\int P(x)dx}$, where the constant added to the indefinite integral is chosen arbitrarily. $T(x)$ is thus defined only up to a constant.

Multiply \eqref{eqDFQlinear} by $T(x)$ on both sides and transform \eqref{eqDFQlinear} to \eqref{eqDFQlinearStep1}, which we already solved.
\subsubsection{Example of linear differential equations}
Solve 
\begin{equation}\label{eqDFQlinearexample1}
\frac{dy}{dx}+ y x=x\quad .
\end{equation} Find a solution such that $y(0)=3$.

\begin{solution}
First we find an integrating factor $T(x)$. By the theoretical section, $T(x)$ is up to a multiplicative constant equal to $e^{\int x dx}= e^{\frac{x^2}2 +C}$. We may choose $T(x):= e^{\frac{x^2}2}$. We multiply \eqref{eqDFQlinearexample1} by $e^{\frac{x^2}2}$ and solve as follows.
\[
\begin{array}{rcl}
\displaystyle e^{\frac{x^2}2}\frac{dy}{dx}+ yxe^{\frac{x^2}2}&=&\displaystyle xe^{\frac{x^2}2}\\
\underbrace{\frac{d}{dx} \left(y(x) e^{\frac{x^2}2} \right)}_{\mathrm{rename~}x~\mathrm{to~}t\mathrm{~and~integrate}}&=&\underbrace{xe^{\frac{x^2}2}}_{\mathrm{rename~}x\mathrm{~to~}t\mathrm{~and~integrate}}\\
\displaystyle\int\limits_{t=x_0}^x \frac{d}{dt} \left(y(t) e^{\frac{t^2}2}\right) dt &=&\displaystyle \int\limits_{t=x_0}^x  te^{\frac{t^2}2}dt\\
\displaystyle\left.y(t) e^{\frac{t^2}2}\right|_{t=x_0}^x&=&\displaystyle \left.e^{\frac{t^2}2}\right|_{t=x_0}^{t=x}\\
\displaystyle y(x)e^{\frac{x^2}2}- y(x_0)e^{\frac{x_0^2}2}&=&\displaystyle e^{\frac{x^2}2}- e^{\frac{x_0^2}2}\\
y(x)&=&\displaystyle \frac{e^{\frac{x^2}2}- e^{\frac{x_0^2}2}+ y(x_0)e^{\frac{x_0^2}2}}{e^{\frac{x^2}2}}\\
y(x)&=&\displaystyle 1+{e^{\frac{x_0^2-x^2}2}}(y(x_0)-1)\quad.
\end{array}
\]

In order to find a solution for which $y(0)=3$, we simply plug in $x_0=0$, $y(x_0)=3$, to get $y(x)=1+2e^{\frac{-x^2}2}$.

\optionalDisplay{
\begin{pspicture}(-5,-5)(5,5)
\psaxes{<->}(0,0)(-5,-5)(5,5)
\newcommand{\Xstart}{1}
\newcommand{\Ystart}{1}
\psplot[linecolor=green]{-4}{4}{ 2.718281828 \Xstart\space \Xstart\space mul x x mul sub exp \Ystart\space 1 sub mul 1 add} 
\renewcommand{\Xstart}{0}
\renewcommand{\Ystart}{-1}
\psplot[linecolor=green]{-4}{4}{ 2.718281828 \Xstart\space \Xstart\space mul x x mul sub exp \Ystart\space 1 sub mul 1 add} 
\renewcommand{\Xstart}{0}
\renewcommand{\Ystart}{3}
\psplot[linecolor=green]{-4}{4}{ 2.718281828 \Xstart\space \Xstart\space mul x x mul sub exp \Ystart\space 1 sub mul 1 add} 

\rput (5,5){The direction field $\frac{dy}{dx}=x-xy$}
  \psset{arrows=->}
  \multido{\ra=-4.0+0.5}{17}{%
    \multido{\rb=-4.0+0.5}{17}{%
      \pstVerb{/xC \ra\space def
               /yC \rb\space def
               /F  xC yC xC mul sub \space def
}
%\psline[linecolor=blue](! xC  yC )(! xC yC)
\psdot[linecolor=red!60](! xC yC)
\psline[linecolor=blue](! xC F ATAN 57.295 mul cos 0.2 mul sub yC F ATAN 57.295 mul sin 0.2 mul sub)(! xC F ATAN 57.295 mul cos 0.2 mul add yC F ATAN 57.295 mul sin 0.2 mul add )
}}
\end{pspicture}
} %end \optionalDisplay
\end{solution}
\section{Sequences, limits, formal power series}\label{secSequencesPowerSeries}
\subsection{Sequences, limits of sequences}
\begin{definition}
\index{sequence} A sequence is an infinite list of numbers 
\[
a_1, a_2, a_3, \dots\quad .
\]
\end{definition}
As usual, we may choose letters other than the letter $a$ to denote a sequence. The number $a_n$ is called the $n^{th}$ term of the sequence. A sequence can be thought of as a function from the non-negative integers to the real numbers $a: \mathbb Z_{>0}\to \mathbb R$. The term $a_n$ can be thought of as the value $a(n)$ of the function $a$ at $n$. A similar definition can be given for sequences of complex numbers.

In our lectures, in the notation $a_n$, we will informally call $n$ the index of $a_n$. We are allowed to relabel sequences to start at an index other than 1. 

A sequence starting with index $1$ is usually denoted as $\{a_n\}$ or, to be more specific, $\{a_n\}_{n=1}^\infty$. A sequence starting at $n=3$ would be denoted as $\{a_n\}_{n=3}^\infty$; a sequence starting at $-1$ would be denoted as $\{a_n\}_{n=-1}^\infty$; and so on.

Sequences can be given by directly producing a formula for the $n^{th}$ term, for example $a_n:=\frac{1}{n}$.

Sequences can be also given recursively, for example $a_1:=1$, $a_2:=1$, $a_n:=a_{n-1}+a_{n-2}$. Another example would be $a_1:=1$, $a_{n+1}= \frac{a_n}{2}+\frac{N}{2a_n}$ for any $N>0$.

We say that a sequence $\{a_n\}$ has limit $L$ and we write 
\[
\lim a_n\to L
\]
if the terms $a_n$ get arbitrarily close to $L$ as $n$ increases. More precisely, 

\importantText{
\begin{tabular}{l}
$\{a_n\}$ has limit $L$ if for every $\varepsilon>0$ we can choose an index $N$ \\
depending on $\varepsilon$, such that $|a_n-L|<\varepsilon$ whenever $n>N$.
\end{tabular}
}

\index{convergent!sequence}\index{divergent!sequence} If a sequence $\{a_n\}$ has limit, we say it \emph{converges} (or is \emph{convergent}), else we say it \emph{diverges} (or is \emph{divergent}).

If $\lim\limits_{x\to \infty} f(x)=L $, then the sequence $\{f(n)\}$ converges to $L$.

We say that
\[
\lim \limits_{n\to \infty} a_n=+\infty
\]
if for every positive number $M$ there is an index $N$ depending on $M$ such that $a_n>M$ whenever $n>N$. 

Note that even though we write $\lim \limits_{n\to \infty} a_n=+\infty$, the sequence $\{a_n\}$ is divergent (this terminology is rather unfortunate, but can be justified as preferable when studying a field of mathematics called ``Topology'').

If $\{a_n\}$ and $\{b_n\}$ are convergent sequences, and $c$ is constant, then
\importantFormula{
\begin{array}{rcl}
\lim \limits_{n\to \infty} (a_n+b_n)&=&\lim \limits_{n\to \infty} a_n+ \lim \limits_{n\to \infty} b_n\\
\lim \limits_{n\to \infty} (a_n-b_n)&=&\lim \limits_{n\to \infty} a_n- \lim \limits_{n\to \infty} b_n\\
\lim \limits_{n\to \infty} (ca_n)&=&c\lim \limits_{n\to \infty} a_n\\
\lim \limits_{n\to \infty} (a_nb_n)&=&\left(\lim \limits_{n\to \infty} a_n\right) \left(\lim \limits_{n\to \infty} b_n\right)\\
\lim \frac{a_n}{b_n}&=&\frac{\lim \limits_{n\to \infty} a_n}{\lim \limits_{n\to \infty} b_n} \quad\quad\quad \mathrm{if~}\lim \limits_{n\to \infty} b_n\neq 0 \mathrm{~and~} b_n\neq 0\mathrm{~for~all~}n\quad .
\end{array}
}

\importantText{
If $\lim\limits_{n\to \infty} a_n=L$ and $f$ is a function that is continuous at $L$, then 
$\lim \limits_{n\to \infty} f(a_n)=f(L)$.
}

The sequence $\{r^n\}$ is convergent if $1< r\leq 1$ for $r$- real, and divergent for other values of $r$.
\importantFormula{\label{eqLimitRtoTheNth}
\lim\limits_{n\to \infty}r^n= \left\{\begin{array}{ll} 0 & \mathrm{if~} -1<r<1\\ 1& \mathrm{if~} r=1\end{array} \right.\quad .
}

A sequence $\{a_n\}$ is increasing if $a_n<a_{n+1}$ for all $n$. A sequence is decreasing if $a_n>a_{n+1}$ for all $n$. A sequence is monotonic if it is either increasing or decreasing. \index{sequence!monotonic} \index{sequence!increasing} \index{sequence!decreasing}  

A sequence $\{a_n\}$ is bounded above by $M$ if $a_n<M$ for all $n$. A sequence is bounded below by $M$ if $a_n>M$ for all $n$. If a sequence is bounded above and below it is called a bounded sequence. \index{sequence!bounded, below and above}

\begin{theorem}\label{thMonotonicSequenceThereom}\index{sequence!monotonic sequence theorem}
 (Monotonic sequence theorem). Every bounded monotonic sequence is convergent.
\end{theorem}
\subsection{Limits of functions (Review)}
\textbf{This section is to be written.}
\subsection{A couple of tools}
\subsubsection{L'Hospital's rule}
\begin{theorem}
\index{theorem!mean value} Mean value theorem (Review). Let $f:[a,b]\to \mathbb R$ (i.e., $f$ is a function from the interval $[a,b]$ to the reals), such that $f$ is continuous in the entire interval $[a,b]$, and differentiable in the open interval $(a,b)$. Then there exists a number $c\in (a,b)$ such that 
\[
f'(c)=\frac{f(b)-f(a)}{b-a}\quad,
\]
or equivalently,
\[
(b-a)f'(c)=f(b)-f(a)\quad .
\]
\end{theorem}

\begin{theorem} \index{L'Hospital's rule}
(L'Hospital's rule). Let $I$ be an open interval and let $a\in I$. Let $f$ and $g$ be functions that are differentiable in the interval $I\setminus \{a\}$ ($I$ minus the point $a$). Suppose either
\[
\lim_{x\to a} f(x)= \lim_{x\to a}g(x)=0
\]
or
\[
\lim_{x\to a} f(x)= \lim_{x\to a}g(x)=\pm \infty\quad .
\]
Suppose in addition 
\[
\lim_{x\to a} \frac{f'(x)}{g'(x)}
\]
exists and $g'(x)\neq 0$ for all $x\in I\setminus \{a\}$. Then 
\[ 
\lim_{x\to a}\frac{f(x)}{g(x)}= \lim_{x\to a} \frac{f'(x)}{g'(x)}\quad .
\]
\end{theorem}

L'Hospital's rule applies for $a=\infty$, i.e., $x\to \infty$. 

Proof of L'Hospital's rule in the case that $\lim\limits_{x\to a}g(x)=0 $. For $x$ sufficiently close to $a$ we have by the Mean value theorem that $f(x)-f(a)=f(x)=  f'(c_x) (x-a) $, where $c_x$ is some point in the interval $(a,x)$ and depending on $x$. Similarly, $g(x)- g(a)= g'(\bar c_x)(x-a)$ for some other point $\bar c_x\in (a,x)$. Therefore 
\[
\lim\limits_{x\to a}\frac{f(x)}{g(x)}=\lim\limits_{x\to a}\frac{f'(c_x) \cancel{(x-a)}}{ g'(\bar c_x)\cancel{(x-a)}} = \lim\limits_{x\to a}\frac{f'(c_x) }{ g'(\bar c_x)}=\lim\limits_{x\to a}\frac{f'(x) }{ g'(x)}\quad , 
\] 
where the very last equality follows from the fact that for every $x$, $c_x$ is in the interval $(a,x)$, and therefore $c_x$ (as a function of $x$) tends to $a$ as $x$ tends to $a$. For the very last equality we are also using the fact that $f'(x)$ and $g'(x)$ are continuously differentiable in a small neighborhood of $a$ (where?).

The above explanation does not deal with the case of $f(x), g(x)\to\pm \infty $ or with $x\to \infty$. A detailed proof in those cases can be found at Wikipedia's page.
\url{http://en.wikipedia.org/wiki/L'H%C3%B4pital's_rule}

A strategy for finding a limit $\lim\limits_{x\to a}F(x)$. The strategy is not guaranteed to work.
\begin{itemize}
\item Step 1. Important(!) Check whether $F(x)$ is well-defined at $x=a$ by plugging in $x=a$. If $F(a)$ is well defined, or is already known to tend to $\pm \infty$, the limit is simply $F(a)$.
\item Step 2. From now on we assume that $F(x)$ is not well-defined at $x=a$, and we do not know its limit. $F(x)$ can be of the form $\frac{0}{0}$, $\frac{\pm \infty }{\pm \infty}$, $ \infty-\infty$, etc. 

Here, the terminology ``of the form $\frac{0}{0}$ or $\frac{\pm\infty}{\pm \infty}$'' is used colloquially; such wording should be avoided in scientific texts (including the natural sciences).
\item Step 3. Try to rewrite $F(x)$ as a quotient of two functions $\frac{f(x)}{g(x)}$ such that $f(x)$ and $g(x)$ are well-defined, or have known limits $f(x), g(x)\to \pm \infty$ as $x\to a$.
\begin{itemize}
\item If $F(x)$ is of the form $f(x)g(x)$ you may try rewriting as $\frac{f(x)}{\frac{1}{g(x)}}$.
\item If $F(x)$ is of the form $f(x)\pm g(x)$ you may try rewriting as $\frac{(f(x)\pm g(x))(f(x)\mp g(x))}{f(x)\mp g(x)}$.
\end{itemize}
Note there is no fixed recipe for rewriting $F$.
\item Step 4. Check whether $\frac{f(a)}{g(a)}$ is of the form $\frac{0}{0}$, or $\frac{\pm\infty}{\pm \infty}$. If so, use L'Hospital's rule. Go back to Step 1.
\end{itemize}
Note that in the above strategy, you might end up going in an infinite cycle. If, in your judgment, you are going in an infinite cycle, try transforming $F(x)$ algebraically.
\subsubsection{Examples of L'Hospital's rule}
\[
\lim\limits_{x\to 0} \underbrace{(x-1)\ln x}_{\mathrm{~of~the~form~}(-1)*(-\infty)} =+\infty\quad .
\]

\[\begin{array}{rcl}
\underbrace{\lim\limits_{x\to 0} x\ln x}_{\mathrm{~of~the~form~}0*(-\infty)} &=&\underbrace{\lim\limits_{x\to 0} \frac{\ln x}{\frac{1}{x}}}_{\mathrm{~of~the~form~}(\pm\infty)/(\pm\infty)}\stackrel{\mathrm{L'Hospital}}{=}\lim\limits_{x\to 0} \frac{(\ln x)'}{\left(\frac{1}{x}\right)'}\\&=& \lim\limits_{x\to 0} \frac{\frac{1}{x}}{\frac{1}{x^2}}=  \lim\limits_{x\to 0} x =0
\end{array}
\]

\[
\begin{array}{rcl}
\lim\limits_{x\to 0} \underbrace{\frac{\sin x}x}_{\mathrm{~of~the~form~}0/0}\stackrel{\mathrm{L'Hospital}}{=} \lim\limits_{x\to 0} \frac{(\sin x)'}{x'}= \lim\limits_{x\to 0} \frac{\cos x}{1}= 1\quad .
\end{array}
\]

\[
\begin{array}{rcl}
\underbrace{\lim\limits_{x\to \infty} \frac{e^x}{x^2}}_{\mathrm{~of~the~form~}\pm \infty/\pm\infty} &\stackrel{\mathrm{L'Hospital}}{=} & \underbrace{\lim\limits_{x\to \infty} \frac{e^x}{2x}}_{\mathrm{~of~the~form~}\pm \infty/\pm\infty}\stackrel{\mathrm{L'Hospital}}{=}\lim\limits_{x\to \infty} \frac{e^x}{2}= +\infty
\end{array}
\]

Let $n\in \mathbb Z_{\geq 0}$ (i.e., $n$ is a non-negative integer). Prove by induction that 
\begin{equation}\label{eqLimiteMinusXtimesxtoNth}
\lim\limits_{x\to \infty} x^ne^{-x} = \lim\limits_{x\to \infty}\frac{ x^n}{e^{x}}=0\quad .
\end{equation}
\begin{solution}
For the base of the induction we have that $\lim\limits_{x\to \infty}\frac{1}{e^{x}}=0$ as $e^x\to +\infty$. Suppose we have already proven by induction that $ \lim\limits_{x\to \infty}\frac{x^{n-1}}{e^{x}}=0$. We aim to prove that $ \lim\limits_{x\to \infty}\frac{x^{n}}{e^{x}}=0$. Indeed, we compute
\[
\begin{array}{rcl}
\underbrace{\lim\limits_{x\to \infty} \frac{x^n}{e^x}}_{\mathrm{~of~the~form~}\pm \infty/\pm\infty} &\stackrel{\mathrm{L'Hospital}}{=} & \lim\limits_{x\to \infty} \frac{nx^{n-1}}{e^x}=n\left( \lim\limits_{x\to \infty} \frac{x^{n-1}}{e^x}\right) \stackrel{\mathrm{by~induction~hypothesis}}{=}0\quad .
\end{array}
\]
which proves the induction step and completes the proof.
\end{solution}

\[
\begin{array}{rcl}
\lim\limits_{x\to \infty} \frac{(\ln x)}{\sqrt{x}} &\stackrel{\mathrm{L'Hospital}}{=} & \lim\limits_{x\to \infty} \frac{(\ln x)'}{(\sqrt{x})'}=
 \lim\limits_{x\to \infty} \frac{1/x}{1/2x^{-1/2}}= \lim\limits_{x\to \infty} \frac{2}{\sqrt{x}}=0
\end{array}
\]

\[
\begin{array}{rcl}
\lim\limits_{x\to 0}\limits \frac{\tan x- x}{x^3}&\stackrel{\mathrm{L'Hospital}}{=} &\lim\limits_{x\to 0} \frac{\frac{1}{\cos^2 x}-1}{3x^2}=
\lim\limits_{x\to 0} \frac{1}{\cos^2x}\frac{1-\cos^2x}{3x^2}\stackrel{\mathrm{see~the~remark!}}{=}\underbrace{\lim\limits_{x\to 0} \frac{1}{\cos^2x}}_{=1} \lim\limits_{x\to 0} \frac{1-\cos^2x}{3x^2} \\
&\stackrel{\mathrm{L'Hospital}}{=}&\lim\limits_{x\to 0}\frac{2\sin x\cos x }{6x}= \lim\limits_{x\to 0}\frac 13\cos x\lim\limits_{x\to 0}\frac{\sin x}{x}\stackrel{\mathrm{L'Hospital}}{=}\frac 13\lim\limits_{x\to 0}\cos x= \frac 13\quad . 
\end{array}
\]
\textbf{Remark.} In general we do not have the right to split off limits: it may happen that $\lim\limits_{x\to a} f(x)g(x)$ exists but $ \left(\lim\limits_{x\to a} f(x)\right) \left(\lim\limits_{x\to a} g(x)\right)$ doesn't. The easiest example for this is $1=\lim\limits_{x\to 0} 1 =  \lim\limits_{x\to 0} \frac{x}{x}$. This expression cannot be split as $\left(\lim\limits_{x\to 0} \frac{1}{x}\right)\left(\lim\limits_{x\to 0} x \right)$: indeed, the latter expression is not well-defined.

However, if $f(a)\neq 0, \pm \infty$ and $f$ is continuous in a small neighborhood of $a$, then we have the right to write $\lim\limits_{x\to a} f(x)g(x)= \left(\lim\limits_{x\to a} f(x)\right) \left(\lim\limits_{x\to a} g(x)\right)= f(a)\left(\lim\limits_{x\to a} g(x)\right)$.

\subsubsection{Improper integrals} \label{secImproperIntegrals}
\index{improper integral} Improper integrals are the combination of definite integrals and limits.

Type I: infinite intervals.

\begin{itemize}
\item Suppose $\int\limits_{t=a}^{x}f(t)dt$ exists for all $t\geq a$. We define 
\[
\int\limits_{a}^{\infty} f(t)dt\eqdef  \lim
\limits_{x\to \infty}\int\limits_{t=a}^{x}f(t)dt\quad,
\]
provided the above limit exists.
\item Similarly, suppose $\int\limits_{t=x}^{a}f(t)dt$ exists for all $t\leq a$. We define 
\[
\int\limits_{-\infty}^{a} f(t)dt\eqdef  \lim
\limits_{x\to \infty}\int\limits_{t=x}^{a}f(t)dt\quad,
\]
provided the above limit exists.
\item Suppose $\int\limits_{0}^{\infty}f(t)dt$ and $\int\limits_{-\infty}^{0}f(t)dt$  exist. We define
\[
\int\limits_{-\infty}^{\infty} f(t)dt\eqdef  \int\limits_{0}^{\infty}f(t)dt+\int\limits_{-\infty}^{0}f(t)dt
\]
provided the above limit exists.
\end{itemize}
In case the limits defining the above improper integrals do not exist, we call the improper integrals \emph{divergent}. Else, if the limits exist, we call them \emph{convergent}. \index{convergent!improper integral} \index{divergent!improper integral}

Type II: integrating discontinuous functions.
\begin{itemize}
\item Suppose $f:[a,b)\to \mathbb R$ ($f$ is a function from $[a,b)$ to the reals) is continuous in $[a,b)$, but discontinuous at $b$. We define 
\[
\int\limits_{a}^{b}f(t)dt\eqdef \lim\limits_{\substack{x\to b \\ x<b}}\int\limits_{a}^{x} f(t)dt\quad , 
\]
provided the limit exists.
\item Similarly, suppose $f:(a,b]\to \mathbb R$ ($f$ is a function from $(a,b]$ to the reals) is continuous in $(a,b]$, but discontinuous at $a$. We define 
\[
\int\limits_{a}^{b}f(t)dt\eqdef \lim\limits_{\substack{x\to a \\ x>a}}\int\limits_{x}^{b} f(t)dt\quad , 
\]
provided the limit exists.
\end{itemize}

The comparison theorem. Suppose $f(x)\geq g(x)\geq 0$.
\begin{itemize}
\item If $\int\limits_{a}^{b}f(x)dx$ is convergent, so is $\int\limits_{a}^{b}g(x)dx$. Here, we allow $ b=\infty$ or $a=-\infty$, as well as we allow improper integrals of type II.
\item If $\int\limits_{a}^{b}g(x)dx$ is divergent, so is $\int\limits_{a}^{b}f(x)dx$. Here, we allow $ b=\infty$ or $a=-\infty$, as well as we allow improper integrals of type II.
\end{itemize}

\subsubsection{Examples of improper integrals}
Let $p\neq 1$. Then
\begin{equation}\label{eqIntegralxTopthPowerToInfty}
\int\limits_{x=1} \frac{1}{x^{p}} dx = \left.-\frac{x^{-p+1}}{-p+1}\right|_{x=1}^{\infty}= \lim\limits_{x\to\infty} 
\frac{1}{(p-1) x^{p-1}}+\frac{1}{p-1} =\doublebrace{\infty}{p<1}{\frac{1}{p-1}}{p>1} \quad . 
\end{equation}

\begin{equation}\label{eqIntegral1overxToToInfty}
\int\limits_{x=1}^{\infty} \frac{1}x dx= \left.\ln x \right|_{x=1}^{\infty}= \infty\quad .
\end{equation}

\begin{equation}\label{eqGamma(1)}
\begin{array}{rcl}
\int\limits_{0}^{\infty}e^{-x}dx&=& \left.- e^{-x}\right|_{x=0}^{x=\infty}=-\lim\limits_{x\to \infty}e^{-x}  - (-e^{-0})\\&=& -\lim \limits_{x\to \infty} \frac{1}{e^{x}} +1=1
\end{array}
\end{equation}


\[ 
\begin{array}{rcl}
\int\limits_{0}^{\infty}xe^{-x}dx&=& \left.-xe^{-x} - e^{-x}\right|_{x=0}^{x=\infty}=\lim\limits_{x\to \infty}\left( -xe^{-x} -e^{-x}\right) - (-0e^{-0}-e^{-0})\\&=& 1-\lim \limits_{x\to \infty} \frac{x+1}{e^{x}}\stackrel{\mathrm{L'Hospital}}{=}1-\lim \limits_{x\to \infty} \frac{1}{e^{x}} =1
\end{array}
\]

More generally, we can compute $\Gamma (n):=\displaystyle\int\limits_{0}^{\infty} x^ne^{-x}dx$. $\Gamma$ is the so called \emph{gamma function}, invented by L. Euler. As we compute below, in case $n$ is an integer, $\Gamma(n)=(n-1)!$. In case $n$ is not an integer, $\Gamma(n)$ may still be defined, and thus $\Gamma$ is a generalization of the notion of factorial. Suppose $n>0$ is an integer. Then
\[
\begin{array}{rcl}
\Gamma (n+1)&:=&\underbrace{\int\limits_{0}^{\infty} x^{n}\underbrace{e^{-x}dx}_{=-d (e^{-x})}}_{\mathrm{integrate~by~parts}}=\left. -x^{n}e^{-x}\right|_{x=0}^{\infty}+ n\underbrace{\int\limits_{0}^{\infty}x^{n-1} e^{-x}dx}_{=\Gamma(n)}\\
&=& \underbrace{\lim\limits_{x\to \infty} -\frac{x^{n}}{e^x}}_{=0\mathrm{~using~\eqref{eqLimiteMinusXtimesxtoNth}}}+ 0^ne^{-0}+n\Gamma(n)= n\Gamma(n)\\
&=& n(n-1)(n-2)=\dots =n(n-1)(n-2)\dots 2*1* \Gamma(1)= n!\quad .
\end{array}
\]
For the very last equality we used the fact that we already computed $\Gamma(1)$ in \eqref{eqGamma(1)}.

Determine whether the integral below is convergent. 
\[
\int\limits_{0}^{\infty}e^{-x^2}dx
\]
\begin{solution}
We cannot evaluate the above integral using the techniques studied so far. However  for $x\geq 1$ we have that $x^2\geq x$ and therefore $e^{x^2}\geq e^{x}>0$ and finally $e^{-x^2}= \frac{1}{e^{x^2}}<\frac{1}{e^x}= e^{-x}$. We have that 
\[
\int\limits_{0}^{\infty}e^{-x^2}dx= \int\limits_{0}^{1}e^{-x^2}dx +\int\limits_{1}^{\infty}e^{-x^2}dx\quad .
\]
The first integral $\int\limits_{0}^{1}e^{-x^2}dx$ is proper and well defined, as $e^{-x^2}$ is a continuous function in the interval $[0,1]$ (and everywhere else).  We already showed that $ \int\limits_{0}^{\infty}e^{-x}dx$ is convergent; we leave it to the reader to show that $ \int\limits_{1}^{\infty}e^{-x}dx$ is convergent too (this is in fact obvious). As $e^{-x}>e^{-x^2}$ for $x>1$, we apply the comparison theorem to get that $\int\limits_{0}^{\infty}e^{-x^2}dx<\int\limits_{1}^{\infty}e^{-x}dx$ is convergent.
\end{solution}

\subsection{Formal power series and operations with them}\label{secFormalPowerSeries}
\index{series!formal power} A \emph{formal power series} is an expression of the form 
\begin{equation}\label{eqFormalPowerSeriesDef}
(a_0 + a_1x+a_2x^2+\dots +a_nx^n+\dots)= \sum_{n=0}^{\infty} a_n x^n \quad .
\end{equation}

\index{series!formal power at a point} A \emph{formal power series in $x-p$}, or \emph{formal power series at $p$} is an expression of the form 
\begin{equation}\label{eqFormalPowerAtPSeriesDef}
(a_0 + a_1(x-p)+a_2(x-p)^2+\dots +a_n(x-p)^n+\dots)= \sum_{n=0}^{\infty} a_n (x-p)^n \quad .
\end{equation}
The theory of formal power series in $y=x-p$ is identical with the theory of formal power series in the indeterminate $y$ and we will not discuss it any further. Formal power series in $y=x-p$ are used when defining Taylor series (see Definition \ref{defTaylorSeries}).

The formal power series \eqref{eqFormalPowerSeriesDef} is to be thought as alternative way of writing the infinite sequence $a_1, a_2, \dots, a_n,\dots $. The ``infinite sum'' in the definition of formal power series does not imply that the sum converges in any way, or that any limits exist. We stress the fact that the expression  \eqref{eqFormalPowerSeriesDef} is  formal - $x$  is an indeterminate, rather than a number. This analogous to the fact that in a polynomial in $x$, the variable $x$ is an indeterminate.

We may or may not be allowed to plug in numbers for $x$ in the expression \eqref{eqFormalPowerSeriesDef}, depending on whether the corresponding sum is convergent. However, the formal power series are defined independent of any convergence issues.

We may (formally) add, subtract, multiply, integrate, differentiate formal power series. With additional assumptions, we may be able to divide power series, as well as carry out formal substitutions replacing $x$ by a formal power series. 

Adding, subtracting and multiplying formal power series is carried out by ``uncovering the infinite brackets'', much in the way that finite expressions are added, subtracted and multiplied.
\begin{itemize}
\item 
\[
\sum_{n=0}^\infty a_n x^n\pm \left(\sum_{n=0}^\infty b_n x^n\right)\eqdef \sum_{n=0}^\infty (a_n+b_n)x^n\quad .
\]
\item
\[
\begin{array}{rcl}
\displaystyle\left(\sum_{n=0}^\infty a_n x^n\right) \left(\sum_{n=0}^\infty b_n x^n\right) &=& 
(a_0 + a_1x+a_2x^2+\dots )(b_0 + b_1x+b_2x^2+\dots )\\&\eqdef& 
a_0b_0 + (a_0b_1+a_1b_0)x + (a_0b_2+a_1b_1+a_2b_0)x^2+\dots \\
&&+(a_0 b_n +a_1b_{n-1}+\dots + a_{n-1}b_1 +a_nb_0 )x^n+\dots \\
&=&\displaystyle \sum_{n=0}^{\infty} \left(\sum_{k=0}^n a_k b_{n-k}  \right)x^n\quad .
\end{array}
\] 
\end{itemize}

Formal power series can be (formally) differentiated and integrated. 
\[
\begin{array}{rcl}
\displaystyle
\frac{d}{dx}\left(\sum_{n=0}^{\infty} a_n x^n\right) &=& \displaystyle \frac{d}{dx} \left(a_0 +a_1x+a_2x^2+\dots \right)\eqdef a_1+2 a_2 x+ 3a_3x^2 +\dots\\
&=&\displaystyle \sum_{n=1}^{\infty} na_nx^{n-1}\quad .
\end{array}
\]

\[
\begin{array}{rcl}
\displaystyle \int \left(\sum_{n=0}^{\infty} a_n x^n\right)dx &=&\displaystyle \int (a_0 +a_1x+a_2x^2+\dots )dx\eqdef C+ a_0x +a_1\frac{x^2}{2}+a_2\frac{x^3}3+\dots 
\\
&=&\displaystyle C+\sum_{n=1}^{\infty} \frac{a_n}{n+1}x^{n+1}\quad . 
\end{array}
\]

\textbf{(!) Optional material. This point does not require material outside of this course, but you may find it technically challenging. You will not be tested on this material}. Let $g(x)$ be a formal power series without a constant term, that is, $g(x)$ is of the form $g(x)=\sum_{n=1}^\infty b_n x^n=  b_1x+b_2x^2+\dots$. Let $f(x)=\sum_{n=0}^{\infty} a_n x^n$ be a formal power series. Then we can carry out substitutions $f(g(x))$, much in the way that substitutions in usual ``finite'' polynomials are carried out. Details follow.

\[
\begin{array}{rcl}
\displaystyle
\left(\sum_{n=1}^{\infty} b_n x^n\right)^k &=&   (b_1 x+ b_2x^2+b_3x^3+\dots)^k\\
&=&\displaystyle \sum_{n=k}^{\infty}\underbrace{\left( \sum_{\substack{\mathrm{all~possible~indices}\\ k\geq i_1, i_2,\dots, i_n\geq 0\\ 
\mathrm{for~which}\\i_1+2i_2+3i_3+\dots + ni_n =n  \\ \mathrm{and} \\ i_1+\dots +i_n=k}}\binom{k}{i_1, i_2, i_3, \dots, i_n}  b_{1}^{i_1}\dots b_{n}^{i_n} \right)}_{=: c_{k,n}} x^n,
\end{array}
\]
where $\binom{k}{i_1, i_2, i_3, \dots, i_n}\eqdef  \doublebrace{\frac{k!}{i_1!i_2!\dots i_n!}}{\mathrm{if~}i_1+\dots i_n=k}{0}{\mathrm{otherwise}}$. Let $c_{k,n}$ be the numbers defined as indicated in the preceding formula. Then
\[
\begin{array}{rcl}
f(g(x)) &\eqdef&\displaystyle a_0+ \sum_{k=1}^\infty a_k\left(\sum_{j=1}^{\infty} b_j x^j\right)^k = a_0+\sum_{k=1}^\infty a_k \left(\sum_{n=k}^{\infty} c_{k,n}x^n \right)\\
&=&\displaystyle a_0+\sum_{n=1}^{\infty} \left(\sum_{k=1}^n a_k c_{k,n} \right)x^n\quad .
\end{array}
\]
Clearly, the above formulas would be cumbersome to work with by hand except for the smallest of examples. In particular, all examples done in this course will use shortcuts that circumvent the preceding formulas. However, the preceding formulas may be needed for a complicated problem arising in practice/outside of the scope of this course. Furthermore, the formulas are well-suited for implementation in computer algebra systems.

\subsection{Maclaurin series as formal power series}\label{secMaclaurinSeries}
\begin{definition}\label{defMaclaurinSeries}
\index{series!Maclaurin} Let $f(x)$ be a function that is infinitely differentiable at $0$. Recall that $f^{(n)}(x)$ denotes the $n^{th}$ derivative of $f(x)$. Then the formal power series
\begin{equation}\label{eqMacLaurinDef}
\begin{array}{rcl}
\maclaurin (f(x))&\eqdef&\displaystyle \sum_{n=0}^{\infty}  \frac{f^{(n)} (0)}{n!} x^n\\
&=& \displaystyle f(0)+f'(0)x+\frac{f''(0)}{2!}x^2+\frac{f'''(0)}{3!}x^3+\dots+\frac{f^{(n)}(0)}{n!}x^n+\dots
\end{array}
\end{equation}
is called the Maclaurin series of $f(x)$. 
\end{definition}
There is no standard notation for Maclaurin series. The notation $\maclaurin (f(x))$ used in this course is certainly unambiguous.

\begin{definition}\label{defTaylorSeries}
\index{series!Taylor} Let $f(x)$ be a function that is infinitely differentiable at the point $p$. Then the formal power series
\begin{equation}\label{eqTaylorDef}
\begin{array}{rcl}
\taylor_p (f(x))&\eqdef&\displaystyle \sum_{n=0}^{\infty}  \frac{f^{(n)} (p)}{n!} (x-p)^n\\&=& \displaystyle f(0)+f'(0)(x-p)+\frac{f''(0)}{2!}(x-p)^2+\dots+\frac{f^{(n)}(0)}{n!}(x-p)^n+\dots
\end{array}
\end{equation}
is called the Taylor series of $f(x)$ at $p$. The Maclaurin series are a partial case of the Taylor series at the point $0$.
\end{definition}
There is no standard notation for Taylor series at $p$. The notation $\taylor_p(f(x))$ used in this course is certainly unambiguous.

The motivation for the definition of Maclaurin series is that, with some conditions on the function $f(x)$, given a number $a$ with sufficiently small absolute value, $f(a)$ equals the limit of the sum of the Maclaurin series of $f(x)$ when we carry out the substitution $x=a$. An analogous statement holds for Taylor series.

In order to make the above point precise, we need to establish
\begin{enumerate}
\item under what conditions we can plug in a number in place of $x$ in \eqref{eqMacLaurinDef} and 
\item under what conditions does the sum \eqref{eqMacLaurinDef} equal the function $f(x)$ when we substitute values for $x$. 
\end{enumerate}
This is one of the central goals of Section \ref{secSeriesConvergence} .

In Section \ref{secFormalPowerSeries}, we learned how to carry out operations with formal power series. Therefore we may compute with Maclaurin series independent of proving properties 1) and 2) above. We postpone the investigation of problem 1)  to section \ref{secSeriesConvergence}. We shall not discuss problem 2) in this course (an illustration of the issue is given by Problem \ref{probDifferentiableNonAnalyticFunctionExample}). In the rest of this section, we will learn instead how to compute with Maclaurin series. 

When computing Maclaurin series, we may use the following properties. The main idea of these properties is that performing operations with functions and then taking Maclaurin series amounts to the same result as first taking the Maclaurin series and then performing the corresponding operation with the formal power series. 
\begin{itemize}
\item 
\[
\maclaurin (f(x)\pm g(x))= \maclaurin (f(x))\pm \maclaurin(g(x))
\]
\item 
\[
\maclaurin (f(x)* g(x))= \maclaurin (f(x))* \maclaurin(g(x))
\]
\item If $f(x)g(x)=1$ for all $x$ in a small interval $I$ containing $0$, then 
\[
\maclaurin (f(x)*g(x))= 1\quad, 
\]
as well as
\[
\maclaurin\left(\frac{1}{f(x)}\right)= \maclaurin(g(x))\quad .
\]
\item A proof of the following two properties will be given in \ref{thDifferentiatingIntegratingPowerSeriesNonFormal}.
\[
\maclaurin \left(\frac{df}{dx}(x)\right)= \frac{d}{dx}\left(\maclaurin(f(x)) \right)\quad .
\]
\item 
\[
\maclaurin \left(\int f(x)dx \right) = \int\left( \maclaurin(f(x))\right)dx \quad .
\]
\end{itemize}
\subsubsection{The Maclaurin series of frequently used functions}
We already studied the Maclaurin series of $e^x, \sin x, \cos x$ in \eqref{eqSinCosMaclaurinSeries} and \eqref{eqExponentFunctionDefinition}.

\[
(1-x)(1+x+x^2+\dots +x^n+\dots) = (1+\cancel{x}+\cancel{x^2}+\dots +\cancel{x^n} +\dots)- (\cancel{x}+\cancel{x^2}+\cancel{x^3}+\dots + \cancel{x^n}+\dots)= 1\quad . 
\]
Therefore 
\begin{equation}\label{eqInfiniteGeometricProgression}
\maclaurin \left( \frac{1}{1-x}\right)= 1+x+x^2+\dots \quad .
\end{equation}

We want to compute
\[
\maclaurin\left(\frac{1}{(1-x)^n}\right)\quad ,
\]
where $n\geq 1$ is an integer. In order to do this note that $\frac{d}{dx}\left(\frac{1}{(1-x)^{n-1}}\right)= \frac{n-1}{(1-x)^{n}} $. Therefore 
\[
\begin{array}{rcl}
\displaystyle \frac{d^{n-1}}{dx^{n-1}} \left(\frac{1}{1-x}\right)&=&\displaystyle  \frac{d^{n-2}}{ dx^{n-2}} \left(\frac{1}{(1-x)^2}\right)= \frac{d^{n-3} }{dx^{ n-3}}\left(\frac{1*2}{(1-x)^3}\right) = \dots \\&=&\displaystyle \frac{d^{ n-k}}{ dx^{n-k} }\left( \frac{1 *2* \dots *(k-1)}{(1-x)^{k}}\right)= \frac{(n-1)!}{(1-x)^n} \quad .
\end{array}
\]
Therefore 
\[\begin{array}{rcl}
\displaystyle \maclaurin\left(\frac{1}{(1-x)^n}\right)&=& \displaystyle \frac{1}{(n-1)!}\frac{d^{n-1}}{dx^{n-1}} \maclaurin \left(\frac{1}{1-x}\right)\\
&=& \displaystyle
\frac{1}{(n-1)!}\frac{d^{n-1}}{dx^{n-1}}(1+x+x^2+\dots +x^k+\dots )\\
&=& \displaystyle \frac{1}{(n-1)!}( (n-1)(n-2)\dots 1+n(n-1)\dots 2 x \\
&&\displaystyle \phantom{\frac{1}{(n-1)!}( }+  \dots + k(k-1)\dots (k-n+2)x^{k-n+1}+\dots )\\
&=&\displaystyle 1+ \frac{n (n-1)\dots 2}{(n-1)!} x + \dots + \frac{ k(k-1)\dots (k-n+2)}{(n-1)!}x^{k-n+1}+\dots \\
&=&\displaystyle \binom{ n-1}{n-1} + \binom{n}{n-1}x+\dots + \binom{k}{n-1}x^{k-n+1}+\dots \\
&=&\displaystyle\sum_{k=0}^\infty \binom {k}{n-1}x^{k-(n-1)}\quad .
\end{array}
\]
For the very last equality recall the definition of the Newton binomial coefficient $\binom{k}{n}$, \eqref{eqBinomialCoeffDefinition}. 

More generally, let us compute 
\[
\maclaurin (1+x)^q\quad , 
\]
where $q\in \mathbb R$ is an arbitrary rational number. Here, we recall that $(x^q)'=qx^{q-1}$ for all $q\in \mathbb R$ as proved in \eqref{eqXtotheAthDerivative}. Because $q$ is an arbitrary real number, we cannot use the trick from the preceding computation: we need to compute the Maclaurin series directly using the definition \eqref{eqMacLaurinDef}. 
\[
\begin{array}{rcl}
\frac{d}{dx}\left( (1+x)^q\right)&=& q (1+x)^{q-1}\\
\frac{d^{2}}{dx^2}\left( (1+x)^q\right)&=& q(q-1) (1+x)^{q-2}\\
\vdots \\
\frac{d^{n}}{dx^n}\left( (1+x)^q\right)&=& q(q-1)(q-2)\dots (q-n+1) (1+x)^{q-n}\quad .
\end{array}
\]
Therefore $\frac{d^{n}}{dx^n}\left( (1+x)^q\right)_{|x=0}=q(q-1)(q-2)\dots (q-n+1) (1+0)^{q-n}= q(q-1)(q-2)\dots (q-n+1)  $. Therefore 
\begin{equation}\label{eqNewtonBinomialGeneralized}
\begin{array}{rcl}
\displaystyle \maclaurin \left( (1+x)^q\right) &=& \displaystyle\sum_{ n=0}^{\infty}\frac{ 1}{n!}\frac{d^n }{dx^n} \left( (1+x)^q\right)_{|x=0} x^n  \\ &=&\displaystyle \sum_{n=0}^{\infty}  \frac{q(q-1)(q-2)\dots (q-n+1)}{n!}x^n= \sum_{n=0}^{\infty} \binom{q}{n}x^n\quad . 
\end{array}
\end{equation}
Note that the definition of binomial coefficient \eqref{eqBinomialCoeffDefinition} allows for $q$ to be an arbitrary real (and even complex) number. The above formula is a generalization of the Newton binomial formula. \index{binomial!generalized formula} As we shall soon learn, $\maclaurin \left( (1+x)^q\right)_{|x=c} =(1+c)^q $   for all $c\in (-1, 1)$.

The power series of $\arctan x$.
\[
\begin{array}{rcl}
\frac{d}{dx}\maclaurin (\arctan x)&=& \displaystyle \maclaurin\left( \frac{d}{dx}(\arctan x)\right)= \maclaurin \left(\frac{1}{1+x^2}\right) \\ 
&=&\displaystyle \maclaurin \left(\frac{1}{1-(-x^2)}\right)\\
& \stackrel{\mathrm{using~} \eqref{eqInfiniteGeometricProgression} }{=} & 1+(-x^2)+(-x^2)^2+(-x^2)^3+\dots+ (-x^2)^n+\dots
\\ &=& 1-x^2+x^4-x^6+\dots+ (-1)^nx^{2n}+\dots \quad .
\end{array}
\]
Therefore 
\[
\begin{array}{rcl}
\maclaurin (\arctan x)&=&\displaystyle \int( 1-x^2+x^4-x^6+\dots (-1)^nx^{2n}+\dots )dx \\
&=&\displaystyle  \underbrace{\arctan(0)}_{=0}+ x-\frac{x^3}3+\frac{x^5}5-\frac{x^7}7+\dots +(-1)^n\frac{x^{2n+1}}{2n+1}+\dots\\
&=&\displaystyle  \sum_{n=0}^{\infty} (-1)^n\frac{x^{2n+1}}{2n+1}\quad .
\end{array}
\]
The power series of $\ln (1-x)$, $\ln(1+x)$.
\[
\begin{array}{rcl}
\displaystyle \frac{d}{dx}\maclaurin(\ln (1-x))&=& \displaystyle \maclaurin \left(\frac{d}{dx}(\ln (1-x))\right) = \maclaurin \left(-\frac{1}{1-x}\right)\\
&=& \displaystyle -1-x-x^2-\dots -x^n-\dots
\end{array}
\]
Therefore 
\[
\begin{array}{rcl}
\displaystyle \maclaurin(\ln (1-x) )&=&\displaystyle \int (-1-x-x^2-\dots -x^n-\dots)dx \\&=&\displaystyle \underbrace{\ln (1-0)}_{=0} - x- \frac{x^2}{2}- \frac{x^3}{3}-\dots - \frac{x^n}{n}-\dots= -\displaystyle\sum_{n=1}\frac{x^n}{n} \quad .
\end{array}
\]
Therefore 
\[
\maclaurin(\ln (1+x) )=\sum_{n=1}(-1)^{n+1}\frac{x^n}{n}\quad .
\]

\[
\begin{array}{rcl}
\frac{d}{dx}\maclaurin(\arcsin{x})&=&\displaystyle \maclaurin\left(\frac{d}{dx}(\arcsin{x})\right)= \maclaurin\left(\frac{1}{\sqrt{1-x^2}}\right)\\
&=&
\maclaurin\left((1+(-x^2))^{-\frac{1}2}\right)\\
&\stackrel{\eqref{eqNewtonBinomialGeneralized}}{=}&\displaystyle 1-\frac{1}{2}(-x^{2})+ \left(\frac{-\frac{1}{2}(-\frac{1}2-1)}{2!}\right)(-x^2)^2+ \dots\\
&&\displaystyle + \left(\frac{-\frac{1}{2}(-\frac{1}2-1)\dots (-\frac{1}2-n+1)}{n!}\right)(-x^2)^n+\dots 
\\
&=& \displaystyle\sum\limits_{n=0}^{\infty} (-1)^n \binom{-\frac{1}{2}}{n} x^{2n}\quad .
\end{array}
\]
Therefore 
\[
\maclaurin(\arcsin x)=\int \maclaurin\left(\frac{1}{\sqrt{1-x^2}}\right) dx =\underbrace{0}_{=\arcsin 0} +\sum_{n=1}^{\infty} (-1)^n \binom{-\frac{1}{2}}{n} \frac{x^{2n+1}}{2n+1}\quad .
\]

\subsubsection{Exercises}
\begin{problem}
Compute the Maclaurin series of the following.
\begin{multicols}{2}
\begin{enumerate}
\item $\displaystyle \maclaurin (\arctan x) $.
\item $\displaystyle \maclaurin (\sqrt{1+x^2})$.
\item $\displaystyle \maclaurin (\arccos 2x)$.
\item $\displaystyle \maclaurin(\ln (1+x^2))$.
\item Recall from \eqref{eqExponentFunctionDefinition} the series $\displaystyle \maclaurin (e^x)$.
\item Recall from \eqref{eqSinCosMaclaurinSeries} the series $\displaystyle \maclaurin (\sin x)$.
\item Recall from \eqref{eqSinCosMaclaurinSeries} the series $\displaystyle \maclaurin (\cos x)$.
\end{enumerate}

\end{multicols}
\end{problem}

\begin{problem}\label{probDifferentiableNonAnalyticFunctionExample}
 \textbf{This problem is of higher difficulty.}
Let $f(x)$ be defined as 
\[
f(x):=\doublebrace{e^{-\frac{1}{x^2}}}{\mathrm{if~} x>0}{0}{\mathrm{otherwise.}}
\]
\begin{enumerate}
\item Prove that if $R(x)$ is an arbitrary rational function, 
\[
\lim\limits_{\substack{x\to 0\\ x>0}} R(x)e^{-\frac{1}{x^2}}=0
\]
\item Prove that $f(x)$ is differentiable at $0$ and $f'(0)=0$.
\item Prove that the Maclaurin series of $f(x)$ are 0 (but $f(x)$ is clearly a non-zero function).
\end{enumerate}

\end{problem}
\begin{problem}\textbf{This problem is of higher difficulty.} Prove the properties of Maclaurin series given in the last item of Section \ref{secMaclaurinSeries}.
\end{problem}

\subsubsection{L'Hospital's rule and limits revisited}\label{secLHospitalRevisited}
\index{L'Hospital's rule}
Suppose we want to compute $\lim\limits_{x\to 0} \frac{f(x)}{g(x)}$ when $f(0)= 0$ and $g(0)=0$. 
L'Hospital's rule teaches us that we should differentiate $f(x)$ and $g(x)$ until we establish whether the limit exists or not. Alternatively, we may write the Maclaurin series $\maclaurin (f(x))$, $\maclaurin (g(x))$. For sufficiently small $x$ and sufficiently well-behaved functions $f(x)$ and $g(x)$ (which include all functions studied in this course), $f(x)$ and $g(x)$ will equal their Maclaurin series (when we substitute values for $x$). Therefore we can substitute $f(x)$ and $g(x)$ by their Maclaurin series, cancel the common power of $x$, and evaluate the limit. This is often easier and quicker than L'Hospital's rule, in particular for problems given on Calculus exams (this statement is valid not only with the current instructor!). In the following items we give examples. 

As we shall see in section \ref{secSeriesConvergence}, for small enough values of $|x|$, $\arcsin x$ equals its Maclaurin series. Therefore, for small $x$, we have that $\arcsin x = x+\frac{1}6x^3 + \frac{3}{40}x^5+\dots $. Therefore for small $x$
\[
\arcsin x- x- \frac{1}6x^3= x^5 \left(\frac{3}{40}+x^2(\dots)\right)\quad .
\] Therefore 
\[
\lim\limits_{x\to 0} \frac{\arcsin x- x- \frac{1}6x^3}{x^5}= \lim\limits_{x\to 0} \frac{ \cancel{x^5} \left(\frac{3}{40}+x^2(\dots)\right)}{\cancel{x^5}} =  \lim\limits_{x\to 0}  \left(\frac{3}{40}+x^2(\dots)\right) =\frac{3}{40}\quad .
\]

Let us ``spice up'' the preceding problem:
\[
\lim\limits_{x\to 0} \frac{\arcsin x- x- \frac{1}6x^3}{\sin^{5} x}= \lim\limits_{x\to 0} \frac{x^5 \left(\frac{3}{40}+x^2(\dots)\right)}{(x-\frac{x^3}6+\dots)^5}=  \lim\limits_{x\to 0} \frac{\cancel{x^5} \left(\frac{3}{40}+x^2(\dots)\right)}{\cancel{x^5}(1-x^2(\dots))}= \frac{3}{40}\quad .
\]

\begin{equation}\label{eqLhospitalExample1}
\lim\limits_{x\to 0}\frac{\ln(1+px)}{x}=\lim \limits_{x\to 0} \frac{xp- \frac{x^2p^2}{2}+\dots  }{x}= \lim \limits_{x\to 0} p- \frac{xp^2}{2}+\dots = p\quad .
\end{equation}

\begin{equation}\label{eqLhospitalExample2}
\lim\limits_{x\to 0}(1+px)^{\frac1x}= \lim\limits_{x\to 0} \left(e^{\ln (1+px)}\right)^{\frac1x} = e^{\lim\limits_{x\to 0}\frac{1}x\ln (1+px)} \stackrel{\eqref{eqLhospitalExample1}}{=}e^{p}\quad .
\end{equation}

We are now in position to prove \eqref{eq(1+x/n)^n=e^x}.
\[
\lim\limits_{n\to \infty} \left(1+\frac{p}{n}\right)^n= \lim_{\substack{n\to \infty \\ x=\frac{1}{n}\\x\to 0}} \left(1+xp\right)^{\frac{1}{x}} \stackrel{\eqref{eqLhospitalExample2}}{=}e^p
\]
\subsubsection{Exercises}
\begin{problem}
Compute the limits. You may use L'Hospital's rule. For the limits for which $x\to 0$, you are allowed to alternatively use the (not yet proved fact) that whenever $x$ belongs to a small enough neighborhood of 0, analytic functions equal their Maclaurin series (all functions given below are analytic in a small enough neighborhood of 0). You may of course also use any other method that works.
\begin{enumerate}
\item $\lim\limits_{x\to 0} \frac{x^2-1}{x^2-x}$.
\item $\lim\limits_{x\to 0} \frac{\sin 4 x}{\tan 5x}$.
\item $\lim\limits_{x\to 0} \frac{\sin x - x +\frac{x^3}{6}}{x^5}$.
\item $\lim\limits_{x\to \infty} \sqrt{1+x^2} - x$.
\item $\lim\limits_{x\to 0} \frac{\cos n x - \cos mx }{x^2}$.
\item $\lim\limits_{x\to 0} \frac{\arctan x - x+ \frac{x^3}{3} }{(\sin x)^5}$.
\item $\lim\limits_{x\to 1}\left(\frac{x}{x-1}- \frac{1}{\ln x} \right)$.
\end{enumerate}
\end{problem}

\section{Series (non-formal), convergence}\label{secSeriesConvergence}
\subsection{Series, definition and key examples}
Let $\{a_k\}_{k=1}^\infty$ be an infinite sequence. Define $S_n= a_1+a_2+a_3+\dots+a_n$. We say that $S_n$ is the $n^{th}$ partial sum of the sequence $\{a_k\}_{k=0}^\infty$.\index{sum!partial of a sequence}

Let $\{a_k\}_{k=1}^\infty$ be an infinite sequence, and $S_n$ be the partial sum of the sequence as defined above. Provided that the limit $\lim\limits_{n\to\infty} S_n$ exists, we define the infinite sum $\sum\limits_{k=1}^\infty a_k $ as
\importantFormula{
\sum_{k=1}^{\infty}a_n\eqdef \lim\limits_{n\to \infty} S_n\quad .
}
We furthermore say that the infinite sum $\sum\limits_{k=1}^{\infty}a_n$ \emph{converges} (or is \emph{convergent}). If the above limit does not exist, we may still write $\sum\limits_{k=1}^{\infty}a_n$, but we say that the sum \emph{diverges} (or is \emph{divergent}). \index{convergent!series (infinite sum)}\index{divergent!series (infinite sum)} 

We have already used infinite series to define $e^x$ in \eqref{eqExponentFunctionDefinition}. We have not proved yet that the series \eqref{eqExponentFunctionDefinition} is convergent (for all $x$). In the coming sections we fix this.

\begin{theorem}\label{thSummandsConvergentSeriesTendToZero}
Suppose $\sum\limits_{k=1}^\infty a_k$ is convergent. Then $\lim\limits_{k\to \infty } a_k=0$.
\end{theorem}

The converse of Theorem \ref{thSummandsConvergentSeriesTendToZero} is not true: for example, $\lim\limits_{n\to \infty} \frac{1}{n}=0$, but 
\begin{equation}\label{eqHarmonicSeries}
\sum_{n=1}^{\infty}\frac{1}n= 1+\frac{1}2+\frac13+\frac14+\dots +\frac1n+\dots=\infty\quad .
\end{equation}
To prove the above, let $S_n:=1+\frac{1}2+\dots +\frac{1}n$ denote the $n^{th}$ partial sum. Then we have that
\begin{equation*}
\begin{array}{rcl}
S_{2^n-1}&=&1+\frac{1}2+\frac13 +\dots +\frac{1}{2^n-1}\\
&=& 1+ \underbrace{\left(1/2+1/3\right)}_{\mathrm{2~summands}} +\underbrace{(1/4+1/5+1/6+1/7)}_{\mathrm{4~summands}}+\dots \\&&+
\underbrace{(1/2^{n-1}+1/(2^{n-1}+1)+\dots +1/(2^{n}-1)}_{2^{n-1}\mathrm{~summands}}\\
&>& 1+ \underbrace{\left(1/4+1/4\right)}_{\mathrm{2~summands}} +\underbrace{(1/8+1/8+1/8+1/8)}_{\mathrm{4~summands}}+\dots \\&& +
\underbrace{(1/2^{n}+\dots +1/2^{n})}_{2^{n-1}\mathrm{~summands}}\\
&=&1+\underbrace{  1/2+\dots +1/2}_{n-1\mathrm{~summands}}= \frac{n+1}2\quad . 
\end{array}
\end{equation*}
Therefore $\lim\limits_{n\to \infty} S_{n}=\infty $. The series \eqref{eqHarmonicSeries} is called \emph{harmonic series}.\index{series!harmonic}

Suppose $|z|<1$. Then $\sum_{n=0}^{\infty} z^n$ is convergent and we have
\importantFormula{
\label{eqGeometricProgressionSeries}
\frac{1}{1-z}=1+z+z^2+z^3+\dots +z^n+\dots= \sum_{n=0}^{\infty} z^n\quad.
}
If $|z|\geq 1$ the summand $z^n$ does not tend to $0$ and therefore the \emph{geometric series} \eqref{eqGeometricProgressionSeries} is divergent. \index{series!geometric}

Let us prove \eqref{eqGeometricProgressionSeries}. We have that 
\[(1-z)(1+z+z^2+\dots +z^{n})= 1- z^{n-1}\quad .
\]
Therefore for $z\neq 1$ we have that 
\[
S_n\eqdef 1+z+z^2+\dots +z^n=\frac{1-z^{n-1}}{1-z}\quad ,
\]
where $S_n$ is the partial sum of the geometric series \eqref{eqGeometricProgressionSeries}. By \eqref{eqLimitRtoTheNth} we know that for $|z|<1$ then $\lim\limits_{n\to \infty} z^n=0$ and therefore 
\[
\sum_{n=0}^{\infty}z^n=\lim\limits_{n=0} \frac{1-z^n}{1-z}= \frac{1}{1-z} \quad.
\]

In some high schools students study \emph{periodic decimal notation}\index{periodic!decimal notation} such as, for example, $1.\overline{17}= 1.1717171717\dots$. We are in a position to give a mathematically rigorous definition of this notation; we do that on two examples, and leave the generalization to the reader.
\[
\begin{array}{rcl}
1.\overline{17}&\eqdef&  1.171717\dots\eqdef 1+ \frac{17}{100}+ \frac{17}{100^2}+\dots +\frac{17}{100^n}+\dots\\
&=&1+\frac{17}{100}\left(1+\frac{1}{100}+\frac{1}{100^2}+\dots \right)\stackrel{\eqref{eqGeometricProgressionSeries}}{=} 1+\frac{17}{100}\frac{1}{\left(1-\frac{1}{100}\right)}=1+\frac{17}{\cancel{100}}\frac{\cancel{100}}{100-1}= 1+\frac{17}{99}= \frac{116}{99}\\
0.\overline{9}&\eqdef&  0.99999999\dots \eqdef \frac{9}{10}+ \frac{9}{10^2}+\dots +\frac{9}{10^n}+\dots\\
&=&\frac{9}{10}\left(1+\frac{1}{10}+\frac{1}{10^2}+\dots \right)\stackrel{\eqref{eqGeometricProgressionSeries}}{=} \frac{9}{10}\frac{1}{\left(1-\frac{1}{10}\right)}=\frac{\cancel{9}}{\cancel{10}}\frac{\cancel{10}}{\cancel{9}}= 1\quad .
\end{array}
\]

Compute the sum $\sum\limits_{n=2}^{\infty}\frac{1}{n(n-1)}$. Let $S_k= \sum\limits_{k=2}^{n}\frac{1}{k(k-1)}$ be the partial sum of the series. We have that $\frac{1}{k(k-1)}= \frac{1}{k-1}-\frac{1}k$ and therefore 
\[
S_n= \left(\frac11-\cancel{\frac12}\right)+\left(\cancel{\frac12}-\cancel{\frac13}\right)+\dots + \left(\cancel{\frac1{n-2}}-\cancel{\frac{1}{n-1}}\right)+\left(\cancel{\frac1{n-1}}-\frac1n\right)
=1-\frac{1}n\quad .
\]
Therefore 
\[
\sum\limits_{n=2}^{\infty}\frac{1}{n(n-1)}= \lim\limits_{n\to \infty} S_n=\lim\limits_{n\to \infty} 1-\frac{1}n=1\quad .
\]

There is no general technique for summing up arbitrary series ``in closed form''. We can carry out the summation explicitly only for several classes of hand-picked examples. 


Suppose $\sum a_n$ and $\sum b_n$ are convergent. Then so is the sum $\sum (ca_n+db_n)$, where $c, d$ are arbitrary constants, and we we have that
\begin{equation}\label{eqLinearCombinationConvergentSeriesIsConvergent}
\sum (ca_n+db_n) = c\sum a_n +d\sum b_n\quad . 
\end{equation}
The fact that $\sum a_n$ and $\sum b_n$ are divergent implies nothing about the convergence of $\sum (ca_n+db_n) $. Can you give a very simple example where $\sum a_n$ and $\sum b_n$  are both divergent but $ \sum (a_n+b_n)$ is convergent?

\begin{theorem}\label{thSeriesComparisonTest} (The comparison test)\index{series!comparison test}. Let $\{a_n\}$ and $\{b_n\}$ be non-negative sequences (i.e., $a_n\geq 0$, $b_n\geq 0$).
\begin{itemize}
\item Suppose $b_n\geq a_n$ for all $n$ and $\sum b_n$ is convergent. Then $\sum a_n$ is also convergent.
\item Suppose  $b_n\leq a_n$ for all $n$ and $\sum b_n$ is divergent. Then $\sum a_n$ is also divergent.
\end{itemize} 
\end{theorem}

Example. Prove $\sum_{n=0}^{\infty}\frac{1}{2^n+n}$ converges. 

\begin{proof}
We have that $\frac{1}{2^n+n}\leq\frac{1}{2^n}$. On the other hand $\sum_{n=0}^{\infty}\frac{1}{2^n}=2$ and therefore the sum converges.
\end{proof}


\subsection{Series - the integral test}
\begin{theorem} \label{thIntegralConvergenceTest} (The integral test).\index{series!integral convergence test} Suppose $f:[1, \infty)\to \mathbb R$ is a decreasing positive function, i.e., $f(x)\geq 0$ and $f(x)\leq f(y)$ whenever $x<y$. Then the series 
\[
\sum_{n=1}^{\infty} f(n) 
\]
converges if and only if the improper integral
\[
\int\limits_{x=1}^{\infty}f(x)dx
\]
converges. 
\end{theorem}
The theorem remains valid if the summation starts at an arbitrary index $a$ if we replace the interval of integration with $[a, \infty)$.

We recall improper integrals were defined in Section \ref{secImproperIntegrals}.

An informal proof of Theorem \ref{thIntegralConvergenceTest} can be given by considering the following picture. 
\optionalDisplay{
\psset{xunit=0.6cm,yunit=0.6cm}

\begin{pspicture}(-1,-1)(9.5,6)

\rput(5.5,4){$\underbrace{ \int\limits_{1}^\infty f(x) dx}_{\mathrm{region~under~curve}} \geq\underbrace{ \sum\limits_{n=2}^{\infty} f(n)}_{\mathrm{blue~region}}$} 

\psaxes[labels=x, ticks=x]{<->}(0,0)(-1,-1)(9,6)

\pscustom[fillstyle=solid, linestyle=none,fillcolor=gray!30]{
\psplot{1}{9}{ 5 x  div}
\psline[linestyle=none](9, 0)(1,0)
}
\psplot[linewidth=1pt]{1}{9}{ 5 x  div}

\multido{\ra=1.0+1.0}{8}{%
\psframe[fillstyle=solid, fillcolor=cyan!40](\ra,0)(! \ra\space 1 add \space 5 \ra\space 1 add div )
}
\end{pspicture}
\begin{pspicture}(-1,-1)(9,6)
\psaxes[labels=x, ticks=x]{<->}(0,0)(-1,-1)(9,6)
\rput(5.5,4){$\underbrace{ \int\limits_{1}^\infty f(x) dx}_{\mathrm{region~under~curve}}\leq\underbrace{ \sum\limits_{n=1}^{\infty} f(n)}_{\mathrm{blue~region}}$} 

\multido{\ra=1.0+1.0}{8}{%
\psframe[fillstyle=solid, fillcolor=cyan!40](\ra,0)(! \ra\space 1 add \space 5 \ra\space div )
}
\psplot[linewidth=1pt]{1}{9}{ 5 x  div}
\end{pspicture}
} %optionalDisplay

As an exercise to the reader, we suggest he or she write up a formal proof. Where do we use the fact that $f(x)$ is decreasing? Does your proof use the Monotonic sequence Theorem \ref{thMonotonicSequenceThereom} and where?

Let $p\in \mathbb R$. Prove that 
\begin{equation}\label{eqSumnToThePthConverges}
\sum\limits_{n=1}^{\infty} \frac{1}{n^p}= \doublebrace{\mathrm{converges}}{\mathrm{for~}p>1}{\mathrm{diverges}}{\mathrm{for~}p\leq 1}\quad .
\end{equation}
\begin{solution}
For $p\geq 0$ we have that $\lim_{n\to \infty} \frac{1}{n^p}$ diverges as $x\to \infty$, and the statement follows from Theorem \ref{thSummandsConvergentSeriesTendToZero}. Suppose now $p< 0$. Define $f(x)\eqdef x^{p}$. Then as $p<0$ we have $f'(x)= px^{p-1}<0$ and so $f$ is decreasing and Theorem \ref{thIntegralConvergenceTest} applies. By \eqref{eqIntegralxTopthPowerToInfty} and \eqref{eqIntegral1overxToToInfty}  the integral $\int\limits_{x=1}^{\infty} x^{-p}dx$ converges for $p>1$ and the statement follows.
\end{solution} 

Establish whether
\[ \sum\limits_{n=2}^{\infty}\frac{1}{n\ln n}
\]
converges. 

\begin{solution}
Consider $\frac{1}{x\ln x}$. We have that $\displaystyle f'(x)= -\frac{1}{x^2\ln x}- \frac{1}{x^2(\ln x)^2}<0$ for $x>2$. Therefore the sum converges if and only if $\int\limits_{x=2}^{\infty}\frac{1}{x\ln x}dx$ converges. We have that
\[ \int\limits_{x=2}^{\infty}\frac{1}{x\ln x}dx= \int\limits_{x=2}^\infty \frac{1}{\ln x}d(\ln x) = \left.\ln (\ln x)\right|_{x=2}^{\infty}= \lim\limits_{x\to \infty} \ln (\ln x ) - \ln (\ln 2)=\infty
\]
and the integral diverges.
\end{solution}

\subsection{Absolute convergence}
\begin{definition}
A series $\sum_{n=1}^\infty a_n$ is defined to be \emph{absolutely convergent} if $\sum_{n=1}^\infty |a_n|$ is convergent. \index{convergent!absolutely convergent series} If a series is convergent but not absolutely convergent, we say that the series is \emph{conditionally convergent}. \index{convergent!condionally convergent series}
\end{definition}

\begin{theorem} \label{thAbsoluteConvergenceImpliesConvergence}
If a series is absolutely convergent then it is also convergent. 
\end{theorem}

\begin{proof}
We have that $0\leq a_n+|a_n|\leq 2|a_n|$. By the definition of absolutely convergent series $\sum |a_n|$ is convergent and so is $\sum 2|a_n|$. Therefore by the comparison test $\sum a_n+|a_n|$ is convergent as well. Therefore $\sum a_n= \sum (a_n+|a_n|)-\sum |a_n|$  is convergent by \eqref{eqLinearCombinationConvergentSeriesIsConvergent}.
\end{proof}

Prove that 
\[
\sum_{n=1}^{\infty} \frac{\cos n}{n^2}
\]
is convergent.

\begin{solution}
$\sum\limits_{n=1}^{\infty} \frac{|\cos n |}{n^2} \leq \sum\limits_{n=1}\frac{1}{n^2}$. The latter is convergent by \eqref{eqSumnToThePthConverges}. As $\frac{|\cos n|}{n^2}\leq \frac{1}{n^2}$ the statement follows from Theorem \ref{thSeriesComparisonTest}.
\end{solution}

\begin{theorem}\label{thSeriesTwoSeriesRatioTest}
(The limit comparison test)\index{series!limit comparison test}. Suppose $a_n\geq 0$ and $b_n\geq 0$ for all $n$. Then if 
\[\lim\limits_{n\to \infty} \frac{a_n}{b_n}= c 
\]
for some number $c$, then either both series $\sum a_n$ and $\sum b_n $ converge or both diverge.
\end{theorem}

\begin{theorem}\label{thRatioTest}(The ratio test). \index{series!ratio test} Suppose $a_n\neq 0$ for $n\geq 2$. 
\begin{itemize}
\item If $\lim\limits_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|= L<1$ exists then the series $\sum\limits_{n=1}^\infty a_n$ is absolutely convergent.
\item If $\lim\limits_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|= L>1$ exists or $\lim\limits_{n\to\infty} \left|\frac{a_{n+1}}{a_{n}}\right| =\infty$, then the series $\sum\limits_{n=1}^\infty a_n$ is divergent.
\end{itemize}
\end{theorem}
Note that if $\lim\limits_{n\to\infty}\left|\frac{a_n}{a_{n+1}}\right|=1$, we cannot use the above theorem to deduce anything about the convergence of $\sum a_n$.

Note that the root test supposes that the limit $\lim\limits_{n\to\infty} \left|\frac{a_{n+1}}{a_{n}}\right| =L$ exists. This is a potential pitfall. To illustrate why, suppose that $\lim\limits_{n\to\infty} \left|\frac{a_{n+1}}{a_{n}}\right| =L<1$ exists. Now define $b_n:=\doublebrace{a_n}{\mathrm{if~}n~\mathrm{is~even}}{0}{\mathrm{otherwise}}$. Then $\lim \limits_{n\to\infty} \left|\frac{b_{n+1}}{b_{n}}\right|$ is not defined for even $n$ and equals 0 for odd $n$. At the same time, as $\sum |b_n|<\sum |a_n|$, the series $\sum b_n$ is absolutely convergent (comparison test) and therefore convergent. In this way $\sum b_n$ is a convergent series for which the ratio test does not apply. However, we were able to prove that $\sum b_n$ converges with a combination of the ratio test and careful use of inequalities (comparison test).

\begin{proof}[ of Theorem \ref{thRatioTest}] 
Suppose first $\lim\limits_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|= L<1 $. Let $M$ be an arbitrary number with $L<M<1$ (for example, $M=\frac{1+L}2$ works). As $\lim\limits_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|=L$ for sufficiently large $n$ we have that $\left|\frac{a_{n+1}}{a_{n}} \right|< M$. More precisely, there exists an index $N$ such that for all $n\geq N$ we have that
\[
|a_{n+1}|< Ma_{n}\quad .
\]
Therefore 
\[
|a_{N+k}|<M |a_{N+k-1}|< M^2 |a_{N+k-2}|<\dots <M^k |a_N|\quad .
\]
Therefore 
\[
\sum_{n\geq N} |a_{n}|<\sum_{k=0}^{\infty} |a_N|M^{k}= \frac{|a_N|}{1-M}\quad ,
\] 
where the last equality was proved in \eqref{eqGeometricProgressionSeries} using $0<M<1$. Our criterion now follows from Theorem \ref{thSeriesComparisonTest} and the fact that the finitely many summands $a_1, \dots, a_{N-1}$ do not affect the convergence of the series.

Suppose next $\lim\limits_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|= L>1 $. We note that the proof is very similar to the preceding one, only inequalities must be reversed.

Let $M$ be an arbitrary number with $L>M>1$ (for example, $M=\frac{1+L}2$ works). As $\lim\limits_{n\to\infty} \left| \frac{a_{n+1}}{a_{n}} \right|=L$ for sufficiently large $n$ we have that $\left|\frac{a_{n+1}}{a_{n}} \right|> M$. More precisely, there exists an index $N$ such that for all $n\geq N$ we have that
\[
|a_{n+1}|> Ma_{n}\quad .
\]
Therefore 
\[
|a_{N+k}|>M |a_{N+k-1}|> M^2 |a_{N+k-2}|>\dots >M^k |a_N|\quad .
\]
Therefore 
\[
\sum_{n\geq N} |a_{n}|>\sum_{k=0}^{\infty} |a_N|M^{k}= \lim_{k\to \infty} \frac{|a_N|(M^k-1)}{M-1}=\infty.
\] 
Our criterion now follows from Theorem \ref{thSeriesComparisonTest}.
\end{proof}

\index{exponent function} It is time to fix a debt long due: we will now prove 
\[
e^x\eqdef \sum_{n=0}^\infty \frac{x^n}{n!} 
\]
defined in \ref{eqExponentFunctionDefinition} is convergent for all $x$. 

We are using the ratio test \ref{thRatioTest}. For $x=0$, the series is equal to 1, and we have nothing to prove; suppose $x\neq 0$.  We have that 
\[
\frac{a_{n+1}}{a_{n}}= \frac{\frac{x^{n+1}}{(n+1)!}}{\frac{x^{n}}{n!}} = \frac{x}{n+1}\quad .
\]
Now $\lim\limits_{n\to \infty} \frac{a_{n+1}}{a_{n}}= \lim\limits_{n\to \infty} \frac{x}{n+1}=0$.

\begin{theorem}\label{thRootTest}(The root test). \index{series!root test}
\begin{itemize}
\item If $\lim\limits_{n\to\infty}\sqrt[n]{|a_n|}= L<1$, then the series $\sum\limits_{n=1}^\infty a_n$ is absolutely convergent.
\item If $\lim\limits_{n\to\infty}\sqrt[n]{|a_n|}= L>1$ or $\lim\limits_{n\to\infty}\sqrt[n]{|a_n|}=\infty$, then the series $\sum\limits_{n=1}^\infty a_n$ is divergent.
\end{itemize}
\end{theorem}
Note again that if $\lim\limits_{n\to\infty}\sqrt[n]{|a_n|}= 1$, we cannot use the above theorem to deduce anything about the convergence of $\sum a_n$.
\subsection{Radius of convergence, convergence of Taylor and Maclaurin series}\label{secTaylorSeriesRadiusConvergence}
In section \ref{secFormalPowerSeries}, we discussed formal power series $\sum a_n x^n$, where $x$ is an indeterminate. We saw that formal power series share the same notation as (non-formal) power series, only the notion of convergence does not apply to them. In this section, we discuss the convergence of the series obtained by substituting a number in place of the indeterminate $x$. In other words, in this section we treat the indeterminate of a formal power series as a number.

\begin{theorem}\label{thRadiusConvergence} Let $\sum\limits_{n=0}^\infty a_n x^n$ be non-formal power series (i.e., $x$ is a number, rather than indeterminate). Then one of the three mutually exclusive possibilities holds.
\begin{itemize}
\item The series converges only when $x=0$.
\item The series is absolutely convergent for all $x$.
\item There is a positive number $R$ such that the series is absolutely convergent whenever $|x|<R$ and diverges whenever $|x|>R$.
\end{itemize}
\end{theorem}
This theorem follows from the ratio test, the comparison test and the fact that absolute convergence implies convergence. We do not present the proof, however an attentive student should be able to carry out the proof on his/her own.

\begin{definition}\label{defRadiusConvergence} \index{radius of convergence}
If the third alternative of Theorem \ref{thRadiusConvergence} holds, the number $R$ is called \emph{radius of convergence} of $\sum\limits_{n=0}^\infty a_n x^n$. If the series converges only for $x=0$, we say that the radius of convergence is $0$, and if the series converges for all $x$, we say that the radius of convergence is $\infty$.
\end{definition}

Consider the power series $\sum\limits_{n=0}^\infty a_n x^n$ with radius of convergence $R$.  Suppose $\lim\limits_{n\to\infty} \frac{a_{n+1}}{a_n}$ is well defined. By the ratio test (Theorem \ref{thRatioTest}), we have that  $\sum\limits_{n=0}^\infty a_n x^n$ converges whenever $\lim\limits_{n\to\infty} \left|\frac{xa_{n+1}}{a_n}\right|<1$ and diverges whenever $\lim\limits_{n\to\infty} \left|\frac{xa_{n+1}}{a_n}\right|>1$. On the other hand, by the definition of radius of convergence, $\sum\limits_{n=0}^\infty a_n x^n$ converges whenever $|x|<R$ and diverges whenever $|x|>R$. Therefore 
\[
\lim\limits_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right|=\frac{1}{R}\quad,
\]
whenever the above limit is defined.
 
Theorem \ref{thRadiusConvergence} holds as stated for complex numbers $x\in \mathbb C$, (the absolute value of a complex number was defined in \ref{secComplexNumbers}), with an almost identical proof. We challenge the reader to attempt such a proof on his/her own.

The complex numbers $x$ for which $|x|<R$ form a disk of radius $R$, which additionally motivates the use of the word ``radius''.  

We note that Theorem \ref{thRadiusConvergence} says nothing about the convergence of $\sum a_n x^n$ when $|x|=R$. In fact the series $\sum a_n x^n$ may converge for all complex $x$ with $|x|=R$ (even though it diverges for all $x$ with $|x|>R$), it may converge for no values of $x$ for which $x=|R|$, or it may converge for some $x$ with $x=|R|$ and fail to converge for other $x$ with $|x|=R$. An example of the first case is given by $\sum\limits_{n=1}^\infty  \frac{x^n}{n^2}$, of the second case by  $\sum\limits_{n=0}^\infty  nx^n$, and of the third case by $\sum\limits_{n=1}^\infty  \frac{x^n}{n} $ (what is the radius of convergence in each case?).

We recall the Taylor of $f(x)$ at $p$ and the Maclaurin series of $f(x)$ (=Taylor series of $f(x)$ at 0) from Definitions \ref{defMaclaurinSeries} and \ref{defTaylorSeries}:
\[
\maclaurin (f(x))= \sum\limits _{n=0}^\infty \frac{f^{(n)}(0)}{n!}x^n\quad\quad\quad \taylor_p(f(x))=\sum\limits_{n=0}^\infty \frac{f^{(n)}(p)}{n!}(x-p)^n\quad .
\]

\begin{theorem}\label{thDifferentiatingIntegratingPowerSeriesNonFormal}
Suppose the power series $\sum\limits_{n=0}^\infty a_n x^n$ has radius of convergence $R$, and for $|x|<R$, let $f(x):=\sum a_n x^n$. 
\begin{itemize}
\item[(a)] $\sum\limits_{n=0}^\infty a_n n x^{n-1}$ and $\sum\limits_{n=0}^\infty a_n\frac{x^{n+1}}{n+1}$ have the same radius of convergence $x=R$ as the starting series.
\item[(b)] $f'(x)=\sum\limits_{n=0}^\infty a_n n x^{n-1}$ and $\displaystyle\int f(x)dx= C+\sum\limits_{n=0}^\infty a_n\frac{x^{n+1}}{n+1} $.
\end{itemize}
\end{theorem}
In other words, this theorem proves that taking Maclaurin series and then differentiating is the same as first differentiating and then taking Maclaurin series.

Theorem \ref{thDifferentiatingIntegratingPowerSeriesNonFormal} (a) can be proved using the ratio test. The pitfalls of the ratio test described in the remarks after \ref{thRatioTest} make a formal proof somewhat tedious (although certainly within reach of a careful student). While we propose such a proof as an exercise to the more motivated students, we shall argue the validity of  Theorem \ref{thDifferentiatingIntegratingPowerSeriesNonFormal} only on condition that $\lim\limits_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right|$ is well defined and exists. Indeed, in the remarks after the definition of radius of convergence, we showed that if $\lim\limits_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right|$ exists it necessarily equals $1/R$. At the same time, $\lim\limits_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right| = \lim\limits_{n\to\infty} \left|\frac{a_{n+1}(n+1)}{a_n n}\right|= \lim\limits_{n\to\infty} \left|\frac{a_{n+1}n}{a_n (n+1)}\right|$ and (a) follows by the ratio test. 

\begin{proof}[ of Theorem \ref{thDifferentiatingIntegratingPowerSeriesNonFormal}(b)]
By the definition of $f'(x)$, for $x<R$, we have that 
\begin{equation}\label{eqThDifferentiatingPowerSeriesProofeq1}
\begin{array}{rcl}
f'(x)&=&\displaystyle\lim\limits_{\substack{h\to 0\\|x+h|<R}} \frac{f(x+h)-f(x)}{h} = \lim\limits_{\substack{h\to 0\\|x+h|<R}} \frac{\sum\limits_{n=0}^{\infty} a_n\left( (x+h)^n-x^n \right) }{h} \\
&=& \displaystyle\lim\limits_{\substack{h\to 0\\|x+h|<R}} \frac{\sum\limits_{n=0}^{\infty} a_n\left( \cancel {x^n}+ hx^{n-1}n+ h^2(\binom{n}{2} x^{n-2}+\binom{n}{3} x^{n-3}h+\dots +h^{n-2})-  \cancel{x^n}\right) }{h}
\\
&=&\displaystyle\lim\limits_{\substack{h\to 0\\|x+h|<R}} \sum\limits_{n=0}^{\infty}\left(na_n x^{n-1}  +ha_n\left(\binom{n}{2} x^{n-2}+h\binom{n}{3}x^{n-3}h+\dots +h^{n-2}\right) \right)\\
&=&\sum\limits_{n=0}^{\infty}na_n x^{n-1}  +\displaystyle\lim\limits_{\substack{h\to 0\\|x+h|<R}}h\sum\limits_{n=0}^{\infty} a_n\left(\binom{n}{2} x^{n-2}+h\binom{n}{3}x^{n-3}h+\dots +h^{n-2}\right) 
\end{array}
\end{equation}
where in the very last equality we use the fact that by (a), the series $\sum\limits_{n=0}^{\infty}\left(na_n x^{n-1}\right)$ is convergent and therefore we can split the series in two summands. Recall from section \ref{secNewtonBinomialReview} that $\binom{n }{k}= \frac{n(n-1)\dots (n-k+1)}{k!}$ and therefore for $n\in \mathbb Z_{\geq 0}$ we have $n^2\binom{n-2}{k}\geq\binom{n}{k+2}$. As $|x|<R$, for small enough $|h|$ we have that $|x|+|h|<\frac{|x|+R}{2}$. Assume $|x|+|h|<\frac{|x|+R}{2}$. Therefore 
\[\begin{array}{rcl}
\left|\binom{n}{2} x^{n-2}+h\binom{n}{3}x^{n-3}h+\dots +h^{n-2}\right|&\leq&  n^2\left( |x|^{n-2}+
\binom{n-2}{1}|h||x|^{n-3}|h|+\dots+\binom{n-2}{k}|h|^kx^{n-2-k} \dots +|h|^{n-2}\right) \\
&=&n^2 (|x|+|h|)^{n-2}<\left(\frac{|x|+R}{2}\right)^{n-2}\quad .
\end{array}
\]
By applying (a) twice we have that $\sum\limits_{n=0}^{\infty} n^2a_n\left(\frac{|x|+R}{2}\right)^{n-2} $ is convergent and equals some number $M$. Therefore by the comparison test, so is 
\[
\sum\limits_{n=0}^{\infty} a_n\left(\binom{n}{2} x^{n-2}+h\binom{n}{3}x^{n-3}h+\dots +h^{n-2}\right)\leq \sum\limits_{n=0}^{\infty} n^2a_n\left(\frac{|x|+R}{2}\right)^{n-2}=M\quad .
\] Therefore 
\[
\left|h\sum\limits_{n=0}^{\infty} a_n\left(\binom{n}{2} x^{n-2}+h\binom{n}{3}x^{n-3}h+\dots +h^{n-2}\right)\right|< h M
\] and therefore the limit on the last line of \eqref{eqThDifferentiatingPowerSeriesProofeq1} equals 0.

This proves that $f'(x)=\sum\limits_{n=0}^\infty a_n n x^{n-1}$. The fact that $\displaystyle\int f(x)dx= C+\sum\limits_{n=0}^\infty a_n\frac{x^{n+1}}{n+1} $ now follows immediately: we already proved that we can differentiate the right hand side one term at a time, and the result is the left hand side by direct computation.
\end{proof}

\begin{theorem}[Taylor's inequality] Let $f$ be an $(N+1)$ times differentiable function. Suppose for $|x-a|<d$, $f^{(N+1)}(x)\leq M$. Then 
\[
\left|f(x)-\sum\limits_{n=0}^N \frac{f^{(n)}(a)}{n!}(x-a)^n\right| \leq \frac{M}{(n+1)!} |x-a|^{n+1} 
\]
for $|x-a|\leq d$.
\end{theorem}
\begin{proof}
For $|t-a|< d$ we have
\[
f^{(N+1)}(t) \leq  M\quad.
\]
Apply $\int\limits_{a}^{x} \bullet dt $. We apply the fundamental theorem of calculus and replace $x$ by $t$ to get
\[
f^{(N)}(t)-f^{(N)}(a)\leq (t-a) M
\]
or 
\[
f^{(N)}(t)\leq f^{(N)}(a)+(t-a) M
\]
Apply again $\int\limits_{a}^{x} \bullet dt $ and replace $x$ by $t$ to get
\[
f^{(N-1)}(t)\leq f^{N-1}(a) +f^{(N)}(a)(t-a)+ M\frac{(t-a)^2}2\quad .
\]
Suppose by induction we have proved that 
\[
f^{(N-k)}(t)\leq f^{N-k}(a) +f^{(N-k+1)}(a)(t-a)+ f^{N-k+2}\frac{(t-a)^2}2+\dots +f^{N}\frac{(t-a)^{k}}{k!}+ M\frac{(t-a)^{k+1}}{(k+1)!} \quad .
\]
We apply $\int\limits_{a}^{x} \bullet dt $ and replace $x$ by $t$ to get
\[
f^{(N-k-1)}(t)\leq f^{N-k-1}(a) +f^{(N-k)}(a)(t-a)+ f^{N-k+1}\frac{(t-a)^2}2+\dots +f^{N}\frac{(t-a)^{k+1}}{(k+1)!}+ M\frac{(t-a)^{k+2}}{(k+2)!} \quad .
\]
In this way by induction we get that 
\[
\left(f(x)-\sum\limits_{n=0}^N \frac{f^{(n)}(a)}{n!}(x-a)^n\right)\leq \frac{M}{(n+1)!} |x-a|^{n+1} \quad.
\]
In an analogous fashion we can prove that 
\[
\left(f(x)-\sum\limits_{n=0}^N \frac{f^{(n)}(a)}{n!}(x-a)^n\right)\geq -\frac{M}{(n+1)!} |x-a|^{n+1} \quad.
\]
which completes the proof of the theorem.
\end{proof}

\section{Polar coordinates}
Polar coordinates of a point $P=(x,y)$ in the plane are any two numbers $(r, \theta)$, $r\geq 0$, for which $(x,y)=(r\cos\theta, r\sin \theta)$. In other words, the relation between the Cartesian coordinates $(x,y)$ and the polar coordinates $(r,\theta)$ of a point $P$ is given by 
\[
\left|\begin{array}{rcl}
x&=&r\cos \theta\\
y&=&r\sin\theta
\end{array}\right.
\]

In these lectures, we allow polar coordinates with $ r<0$. Whether one allows $r<0$ is a matter of convention, which might vary between computer algebra systems (if in doubt, assume that $r<0$ is not allowed).

To obtain polar coordinates from Cartesian coordinates we compute that $x^2+y^2= r^2(\cos^2 \theta+\sin^2\theta)=r^2$ and therefore $r=\pm\sqrt{x^2+y^2}$. Furthermore we have that $\frac{y}{x}= \frac{\cancel{r}\sin \theta}{\cancel{r}\cos \theta}=\tan \theta$. To summarize:
\[
\left|\begin{array}{rcl}
r&=&\pm\sqrt{x^2+y^2}\\
\tan\theta&=&\frac{y}{x} \quad \mathrm{if~}x\neq 0 \quad .
\end{array}\right.
\]
Note that the equality $\tan \theta = \frac{y}{x}$ is not always defined. Also note that  $\tan \theta=\frac{y}{x}$ does not determine $\theta$ uniquely, as it is not clear in which interval $(k\pi -\frac{\pi}{2}, k\pi+\frac{\pi}{2})$, $k\in \mathbb Z$ does $\theta$ lie.

$r$ denotes the distance from $P$ to the origin and $\theta$ denotes the angle between the segment $OP$ and the $x$-axis, as indicated on the picture below.

\optionalDisplay{
\psset{xunit=1cm,yunit=1cm}
\begin{pspicture*}(-1.5,-1.5)(5,4)
\psline[linecolor=gray](-1.5,0)(2.5,0) % x-axis
\psline[linecolor=gray](0,-1.5)(0,2.5) % y-axis
\parametricplot[linecolor=red]{0}{60}{t cos 0.25 mul t 1000 div 1 add mul t sin 0.25 mul t 1000 div 1 add mul} 
\parametricplot[linecolor=red, arrows=->]{0}{780}{t cos 0.55 mul t 1000 div 1 add mul t sin 0.55 mul t 1000 div 1 add mul} 
\psline(0,0)(2,  3.464)
\rput(2,  3.464){$\bullet$}
\rput[lt](2,  3.264){$P= (r\cos \theta, r\sin \theta)$}
\rput(0.35,  0.25){$\theta$}
\end{pspicture*}
} %optionalDisplay

Polar coordinates are not unique: if $(r, \theta)$ are polar coordinates of $P=(x,y)$, then so are $(r, \theta+2\pi)$.

We have already encountered polar coordinates when studying polar form of complex numbers. Let $z\eqdef x+iy$, $x,y\in \mathbb R$. Then we recall from section \ref{secPolarFormComplexNumbers} that there exist real numbers $\rho$, $r\eqdef e^\rho$ and $\theta$ such that $z=x+iy= e^{\rho+i\theta}= e^\rho(\cos \theta+i\sin \theta)= r(\cos \theta + i\sin \theta)$. We recall the latter equality is the polar form of the complex number $z$. Then $(r, \theta)$ are the polar coordinates of the point $(x,y)$. 

The relationship between polar form of complex numbers and polar coordinates can be summed up in the following equalities.
\[
\begin{array}{rclclcl}
r(\cos \theta+i\sin\theta) &=&  x+iy&=& z&=&\Re (z)+i\Im(z)\\
r\cos\theta&=&x&=&\Re(z)\\
r\sin\theta&=&y&=&\Im(z)\\
\theta&&&=& \arg (z)\\
\tan \theta &=&\displaystyle\frac{y}{x}&=&\displaystyle\frac{\Im (z)}{\Re (z)}\\
r&=&\sqrt{x^2+y^2}&=& |z|&=&\sqrt{z\bar z}\\
\end{array}
\]

\index{cardioid}\index{spiral} Sometimes a curve is given in polar coordinates by specifying a relationship between $r$ and $\theta$, such as $r=\theta$ (spiral) or $r=1+\sin \theta$ (cardioid), $r=2\cos \theta$ (circle of radius one centered at $(1,0)$).

\optionalDisplay{
\psset{xunit=1cm,yunit=1cm}
\begin{pspicture*}(-5,-5)(5,5)
\rput (3,3){$r=\theta$}
\psaxes[labels=none, ticks=x]{<->}(0,0)(-4.5,-4.5)(4.5,4.5)
\parametricplot[linecolor=red, plotpoints=500]{0}{720}{t cos t mul 180 div t sin t mul 180 div } 
\end{pspicture*}

\psset{xunit=1cm,yunit=1cm}
\begin{pspicture*}(-5,-5)(5,5)
\rput (3,3){$r=1+\sin\theta$}
\psaxes[labels=none, ticks=x]{<->}(0,0)(-4.5,-4.5)(4.5,4.5)
\parametricplot[linecolor=red, plotpoints=500]{0}{360}{1 t sin add t cos mul
 1 t sin add t sin mul} 
\end{pspicture*}

\psset{xunit=1cm,yunit=1cm}
\begin{pspicture*}(-5,-5)(5,5)
\rput (3,3){$r=2\cos\theta$}
\psaxes[labels=none, ticks=x]{<->}(0,0)(-4.5,-4.5)(4.5,4.5)
\parametricplot[linecolor=red, plotpoints=500]{0}{360}
{ t cos 2 mul t cos mul
  t cos 2 mul t sin mul} 
\end{pspicture*}
} %optionalDisplay

\index{area!enclosed by curve in polar coord.} Let $r=f(\theta), \theta\in [a,b]$ be a curve given in polar coordinates.  Let $A$ denote the figure given by the union of the segments connecting each point the curve to the origin. Suppose no two points on the curve lie on the same ray from the origin. Then the area of $A$ is given by 
\importantFormula{\label{eqAreaUnderPolarCurve}
area(A)\eqdef \int\limits_{\theta=a}^b \frac{f(\theta)^2}{2}d\theta\quad .
}

A motivation for the above definition may be given as follows. 
\psset{xunit=1cm,yunit=1cm}

\optionalDisplay{
\begin{pspicture*}(-4,-1)(7.3,5.4)
\newcommand{\dThetA}{20}
\rput(4,5){$r=f(\theta)$}
\psaxes[labels=none]{<->}(0,0)(-3.5,-0.5)(4.5,5.2)
\multido{\rb=10+\dThetA}{6}{%
\pstVerb{/rR \rb\space 30 div 2 add def
/rRPlusTheta \rb\space \dThetA\space add 30 div 2 add def
}
\pscustom[fillcolor=blue!30, fillstyle=solid, linecolor=blue]{\psline(0,0)(! rR \rb \space cos mul rR \rb \space sin mul) (! rRPlusTheta \rb \space\dThetA\space add cos mul rRPlusTheta \rb \space \dThetA\space add sin mul) }
}
\parametricplot[linecolor=red, plotpoints=500]{10}{130}
{ t 30 div 2 add  t cos mul
  t 30 div 2 add  t sin mul} 
\parametricplot[linecolor=red]{30}{50} %{! \dThetA 10 add}
{t cos 1 mul
 t sin 1 mul } 
\rput[lt](0.8, 0.8){\tiny$\Delta\theta$}
\parametricplot[linecolor=red, arrows=->]{0}{30} %{! \dThetA 10 add}
{t cos 0.5 mul
 t sin 0.5 mul } 
\rput[lt](0.55,0.3){\tiny$\theta_1$}
\parametricplot[linecolor=red, arrows=->]{0}{50} %{! \dThetA 10 add}
{t cos 0.85 mul
 t sin 0.85 mul } 
\rput[lt](0.85,0.4){\tiny$\theta_2$}

\rput[tr] (-0.1,-0.1){$O$}
\rput[tl] (2.7,1.5){$P=(f(\theta_1)\cos \theta_1, f(\theta_1)\sin \theta_1)$}
\rput[tl] (2.51,2.95){$Q=(f(\theta_2)\cos \theta_2, f(\theta_2)\sin \theta_2)$}
\end{pspicture*}
} %optionalDisplay

Let $N$ be a large number and split the interval $[a,b]$ using $N+1$ equally spaced points $a=\theta_0\leq\theta_1\leq\dots\leq\theta_{N-1}\leq \theta_N=b$ into $N$ equal segments, each of the form $[\theta_j,\theta_{j+1}] $. Denote the length of each such segment by $\Delta\theta$, i.e., let $\Delta\theta\eqdef \frac{b-a}{N}$. Then the area of $A$ is approximated by triangles with vertices on the curve of the form $(f(\theta_j)\cos \theta_j, f(\theta_{j+1} )\cos \theta_{j+1})$ as drawn in the figure. Consider one such triangle, $OPQ$, as indicated in the figure. The area of triangle $OPQ $ is $\frac{|OP| |OQ|\sin (\Delta\theta)}{2}= \frac{f(\theta_1)f(\theta_2)\sin (\Delta\theta)}{2} $. In other words the area of $A$ is approximated by
\[
\sum_{j=0}^{N-1} \frac{f(\theta_j)f(\theta_{j+1})\sin (\Delta\theta)}{2}= \frac{\sin(\Delta\theta)}{\Delta\theta}\sum_{j=0}^{N-1} \frac{f(\theta_j)f(\theta_{j}+\Delta\theta)\Delta\theta}{2}\quad .
\]
In the above sum, the multiplicand $\frac{\sin(\Delta\theta)}{\Delta\theta}$ tends to $1$ as $\Delta\theta$ tends to $0$. Therefore as $\Delta$ tends to zero, the expression tends to the limit of the second multiplicand. On the other hand, one can show that the second multiplicand approximates the integral $\int\limits_{\theta=a}^b \frac{f(\theta)^2}{2}d\theta$. This motivates definition of area given in \eqref{eqAreaUnderPolarCurve}.

Curve length of curve of the form $\gamma:|r=f(\theta),\theta\in [a,b]$. Then $(x,y)= (r\cos\theta, r\sin \theta)= (\underbrace{f(\theta)\cos\theta}_{=:g_1(\theta)}, \underbrace{ f(\theta)\sin\theta}_{=:g_2(\theta)} $. Recall the definition of curve length \eqref{eqDefLengthCurve}:
\[
\int\limits_{a}^b\sqrt{\left(\frac{dg_1}{d\theta}\right)^2+\left(\frac{dg_2}{d\theta}\right)^2}d\theta \quad.
\]
We substitute $g_1(\theta)=f(\theta)\cos\theta$, $g_2(\theta)=f(\theta)\sin\theta$ and carry out the operations:
\[
\begin{array}{l}
\displaystyle\int\limits_{a}^b\sqrt{ (-\sin\theta f(\theta) + \cos\theta f'(\theta) )^2+(\cos\theta f(\theta)+\sin \theta f'(\theta) )^2}d\theta=\\ 
\displaystyle \int \limits_{a}^b \sqrt{\sin^2\theta f^2(\theta)- \cancel{2\sin\theta\cos\theta f'(\theta)f(\theta)}+\cos^2\theta (f'(\theta))^2+ \cos^2\theta f^2(\theta)+\cancel{2\sin\theta\cos\theta f'(\theta)f(\theta)} \sin^2 (f'(\theta))^2 }d\theta\quad .
\end{array}
\]
Regrouping the above expression using $\sin^2\theta+\cos^2\theta=1$ we finally get that the length of a curve given in polar coordinates as $r=f(\theta)$ is computed as
\importantFormula{\label{eqCurveLengthPolarCoords}
\int\limits_{a}^b \sqrt{f^2(\theta)+ (f'(\theta))^2}d\theta
}

Find the area of the figure that lies outside of figure enclosed by $r_1=3\cos\theta_1, \theta_1\in [0,\pi)$ and inside the figure enclosed by $r_2=1+\cos\theta_2, \theta\in [-\pi,\pi)$. %Find the perimeter of the curve.

\begin{solution}
\optionalDisplay{
\begin{pspicture}(-1, -4)(4,4)
\pscustom*[fillcolor=red!20, linecolor=red!20]{
\parametricplot[linecolor=red, plotpoints=1000]{-60}{60}{3 t cos mul t cos mul 3 t cos mul t sin mul}
\parametricplot[linecolor=red, plotpoints=1000]{60}{-60}{1 t cos add t cos mul 1 t cos add t sin mul}
}
\parametricplot[linecolor=red, plotpoints=1000]{0}{360}{1 t cos add t cos mul 1 t cos add t sin mul}
\parametricplot[linecolor=red, plotpoints=1000]{0}{360}{3 t cos mul t cos mul 3 t cos mul t sin mul}
\psaxes[labels=none]{<->}(0,0)(-1,-2.5)(3.5,2.5)
\rput[c](0.75,1.299){$\bullet$}
\rput[c](0.75,-1.299){$\bullet$}
\rput[b](0.85,1.51){$\left(\frac{3}4,\frac{3\sqrt{3}}{4} \right)$}
\rput[t](0.85,-1.51){$\left(\frac{3}4,-\frac{3\sqrt{3}}{4} \right)$}
\psline[linestyle=dotted](0,0)(0.75,1.299)
\psline[linestyle=dotted](0,0)(0.75,-1.299)
\end{pspicture}
} %optionalDisplay

We first plot the curves using computer to get the above picture. We see that the curve $r_1=3\cos\theta_1$ is a circle and that $r_2=1+\cos\theta_2$ is a cardioid (rotated at an angle of $-\frac{\pi}{2}$). Let us verify that indeed $r_1=3\cos\theta_1$ is a circle. Multiplying by $r_1$ we get $r_1^2=3r_1\cos\theta_1= 3x$. Therefore $x^2+y^2-3x=0$, or, after completing the square, $(x-\frac{3}{2})^2+y^2=\frac{9}{4}$. Thus $r_1=3\cos\theta_1$ is a circle centered at $(\frac{3}{2},0)$ and with radius $\frac{3}{2}$. From the picture we see that the two curves intersect in three points, one of which is the origin. We get an additional intersection of the two curves when $r_1=r_2$, $\theta_1=\theta_2$ and $1+\cos\theta=3\cos\theta$, where we have set $\theta:=\theta_1=\theta_2$. In other words, $\cos\theta=\frac{1}{2}$ and as $\theta\in [0,\pi)$, we get $\theta=\pm \frac{\pi}{3}$. Thus we have a second intersection point $x=\frac{3}{4}$, $y=\frac{3\sqrt{3}}{4}$. By symmetry with respect to the $x$ axis we get that the final intersection point is $(x,y)=(\frac{3}4,-\frac{3\sqrt{3}}{4}) $.

Therefore the area of the enclosed curve is given by the area of the figure obtained by connecting $r=3\cos\theta, \theta\in(-\frac{\pi}{3}, \frac{\pi}{3})$ to the origin minus the area of the figure obtained by connecting $r=1+\cos\theta, \theta\in(-\frac{\pi}{3}, \frac{\pi}{3})$ to the origin. We can now use \eqref{eqAreaUnderPolarCurve} to get the answer

\[
\begin{array}{rcl}
\displaystyle\int\limits_{-\frac{\pi}{3}}^{\frac{\pi}{3}} \frac{1}2\left(9\cos^2\theta-(1+\cos\theta)^2\right) d\theta&=& \displaystyle\int\limits_{-\frac{\pi}{3}}^{\frac{\pi}{3}} \frac{1}2\left(8\underbrace{\cos^2\theta}_{=\frac{1+\cos(2\theta)}2  }-1-2\cos\theta \right) d\theta\\
&=&\displaystyle\frac{1}2\int\limits_{-\frac{\pi}{3}}^{\frac{\pi}{3}} \left(4+4 \cos 2\theta-1-2\cos\theta \right) d\theta
\\
&=&\pi+\left.\sin2\theta\right|_{\theta=-\frac{\pi}{3}}^{\theta=\frac{\pi}3}-\left.\sin\theta\right|_{\theta=-\frac{\pi}{3}}^{\theta=\frac{\pi}3}\\
&=&\pi
\end{array}
\]
\end{solution}

\section{Index}
\textbf{The index is incomplete at the moment. }
\printindex
\end{document}