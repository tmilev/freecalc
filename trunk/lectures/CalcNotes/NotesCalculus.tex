\documentclass[12pt]{book}
%Notes.
%EITHER 1. Compile with the dvi->ps->pdf toolchain
%OR     2. Compile with pdflatex -shell-escape

%Notes: modify command \optionalDisplay to speed up computation times.
\usepackage{etex}
\usepackage{amsmath, amsfonts, amssymb, verbatim, hyperref,ifthen}
\usepackage{amsthm}
\usepackage{makeidx}

\usepackage{pst-math}
\usepackage{pst-solides3d}
\usepackage{pst-3dplot}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{cancel}
\usepackage{caption}
\usepackage{longtable}
\usepackage{etoolbox}
\usepackage{comment}
\usepackage{cleveref}

% To make everything into one file, we include the following style file directly.
% This might need to be updated over time.
\input{../pstricks-commands}
\input{../example-templates}
\renewcommand{\optionalDisplay}[1]{}

\crefformat{footnote}{#2\footnotemark[#1]#3}

\newtoggle{solutions}
\newtoggle{solutionsExtra}
\newtoggle{answers}
\toggletrue{answers}
\toggletrue{solutions}

\newcommand{\fcProblemRef}{\theproblem.\theenumi}
\newcommand{\fcSubProblemRef}{\theenumi.\theenumii}

\renewcommand{\Im}{\mathrm{Im}}
\renewcommand{\Re}{\mathrm{Re}}
\newcommand{\eqdef}{\textbf{:=}}
\newcommand{\eqAttention}{\stackrel{(!)}{=}}
\newcommand{\importantText}[1]{ \framebox{#1}}
\newenvironment{tableFixed}{~\\~\medskip\begin{minipage}{\textwidth}\captionsetup{type=table} }{ \medskip \end{minipage} \medskip }
\newenvironment{figureFixed}{~\\~\medskip\begin{minipage}{\textwidth} \captionsetup{type=figure} }{ \medskip \end{minipage} \medskip }

%\newenvironment{proof}[1][]{ \textbf{Proof#1.} }{$\Box$\medskip}
\newenvironment{proofOptional}[1][]{ \noindent \textbf{(Optional) Proof#1.}}{$\Box$\medskip}
\newenvironment{solution}{\medskip\noindent\textbf{Solution.} }{$\Box$}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{prop}[theorem]{Proposition}

\DeclareMathOperator{\arccot}{arccot}
\DeclareMathOperator{\arcsec}{arcsec}
\DeclareMathOperator{\arccsc}{arccsc}
\DeclareMathOperator{\arcsinh}{arcsinh}
\DeclareMathOperator{\arccosh}{arcosh}
\DeclareMathOperator{\arctanh}{arctanh}
\renewcommand{\Arcsin}{\arcsin}
\renewcommand{\Arccos}{\arccos}
\renewcommand{\Arctan}{\arctan}
\renewcommand{\Arccot}{\arccot}
\renewcommand{\Arcsec}{\arcsec}
\renewcommand{\Arccsc}{\arccsc}


\newcommand{\doublebrace}[4]{\left\{\begin{array}{ll} #1 & #2 \\#3 & #4  \end{array} \right.}

\newcommand{\optionalMaterial}{\textbf{(Optional)}}
\renewcommand{\emph}{\textbf}
\newcommand{\additionalProblemLabel}{}

\newcommand{\answer}[1]{\iftoggle{answers}{ \hfill{~} \rotatebox{180}{\tiny answer: #1}}{}} %
\newcommand{\hiddenanswer}[1]{\iftoggle{solutions}{ %
\iftoggle{answers}{ %
\hfill{~} \rotatebox{180}{\tiny answer: #1}}
{}} %
{}} %
\newcommand{\points}[1]{\item}
\newcommand{\pointsii}[1]{\item}

%\newcommand{\diff}{\text{d}}

\makeindex
\author{Authors: the freecalc project contributors \\~\\ Notes composition started by Todor Milev.}
\title{Calculus for beginners \\Draft version \\ ~\\
\[
\lim\limits_{x\to 5^+}\frac{1}{x-5}= ?
\]
~\\
\[
\lim\limits_{x\to 8^+}\frac{1}{x-8}=\infty
\]
~\\
$\Rightarrow$
~\\
\[
\lim\limits_{x\to 5^+}\frac{1}{x-5}= \begin{postscript}\rotatebox{90}{5}\end{postscript}
\]
}
\begin{document}
\maketitle

{
~

\bigskip

~

\bigskip

~

\bigskip

~

\bigskip

\begin{center}
\Huge{\color{red}\rotatebox{45}{\textbf{ DRAFT VERSION}}}
\end{center}
} %end centering

%\noindent \textbf{Study materials. } The present notes and the textbook by James Stewart, Calculus, 7th edition, published by Brooks Cole, 2012. ISBN-13: 978-0-538-49781-7, ISBN-10: 0-538-49781-5.


\chapter*{Preface}


\section*{Disclaimer}
This text is in preparation and does contain typos and errors. Please read critically!

\section*{Additional materials} Students are strongly encouraged to look up mathematics on Wikipedia.


\section*{License to redistribute and modify} This text and its source code is licensed to you under the Creative Commons license CC BY 3.0. The license states that you are free
\begin{itemize}
\item to Share - to copy, distribute and transmit the work,
\item to Remix - to adapt, change, etc., the work,
\item to make commercial use of the work,
\end{itemize}
as long as you reasonably acknowledge the original project (a notice of use of the freecalc project is sufficient). For more details see the full text of the license \url{http://creativecommons.org/licenses/by/3.0/}.

\noindent \textbf{Compiling the .tex file: a note to the student.} If you would like to modify this file, you need to learn how to use \LaTeX. Note that all graphics in the notes are drawn using commands in the .tex source (the text uses no external images). The .tex file should compile ``out of the box'' on a Linux distribution with \LaTeX~ installed.

A semi-permanent link of the .tex source of this textbook can be found here. If the link is no longer functioning, please search for ``freecalculus'' project on the web.

\url{https://sourceforge.net/projects/freecalculus/}

\section*{Using the textbook}

Some material in this book is marked as \optionalMaterial. Such material is considered to be outside of the scope of a regular Calculus course, yet may be needed to either provide a more rigorous exposition, or to provide a broader perspective on the topic.

While material marked as \optionalMaterial ~should be accessible to a motivated student, it is expected to be of higher difficulty than the rest of the book. The author does not recommend the material marked as \optionalMaterial ~ as a requirement for successful completion of a course in Calculus.

\section*{Scope of the textbook}
This textbook does not attempt a rigorous exposition of the subject. However, all sacrifices of rigor have been made by omitting material and giving informal explanations of proofs of theorems. It has been the goal of this textbook to use completely standard, precise, and, even if slightly simplified, professional mathematical language.

As the most important example of this we note that, while we have avoided a formal definition of differential forms, all statements about differential forms are fully compatible with the commonly accepted professional mathematical definition. To the calculus instructors reading this text, we recall that differential forms are defined as the dual (in the sense of linear algebra) vector space of the tangent vector space, which in turn is most frequently defined using linear differential operators on manifolds.

\tableofcontents

%\begin{comment}




%Here is a list of common notation and abbreviations. %This list will grow as the course progresses. Feel free to request additional notation.
%\begin{itemize}
%\item The expression $x\in Y$ is read ``$x$ belongs to $Y$''.
%\item $\mathbb Z$  denotes the integers.
%\item $\mathbb Z_{\geq 0}$ denotes the non-negative integers.
%\item $\mathbb R$ denotes the real numbers.
%\item $\mathbb R^2$ denotes the two dimensional vector space, i.e., the set of pairs of real numbers $(r_1, r_2)$. Such a pair of real numbers is usually referred to as a 2-dimensional vector. The numbers $r_1, r_2$ are called coordinates.
%\item $\mathbb R^3$ denotes the two dimensional vector space, i.e., the set of triples of real numbers $(r_1, r_2, r_3)$. Such a triple of real numbers is usually referred to as a 3-dimensional vector. The numbers $r_1, r_2, r_3$ are called coordinates.
%\item $\mathbb R^n$ is the generalization of $\mathbb R^2$, $\mathbb R^3$, for arbitrary number of coordinates.
%\item $\mathbb C$ denotes the complex numbers, defined in section \ref{secComplexNumbers}.
%\item $\displaystyle\sum_{k=1}^n f(k)\eqdef f(1)+f(2)+\dots +f(k-1)+f(k)$ is read ``sum of $f(k)$ as $k$ runs over the values $1,2,\dots, n$''. Below the sum sign we indicate the variable which varies as we take the sum. This variable is often called ``dummy variable''.
%\item $\displaystyle\int\limits_{x=a}^b f(x)\diff x$ is read ``integral of $f(x)\diff x$ as $x$ runs in the interval $[a,b]$''. The similarities in the notation for $\sum$ and the $\int$ (can you spot such similarities?) are not a coincidence!
%\end{itemize}

\chapter{Basic mathematical language}


\section{Equalities vs equalities by definition }
Throughout this textbook, we use $\eqdef$ to define a formula, and we use $=$ to derive/deduce a formula. For example, when we write
\[ 2+2=4
\]
we mean that we derive this equality (in this case, through the rules of elementary school arithmetics). When we derive an equality, both sides of the equality are already known objects (in the above case, the right hand side is the number $4$ and the left hand side is an expression built by the operation $+$ and two copies of the number $2$).  On the other hand, when we write
\[
\tan x\eqdef\frac{\sin x}{\cos x}\quad ,
\]
we do not deduce the above formula; rather, we \emph{define} the expression $\tan x$ via the expression $\displaystyle \frac{ \sin x}{\cos x}$.

\textbf{A note to computer scientists. } In many of the modern computer programming languages (for example, C, C++, java, Python, javascript), the equality sign is used in a way that does not corresponds to either $=$, or to $\eqdef$. In those programming languages, the $=$ sign instructs a computer to change the state of a given variable. This programming language operation does not have a corresponding symbolic mathematical notation; instead it roughly corresponds to the mathematical use of the expression ``Set $x$ to be \dots ''.
\section{Sets and elements}
\url{http://en.wikipedia.org/wiki/Set_(mathematics)}

In mathematics, a \emph{set} is a collection of distinct objects, considered as an object in its own right. The objects contained by a set are also called \emph{elements} of the set. We note that in the definition of a set, the words ``collection'', ``distinct'' and ``object'' are used as English language words, rather than as mathematical terms. In this way, the mathematical notions of sets and an elements of a set are based on the common sense and understanding of the reader.

Sets are customarily denoted by using curly brackets enclosing the elements of the set. For example,
\[
\{1,2,5\}
\]
is the set consisting of the three elements $1,2,5$. When an element $x$ is contained in a set $X$, we write
\[
x\in X
\]
and we read it as ``$x$ belongs to $X$''. Alternatively, we may say that $x$ lies in $X$ or $x$ is in $X$. When and element $y$ does not belong to a set $X$, we write
\[
y\notin X\quad.
\]
For example, $1\in \{1,2,5\}$ and $3\notin \{1,2,5\}$.
\section{Functions: domains, codomains, ranges} \label{secInverseFunctionBasics}

\url{http://en.wikipedia.org/wiki/Injective_function} \quad .


\begin{definition}\index{function} A function $f$ is a rule that assigns to each element $x$ in a set $D$ exactly one element, called $f(x)$, in a set $C$. We write
\[
f: D\to C \quad .
\]
We call the set $D$ the \emph{domain} of $f$ and $C$ the \emph{codomain} of $f$.
\end{definition}
We can imagine a function as a machine that takes input from its domain (``input set''), and produces output located in its codomain (``target set''). Furthermore, for each object at the input, exactly one output object is produced.
\begin{figureFixed}
\optionalDisplay{
\psset{xunit=0.6cm,yunit=0.6cm}
\begin{pspicture}(0,0)(5,5)
\rput[r](-1.2,0){$D$}
\psellipse*[linecolor=cyan](0,0)(1, 3)
\fcFullDot{0}{2.5}
\fcFullDot{0}{1.5}
\fcFullDot{0}{0.5}
\fcFullDot{0}{-0.5}
\fcFullDot{0}{-1.5}
\fcFullDot{0}{-2.5}
\rput[t](0, -3.2){Domain}

\rput[l](6.2,0){$C$}
\psellipse*[linecolor=cyan](5,0)(1, 3)
\fcFullDot{5}{2.5}
\fcFullDot{5}{1.25}
\fcFullDot{5}{0}
\fcFullDot{5}{-1.25}
\fcFullDot{5}{-2.5}
\fcFullDot{5}{-2.5}
\rput[t](5, -3.2){Co-domain}
\rput(2.5, 2.5){$f$}
\psline[linestyle=dashed]{->}(0,2.5)(5,1.25)
\psline[linestyle=dashed]{->}(0,1.5)(5,-1.25)
\psline[linestyle=dashed]{->}(0,0.5)(5,-2.5)
\psline[linestyle=dashed]{->}(0,-0.5)(5,2.5)
\psline[linestyle=dashed]{->}(0,-1.5)(5,-1.25)
\psline[linestyle=dashed]{->}(0,-2.5)(5,2.5)
\end{pspicture}


} %end optional display
\caption{A function $f$ can be represented as a set of arrows from its domain to its co-domain.\label{figFunctionAsArrows}}
\end{figureFixed}
We call the elements of the domain the \emph{arguments} of the function. For an element $x\in D$, we write $f(x) $ to denote the output of the function $f$ with input $x$. We call $f(x)$ the \emph{image} of $x$ under $f$; we may alternatively say that $f$ \emph{maps} $x$ to $f(x)$.

It is not required that all elements in the codomain be obtainable by applying $f$ to the input. For example, for the function $f$ given in Figure \ref{figFunctionAsArrows}, one of the elements of the codomain $C$ is not the image of any element of the domain $D$ (which element?).

\begin{definition}\index{function!range}
Given a function $f:D\to C$, the set
\[
 \{f(x) | x\in D \},
\]
is called the \emph{range} of $f$.
\end{definition}
In other words, the range of $f$ is the set of all images of elements in the domain of $f$.
\section{Inverse functions}

\begin{definition} ~
\begin{itemize}
\item \index{injection}\index{one to one} A function $f$ is \emph{injective (or one to one)} if  for all $x_1\neq x_2$ we have that $f(x_1)\neq f(x_2)$.
\item \index{function!preimage} If $f(x)=r$ we say that $x$ is a \emph{pre-image} of $r$.
\item \index{surjection}
A function $f$ is a \emph{surjection (or onto)} if for every element $r$ in the codomain $C$, there exists an element $x$ with $f(x)=r$.
\item \index{bijection} A function is a \emph{bijection (or one to one and onto)} if it is both a surjection and an injection.
\end{itemize}
\end{definition}
In other words, $f$ is injection if it sends different elements to different elements, and $f$ is a surjection if all elements in its  codomain (``target set'') are obtained as images of the function $f$. In this way, $f$ is bijection if it sends different elements to different elements and every element in the codomain of $f$ has a preimage.

Let $f: \mathbb R\to \mathbb R$ (``$f$ maps the set of real numbers to the set of real numbers''). Let $y=f(x)$ be the graph of $f$.
\begin{criterion}[Horizontal line test]
$f$ is injective if every horizontal line $y=const$ intersects the graph $y=f(x)$ no more than once.
\end{criterion}

\begin{definition}\index{function!inverse}
Let $f:X\to Y$ be a function from a set $X$ onto its codomain $Y$, such that $f$ is a bijection (different elements are sent to different ones and every element in the target set has a preimage). Then the function $g: Y\to X$ is called the inverse of $f$ if $g(f(x))=x$ for all $x\in X$ and $f(g(y))=Y$ for all $y\in Y$.
\end{definition}

The inverse of $f$ ``undoes'' the action of $f$.

\subsection{Inverse function notation }
\noindent The inverse of $f$ is denoted as $f^{-1}$. This notation is one of the most frequent causes of student confusion. In attempt to resolve that confusion we make a few important notes on notation. First and foremost,

\importantText{
do not confuse $f^{-1}(x)$ with $\frac{1}{f(x)} =\left(f(x)\right)^{-1}$.
}

\noindent The two notations are different: the position of the superscript $^{-1}$ is different for $f^{-1}(x)$ and $\left(f(x)\right)^{-1}$.

It is important to note that the notation $f^{2}(x)$ is an abbreviation for $(f(x))^2$,  $f^{3}(x)$ is an abbreviation of $(f(x))^3$, and so on. Nevertheless, $f^{-1}(x)$ is not the abbreviation of $\left(f(x)\right)^{-1}$ and hence does not follow the pattern. The mathematical use of superscripts of functions follows the following rules.
\[
f^{n}(x)= \left\{ \begin{array}{ll}\text{stands for } \left(f(x)\right)^n & \text{when } n=1,2,3,\dots \\
\text{stands for inverse of } f \text{ applied to }x&\text{when } n=-1\\
\text{should be avoided }  & \text{when } n\neq -1, 1,2,3,\dots .
\end{array} \right.
\]
\textbf{Advice.} To avoid confusion, use whenever possible/convenient use the completely unambiguous $\frac{1}{f(x)}$ instead of $\left(f(x)\right)^{-1}$.

\importantText{
\begin{tabular}{l}
Whenever in doubt about notation, remove the ambiguity by \\
proper use of English language: write \\
\importantText{``Let $g$ be the inverse function of $f$. Then $g(x)=\dots$  ''}\\
instead of \\
\importantText{$f^{-1}(x)=\dots$}\quad .
\end{tabular}
}

For the student familiar with trigonometry, we make one more note. The inverse functions of the trigonometric functions $\sin, \cos, \tan$ have special names: the inverse function of $\sin $ is called $ \arcsin$, the inverse of $\cos $ is $\arccos$, the inverse of $\tan $ is $\arctan$. It is the personal opinion of one of the authors that you should

\importantText{
\begin{tabular}{l}
avoid the use the superscript $^{-1}$ to denote inverses \\
of trigonometric functions. Write \\
\importantText{
$\arcsin x, \arccos x, \arctan x$
}
\\
instead of
\\
\importantText{
$\sin^{-1}x, \cos^{-1}x, \tan^{-1}x$.
}
\end{tabular}
}
\chapter{Sequences and numbers}
In this chapter we define the most common sets of numbers used in mathematics - the integers, the rational numbers, the real numbers and the complex numbers.

We may recommend that a Calculus course skip this Chapter as this material should be familiar to the student from a precalculus course.

\section{Integers and sequences}\label{secSequences}
In this section we assume knowledge of the set of integers, and use that knowledge to define sequences.
\section{\optionalMaterial From integers to rational numbers }\label{secRationalNumbers}
\begin{definition}
A rational number is a number that can be expressed as the quotient of two integers. The set of all rational numbers is denoted by $\mathbb Q$.
\end{definition}
As we know from high school arithmetics, if \[\frac{p}{q}=\frac{s}{t}\quad ,\]
then $tp=qs$ and so \[tp-qs=0\quad .\]
That is why we may view the set of rational number $\mathbb Q $ as the set of pairs of numbers $(p,q)$ with condition that we identify $ (p,q)$ with $(s,t)$ if $tp-qs=0$.

\section{\optionalMaterial From rational numbers to real numbers}\label{secReals}



\section{\optionalMaterial Real number axioms}\label{secRealNumbersDef}
\section{ Complex numbers}

\label{secComplexNumbers}

\noindent \url{http://en.wikipedia.org/wiki/Complex_number}

\index{complex numbers}\index{imaginary!numbers}\index{imaginary!unit} The complex numbers are a set of numbers which contain the real numbers. The complex numbers are defined as the set of all pairs of real numbers $(a,b)$, equipped with addition, subtraction, multiplication and division operations, which we define later in the present section. The set complex numbers is denoted by $\mathbb C$. When referring to a complex number, instead of using the notation $(a,b)$ it is accepted to use the notation
\[
a+i b\quad .
\]
In this way, the set of complex numbers $\mathbb C$ becomes
\[
\mathbb C\eqdef \{a+ib| a,b\in \mathbb R\}\quad.
\]
It is commonly accepted to abbreviate $ a+i*0$ as $a$, $0+i*b$ as $i*b$, $0+0*i$ as $0$ and $0+1*i$ as $i$. Complex numbers of the form $a=a+i*0$ are in addition called real numbers and complex numbers of the form $0+i*b=i*b$ are called imaginary numbers. \index{complex numbers!imaginary part of}\index{complex numbers!real part of} \index{real!part} \index{imaginary!part}The number $a$ in $a+ib$ is referred to as the \emph{real part} of $a+ib$ and the number $b$ in $a+ib$ is referred to as the of $a+ib$. We write
\[
\Re (a+ib)\eqdef a \quad, \quad \quad \Im (a+ib)\eqdef b \quad .
\]
The names of the various parts of $z=a+ib$ are given below.
\[
z=\underbrace{\overbrace{a}^{\text{Real part}=\Re z}+ \overbrace{i\underbrace{b}_{\text{Imaginary part}=\Im z}}^{\text{imaginary number}} }_{\text{Complex number}}
\]
Complex number addition is defined in the manner suggested by the complex number notation:
\[(a+i b)+ (c+id)\eqdef (a+c) + i(b+d) \quad .
\]
In order to have that subtraction be the inverse operation of the addition operation subtraction must be defined by
\[
(a+i b)- (c+id)\eqdef (a-c) + i(b-d) \quad .
\]
Complex number addition is defined in the manner suggested by complex number notation, with the additional requirement that
\[
i^2\eqdef -1\quad .
\]
In other words, complex multiplication is defined as
\[
(a+ i b)(c+id)=ac +i^2 bd +i bc + i ad= (ac-bd)+i(bc+ad) \quad .
\]
Before we define complex division we cover the operation of complex conjugation. The \emph{complex conjugate} of a number $z$, denoted by $\bar z$, is defined via
\index{complex!conjugation}
\[\overline{a+ib}\eqdef a-ib \quad .
\]

\begin{prop}
Complex conjugation respects both addition and multiplication:
\begin{equation} \boxed{
\label{eqComplexConjugationIsAddHMM}
\overline {z +w}= \bar z +\bar w\quad .
}
\end{equation}
\begin{equation}\boxed{
\label{eqComplexConjugationIsFieldHMM}  \overline {z w}= \bar z \bar w\quad .
}
\end{equation}

\end{prop}
\begin{proof}
We leave the proof of \eqref{eqComplexConjugationIsAddHMM} to the reader. Let $z= a+ib$ and $w=c+id$. Then
\begin{equation*}
\begin{array}{rcl}\overline {zw}&=& \overline{(a+ib)(c+id)}= \overline{ac-bd +i(ad+bc) }\\&=& ac-bd-i(ad+bc)= a(c-id)-ibc+i^{2}bd\\&=& a(c-id)-ib(c-id)= (a-ib)(c-id)\\&=&\bar z \bar w\quad .
\end{array}
\end{equation*}
\end{proof}
\index{complex!absolute value}  The absolute value of a complex number $z$, denoted by $|z|$, is defined via
\[
|a+ib|\eqdef \sqrt{a^2+b^2}\quad .
\]
We have that
\[
|a+ib|^2\eqdef (a+ib)\overline{(a+ib)}=(a+ib)(a-ib)=a^2+b^2 \quad .
\]

Division of a complex number by a real number is defined in the manner suggested by complex number notation:
\[
\frac{a+ib}{c}= \frac{a}{c}+i\frac{b}{c}, \quad c\neq 0\quad .
\]
In order to have that division be the inverse operation to multiplication, it follows that the division of $1$ by a number must be defined by
\[
\frac{1}{c+id}= \frac{c-id}{(c+id)(c-id)}=\frac{c}{c^2+d^2}-i\frac{\diff}{c^2+d^2}  \quad c+id\neq 0.
\]
Therefore division of arbitrary complex numbers is defined as
\[
\frac{a+ib}{c+id}=(a+ib)\frac{1}{c+id}= \frac{(a+ib)(c-id)}{(c+id)(c-id)}= \frac{ac+bd}{c^2+d^2}+i\frac{(bc-ad)}{c^2+d^2} \quad .
\]

\subsection{Examples and Exercises}
\begin{problem}
\input{../../modules/complex-numbers/homework/complex-number-arithmetics-1}
\end{problem}
\input{../../modules/complex-numbers/homework/complex-number-arithmetics-1-solutions}



\section[\optionalMaterial A dictionary of numbers]{\optionalMaterial A dictionary of numbers: integers, rational numbers, algebraic numbers, transcendental numbers, real numbers, complex numbers}
%\end{comment}
%\begin{comment}
\chapter{A few algebra techniques}
In the present chapter we develop a few basic formulas, and state a few basic combinatorial facts. The chapter relies on a conceptual, high-school level understanding of the notion of ``formula'', but does not require any practical knowledge other than applying the rules of arithmetic.
\section{Factorials, binomial coefficients}\label{secFactorial}
Let $n$ be a positive integer, i.e., let $n \in \mathbb Z_{\geq 0}$ (non-negative integers).
\begin{definition}[Factorial] \index{factorial}
For $n>0$, we define the number $n!$ by
\[
n!\eqdef n\cdot(n-1)\cdot(n-2)\cdots 4\cdot 3\cdot 2 \cdot 1 \quad  .
\]
and we define
\[
0!\eqdef 1\quad .
\]
\end{definition}
The number $n!$ is read as ``$n$ factorial''. The first few factorial values are
\[\begin{array}{rcl}
0!&=&1\\
1!&=&1\\
2!&=&2\\
3!&=&6\\
4!&=&4\cdot 3!=24\\
5!&=&5\cdot 4!=120\\
6!&=&6\cdot 5!=720\\
7!&=&7\cdot 6!=5040  \\
&\vdots&\quad \quad .
\end{array}
\]

\index{binomial!formula} Newton binomial formula.
\index{binomial!coefficient definition} Let $k\in \mathbb Z_{>0}$. The binomial coefficient $\binom{x}{k}$ is read as ``$x$ choose $k$''.
\begin{equation}\boxed{
\label{eqBinomialCoeffDefinition}
\binom{x}{k}\eqdef \frac{ \overbrace{ x(x-1)(x-2)(x-3)\dots (x-k+1)}^{\mathrm{k~multiplicands~total}} } {k!} \quad .
}
\end{equation}
\[\binom{x}{0}\eqdef 1 \quad .
\]

Suppose $k,n\in \mathbb Z_{\geq 0}$ (non-negative integers). Then

\[\binom{n}{k} = \frac{n!}{(n-k)! k!}\quad .
\]

\section{Newton binomial formula}\label{secNewtonBinomialReview}
\url{http://en.wikipedia.org/wiki/Binomial_theorem}



\begin{equation}\boxed{\label{eqNewtonBinomialFormula}
\begin{array}{rcl}
(a+b)^{n} &=&\displaystyle \sum\limits_{i=0}^n \binom {n}{k} a^{n-k}b^{k} \\
&=&\displaystyle \binom{n}{0} a^n + \binom{n}{1}\phantom{n}a^{n-1}b +\dots+ \binom{n}{n-1} \phantom{n} ab^{n-1} + \binom{n}{n} b^n\\
&=&\displaystyle \phantom{\binom{n}{0}}a^n + \phantom{\binom{n}1} n a^{n-1}b+\dots + \phantom{\binom{n}{n-1}} na b^{n-1}+\phantom{\binom{n}{n}} b^n \quad .
\end{array}
}
\end{equation}

\chapter[Exponents \& trig - Euclidean approach]{Exponents, complex numbers and trigonometric functions: Euclidean geometry approach} \label{chapterExponentsTrigFromEuclideanGeometry}

In the present chapter we construct via Euclidean geometry the trigonometric functions. We furthermore construct the exponent function. Finally, we relate the two using Euler's formula. 

An alternative approach to the construction of the trigonometric and exponent function exists through power series. We present that alternative approach in Chapter \ref{chapterExponentsTrigFromPowerSeries}. The two approaches  are equivalent as we shall prove in the end of Chapter \ref{chapterExponentsTrigFromPowerSeries}. 

Historically, the Euclidean geometry approach to trigonometry appeared first. We note that the power series approach to the material in Chapter \ref{chapterExponentsTrigFromPowerSeries} is algebraically more elegant and more useful for computational purposes. On the other hand, the present chapter has the advantage of making the geometric interpretation of the trigonometric functions immediately obvious - in Chapter \ref{chapterExponentsTrigFromPowerSeries} that becomes apparent only after a reasonable amount of algebraic preparation.


The two alternative chapters \ref{chapterExponentsTrigFromEuclideanGeometry} and \ref{chapterExponentsTrigFromPowerSeries} are independent of one another and can be read in both orders. A shorter Calculus course may choose one of the two chapters and present the other as an optional reading.

A good understanding of trigonometry requires that the reader be familiar with the material in both chapters. 

\section{Equations of circles}\label{secCircleEquation}
A circle with center $O$ and radius $r$ is the set of points in the plane at distance $ r$ from the point $O$. 

\subsection{The unit circle}
Fix a coordinate system in the plane. The \emph{unit circle} is defined as the circle with radius $1$ and center at the origin of the coordinate system. We aim to introduce an equation that is satisfied by all points lying on the unit circle circle.

We recall that one of the formulations of the Pythagorian Theorem is that the distance $d$ between a point with coordinates $(x,y)$ and the point $(0,0)$ is given by
\[
\begin{array}{rcl}
d= \sqrt{x^2+y^2}\quad .
\end{array}
\]
Therefore a point lies on the unit circle if and only if $d=1$, and therefore all points lying on the unit circle satisfy

\[
\begin{array}{rcll|l}
\sqrt{x^2+y^2}&=&1&&\text{square both sides}\\
x^2+y^2&=&1\quad ,
\end{array}
\]
and conversely, all points that satisfy the equation above lie on the unit circle. If we transfer $x^2$ to the other side of the equality, we get that

\[
\begin{array}{rcll|l}
y^2&=&1-x^2\\
y&=&\pm \sqrt{1-x^2}\quad .
\end{array}
\]

Provided that we know how to compute values of the square root function $\sqrt{}$, we know how to compute (approximately) the value of $\sqrt{1-x^2}$. In this way we can plot a circle by plotting the two functions $\sqrt{1-x^2}$ and $-\sqrt{1-x^2}$ as shown below. 

\begin{pspicture}(-1.3, -1.3)(1.3, 1.3)
\psplot[linecolor=\fcColorGraph]{-1}{1}{1 x x mul sub sqrt}
\psplot[linecolor=orange]{-1}{1}{1 x x mul sub sqrt -1 mul}
\fcAxesStandardNoFrame{-1.2}{-1.2}{1.2}{1.2}
\end{pspicture}\\

\subsection{Equations of an arbitrary circle}
In this section we introduce an equation of an arbitrary circle. Suppose a circle has center with coordinates $(h,k) $ and radius $r$. The distance between an arbitrary point $(x,y)$ and $(h, k)$ is then 

\[
d=\sqrt{(x-h)^2+(y-k)^2}.
\]
Just as in the case of a unit circle, we set $d=r$ and compute
\[
\begin{array}{rcll|l}
\sqrt{(x-h)^2+(y-k)^2}&=&r &&\text{square both sides}\\
(x-h)^2+(y-k)^2&=&r^2\quad .
\end{array}
\]
The above is called the \emph{(implicit) equation of a circle}.
\subsection{Intersections of a circle and a line}
In this section we consider the problem of finding the intersection of a circle and a line. This problem plays a role in understanding the definition of an angle in Section \ref{secAngleDef}.

Since we have already studied the equations of lines and equations of a circle, the intersection is purely a matter of algebra: if the circle is given by equation $(x-h)^2+(y-k)^2=r^2$ and the line  - by equation $ax+by=c$, then all we need to do is solve the system of equations
\[
\left| \begin{array}{rcl}
(x-h)^2+(y-k)^2&=&r^2\\
ax+by&=&c
\end{array}\right.\quad .
\]
This a system of polynomial equations. There are three distinct cases that happen when solving the system: we either get $2$ solutions (the line intersects the circle in two points), $1$ solution (the line is tangent to the circle) or $0$ solutions (the line does not meet the circle). 


The rest of this section is dedicated to showing the algebra in a few examples. 
\begin{problem}
Find the intersection of the circle and the line.
\begin{enumerate}[ref={\fcProblemRef}]
\item The circle with center $(0,0)$ and radius $1$ and the line with equation $y=2x+\frac{1}{2} $.
\item The circle with center $(1,2)$ and radius $2$ and the line $ x=3$.
\item The circle with center $(-1,-1)$ and radius $1$ the line  $x+y=0$.
\end{enumerate}
\end{problem}


\section{Definition of Angle}\label{secAngleDef}
The term angle is used to denote three distinct mathematical objects: first, the angle formed by two rays, second, the angle-measure of such a figure, and third, the angle-measure of a rotation. By convention, all three objects are referred to as ``angle''; one is expected to use context to decide whether ``angle'' means ``angle formed by two rays'', ``angle measure of an angle formed by two rays'' or ``angle of rotation''. Except for the present section, we shall fully take advantage of this convention and use the term ``angle'' without further clarification.

\subsection{Angle given by two rays}
In planar  geometry, a \emph{geometric angle} (\emph{angle} for short) is the figure formed by two rays, called \emph{arms} or \emph{sides} of the angle, sharing a common endpoint, called the \emph{vertex} of the angle. The rays are ordered: the ray that comes first is called the \emph{initial arm (side)} of the angle, and the other ray - the \emph{terminal arm} (side) of the angle. By convention, the rays are allowed to coincide; the resulting angle is then called the \emph{zero angle}.

In Figure \ref{figAngleDef}, the vertex of the angle is denoted by $O$. Since a ray can be identified by its starting point and any other point on the ray, an angle can be identified by specifying its vertex and one point on each of its arms. In Figure \ref{figAngleDef}, we have chosen the point $A$ on the first ray and the point $B$ on the second. We denote the angle by $\angle AOB $. The choice of points $A$ and $B$ is not unique - for example if we choose another point, say $A'$, on the ray $OA$, then $\angle AOB$ and $\angle A'OB$ coincide. 

By definition, we distinguish between two angles that have the same pair of rays but in the opposite order. For example $\angle AOB $ and angle $\angle BOA $ are by definition not equal. To indicate on a picture which ray comes first, we draw a small arc with an arrow from the first ray to the second as shown in Figure \ref{figAngleDef}.

\begin{figureFixed}
\begin{center}
\begin{pspicture}(-1,-1)(6, 4)
\tiny
\psline[arrows=->](0,0)(4,0)
\rput[lt](3,-0.2){initial arm}
\psline[arrows=->](0,0)(! 2 3 sqrt 2 mul )
\fcAngleBetweenVectors[arrows=->, linecolor=blue]{[2 0]}{[1 3 sqrt]}{0.3}{}
\rput[b]{60}(! 1.5 3 sqrt 1.5 mul 0.2 add){terminal arm}
\fcFullDot[linecolor=blue]{0}{0}
\rput[t] (-0.2, -0.2){$O$}
\fcFullDot{0.5}{0}
\rput[t] (0.5, -0.2){$A$}
\fcFullDot{1.5}{0}
\rput[t] (1.5, -0.2){$A'$}
\fcFullDot{1 0.8 mul }{3 sqrt 0.8 mul}
\rput[rt] (0.6, 1.5){$B$}
\end{pspicture}
\caption{ Angle definition. \\ $\angle AOB$ and $\angle A'OB$\\ represent the same angle.} \label{figAngleDef}
\end{center}
\end{figureFixed}


\begin{figureFixed}
\begin{center}
\begin{pspicture}(-1,-1)(6, 4)
\tiny
\psline[arrows=->](0,0)(2,0)
\psline[arrows=->](0,0)(! 1 3 sqrt )
\fcAngleBetweenVectors[arrows=->, linecolor=blue]{[1 0]}{[0.5 3 sqrt 2 div]}{0.3}{}
\fcFullDot[linecolor=blue]{0}{0}
\rput[t] (-0.2, -0.2){$O$}
\fcFullDot{1.5}{0}
\rput[t] (1.5, -0.2){$A$}
\fcFullDot{1 0.8 mul }{3 sqrt 0.8 mul}
\rput[rt] (0.6, 1.5){$B$}

\rput(5,1){
\psline[arrows=->](0,0)(0, 2)
\psline[arrows=->](0,0)(! 3 sqrt -1 mul 1)
\fcAngleBetweenVectors[arrows=->, linecolor=blue]{[0 2]}{[ 3 sqrt -1 mul 1]}{0.3}{}
\fcFullDot[linecolor=blue]{0}{0}
\rput[t] (-0.2, -0.2){$O_1$}
\fcFullDot{0}{1.5}
\rput[l] (0.2, 1.5){$A_1$}
\fcFullDot{3 sqrt -0.8 mul}{1 0.8 mul }
\rput[t] (-1.5, 0.6){$B_1$}
}
\end{pspicture}
\caption{Equal angles. \\ $\angle AOB=\angle A_1OB_1$}\label{figEqualAngles}
\end{center}
\end{figureFixed}

Two geometric angles are considered \emph{congruent(equivalent)} when the two figures can be transformed to one another with a sequence of rotations and translations in the plane. In other words, two geometric angles are considered congruent when one can be mapped onto the other by a rigid motion in the plane - the matching should respect the order of the rays of the angle (first ray goes to first, second to second). In Figure  \ref{figEqualAngles}, if we first translate vertex $O_1$ onto vertex $O$, and then rotate about vertex $O$, we can match the two rays of the angles in the correct order. 

We note that we have not given precise definitions of rotation and translation -  that lies outside of the scope of the present section. At present, we expect the reader to rely on one's intuition for the meaning of ``translation'' and ``rotation''.

Two geometric angles are congruent if and only if their angle measures are equal. We recall that the word ``angle'' refers to both ``geometric angle'' and ``angle measure''. Therefore when one says ``the two angles are equal'' it is to be interpreted as ``the geometric angles are congruent'' or equivalently as ``the angle measures of the two geometric angles are equal''. 



\subsection{Angle measure}~


\begin{figureFixed}
\begin{center}
\begin{pspicture}(-1,-1)(6, 4)
\tiny
\psline[arrows=->](0,0)(2,0)
\psline[arrows=->](0,0)(! 1 3 sqrt )
\fcAngleBetweenVectors[arrows=->, linecolor=blue]{[2 0]}{[1 3 sqrt]}{0.3}{}%
\fcAngleBetweenVectors[linecolor=red]{[2 0]}{[1 3 sqrt]}{1}{}%
\parametricplot[linestyle=dashed, linecolor=red!60]{60}{360}{t cos t sin}
\fcFullDot[linecolor=blue]{0}{0}%
\rput[t] (-0.2, -0.2){$O$}%
\fcFullDot{1}{0}%
\rput[t] (1, -0.2){$A$}%
\fcFullDot{0.5 }{3 sqrt 0.5 mul}%
\rput[rt] (0.6, 1.2){$B$}%
\end{pspicture}
\caption{ \label{figAngleMeasureDef} Angle measure definition.\\ $\angle AOB$ is measured by the \\
length of the arc \\ that it cuts off the unit circle.} 
\end{center}
\end{figureFixed}

To the angle given in Figure \ref{figAngleMeasureDef}, we can assign a measure in the following fashion. Draw a circle of radius $1$ (unit circle) with center at $O$. Let the first ray intersect the unit circle in point $A$ and the second ray - at point $B$. Define the \emph{measure of the angle} $\angle AOB$, or \emph{angle} for short, as the length of the shorter of the two arcs of the unit circle that start at $A$ and end at $B$, taken with positive sign if the ray $OA $ lies clockwise from the ray $OB$, and taken with negative sign else. 

We note that we have not yet given a mathematical definition of the word ``clockwise''. Much later, we shall use the mathematical term ``orientation'' to give a precise mathematical meaning to the word ``clockwise''. Until then, we expect the reader to rely on one's intuition. 

We also note that we have not given a definition of the lengths of an arc of a circle. This omission will again be fixed much later (Section \ref{secCurveLength}), when we have developed the full power of integral Calculus. Until then, we ask the reader to think of the length of an arc of a circle informally. One way to do so is to think of a rope curved around an arc of a circle; measuring the arc length would then correspond to straightening the rope and measuring its length. We advise the reader not satisfied with this interpretation to take a quick look at Figure \ref{figCurveLengthDef}.
\subsection{Angles of rotation}


\subsubsection{Degrees and radians}
We now proceed to assign a measurement unit to angles. Since a geometric angle is measured by a shorter of two arcs that make up a circle, a geometric angle cannot be measured than more half a circle. As the reader probably knows (and we shall prove later), the circumference of half a circle with radius 1 is $\pi$, where
\[
\pi= 3.14159265\dots. 
\]


In this way, we have assigned to each geometric angle a number in the interval $\left(-\pi, \pi \right]$. This number is called the measure of the angle in \emph{radians}. 


In practice, it is often desired to work with exact fractions of the length of a circle. The most frequently encountered fractions of angles have angle measures $\frac{\pi}{2}$, $\frac{\pi }{3}$, $\frac{\pi }{4}$, $\frac{\pi }{5}$, $\frac{\pi }{6}$, $\frac{\pi }{9}$. To avoid computation with fractions it makes sense to introduce a new measurement unit of angle - the degrees unit. We say that $\pi$ radians are equal to $180^\circ$ degrees. Here the number $180$ is chosen for historical reasons, but this historical choice is well justified: under it, the frequently encountered angles listed above have integer measures. In Table \ref{tableFrequentlyEncounteredAcuteAngles} below we indicate the degree measures of frequently encountered angles. 

\begin{tableFixed}
\begin{tabular}{ccccccccccc}
Radians & $\frac{5\pi}{6}$& $\frac{3\pi}{4}$ & $\frac{2\pi}{3}$&  $\frac{\pi}{2}$& $\frac{\pi }{3}$ & $\frac{\pi }{4}$ &$\frac{\pi }{5}$& $\frac{\pi }{6}$&  $\frac{\pi }{9}$ \\
Degrees& $150^\circ$ &$135^\circ$ &$120^\circ$& $90^\circ $& $60^\circ $& $45^\circ $& $36^\circ $& $30^\circ $& $20^\circ $& 
\end{tabular}
\caption{Some frequently encountered angles in radians and degrees.}\label{tableFrequentlyEncounteredAcuteAngles}
\end{tableFixed}








 
\subsection{Exponent function: integer vs non-integer exponent}
We recall that when $n$ is an integer,  $a^{n}$ denotes the expression $\underbrace{a\cdot\dots \cdot a}_{n\text{ times}}$.

\chapter{Limits, continuity, derivatives}
\section{The tangent problem}
Consider the computer- generated plot of the function $f(x)=\frac{x^2}2$ below. The line drawn in blue in Figure \ref{figTangentIdeaFor} appears to just touch the graph of the function $y=\frac{x^2}2$ at the point $P=(2,f(2))=(2, \frac{2^2}{2}) =(2,2)$. In mathematical language we say that the blue line is tangent to the graph of $ y=\frac{x^2}2$ (``tangent'' comes from Latin, ``to touch''). We shall give a formal definition of a tangent line later in Section \ref{secDefTangent}; until then, we shall refer to a ``tangent line'' informally, relying on the reader's intuition.

We are seeking an equation for the tangent through $P=(2,2)=(2, f(2))$ drawn in \ref{figTangentIdeaFor}. We know one point on the tangent line - namely the point $P=(2,2)$. Recall that every non-vertical line has equation
\[
y=mx+c
\]
for some numbers $m$ and $c$, where $m$ is called the slope
of the line and $c$ is called the $y$-intercept of the line. As the tangent line passes through the point $P=(2,2)$, it has equation $y-2=m(x-2)$ for some slope $m$ that we yet need to define.

\begin{figureFixed}
\begin{center}
\optionalDisplay{
%\psset{xunit=1cm, yunit=1cm}
%\begin{pspicture}(-1, -5)(3,5)
%\psframe*[linecolor=white](-1,-5)(3,5)
%\tiny
%\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3,4.5)
%Function formula: 1/2 ((x)^{2})
%\rput(1,3){$y= \frac{x^2}2$}
%\psplot[linecolor=\fcColorGraph, plotpoints=1000]{-1}{3}{x 2 exp 0.5 mul }
%\fcFullDot{2}{2}
%\rput[tl](2.1, 1.9){$P=(2,2)$}
%\fcXTick{2}
%\fcFullDot{0}{0}
%\rput[tl](0.1, -0.1){$Q=\left(0,0\right)$}
%\psplot[linecolor=\fcColorTangent, plotpoints=1000]{-0.5}{3}{x}
%\end{pspicture}
\noindent \psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1, -5)(3,5)
\psframe*[linecolor=white](-1,-5)(3,5)
\tiny
\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3.1,4.805)
%Function formula: 1/2 ((x)^{2})
\rput(1,3){$y= \frac{x^2}2$}
\psplot[linecolor=\fcColorGraph, plotpoints=1000]{-1}{3.1}{x 2 exp 0.5 mul }
\fcFullDot{2}{2}
\rput[br](1.9, 2.1){$P$}
\rput[tl](2.1, 1.9){$(2,2)$}
\fcXTick{2}
\fcFullDot{1}{0.5}
\rput[br](0.9, 0.6){$Q$}
\rput[tl](1.1, 0.4){$\left(1,0.5\right)$}
\fcXTick{1}
\psline[linecolor=\fcColorTangent](0.3333333,-0.5)(3,3.5)
\end{pspicture}
\psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1, -5)(3,5)
\psframe*[linecolor=white](-1,-5)(3,5)
\tiny
\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3.1,4.805)
%Function formula: 1/2 ((x)^{2})
\rput(1,3){$y= \frac{x^2}2$}
\psplot[linecolor=\fcColorGraph, plotpoints=1000]{-1}{3.1}{x 2 exp 0.5 mul }
\fcFullDot{2}{2}
\rput[br](1.9, 2.1){$P$}
\rput[tl](2.1, 1.9){$(2,2)$}
\fcXTick{2}
\fcFullDot{1.5}{1.125}
\rput[br](1.4, 1.225){$Q$}
\rput[tl](1.5, 1.025){$\left(1.5, 1.125\right)$}
\fcXTick{1.5}
%Function formula: 7/4 x-3/2
\psplot[linecolor=\fcColorTangent, plotpoints=1000]{0.571428571}{3.1}{-1.5 x 1.75 mul add }
\end{pspicture}
\psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1, -5)(3,5)
\psframe*[linecolor=white](-1,-5)(3,5)
\tiny
\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3.1,4.805)
%Function formula: 1/2 ((x)^{2})
\rput(1,3){$y= \frac{x^2}2$}
\psplot[linecolor=\fcColorGraph, plotpoints=1000]{-1}{3.1}{x 2 exp 0.5 mul }
\fcFullDot{2}{2}
\rput[br](1.9, 2.1){$P$}
\rput[tl](2.1, 1.9){$(2,2)$}
\fcXTick{2}
\fcFullDot{1.9}{1.805}
\rput[br](1.8, 1.705){$Q$}
\rput[tl](1.7, 1.505){$\left(1.5, 1.125\right)$}
\fcXTick{1.9}
%Function formula: 39/20 x-19/10
\psplot[linecolor=\fcColorTangent, plotpoints=1000]{0.717948718}{3.1}{-1.9 x 1.95 mul add }
\end{pspicture}

\noindent \psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1, -5)(3,5)
\psframe*[linecolor=white](-1,-5)(3,5)
\tiny
\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3.1,4.805)
%Function formula: 1/2 ((x)^{2})
\rput(1,3){$y= \frac{x^2}2$}
\psplot[linecolor=\fcColorGraph, plotpoints=1000]{-1}{3.1}{x 2 exp 0.5 mul }
\fcFullDot{2}{2}
\rput[br](1.9, 2.1){$P$}
\rput[tl](2.1, 1.9){$(2,2)$}
\fcXTick{2}
\psline[linecolor=\fcColorTangent](0.75,-0.5)(3,4)
\end{pspicture}

\noindent \psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1, -5)(3,5)
\psframe*[linecolor=white](-1,-5)(3,5)
\tiny
\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3.1,4.805)
\fcFullDot{2}{2}
\rput[r](1.9, 2){$(2,2)$}
\rput[tl](2.1, 1.9){$P$}
\fcXTick{2}
\fcFullDot{2.1}{2.205}
\rput[br](2, 2.305){$(2.1,2.205)$}
\rput[tl](2.2, 2.205){$Q$}
\fcXTick{2.1}
\rput(1,4){$y= \frac{x^2}2$}
%Function formula: 41/20 x-21/10
\psplot[linecolor=\fcColorTangent, plotpoints=1000]{0.780488}{3.1}{-2.1 x 2.05 mul add }
%Function formula: 1/2 x^{2}
\psplot[linecolor=\fcColorGraph, plotpoints=1000]{-1}{3.1}{x 2 exp 0.5 mul }
\end{pspicture}
\psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1, -5)(3,5)
\psframe*[linecolor=white](-1,-5)(3,5)
\tiny
\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3.1,4.805)
\fcFullDot{2}{2}
\fcFullDot{2}{2}
\rput[r](1.9, 2){$(2,2)$}
\rput[tl](2.1, 1.9){$P$}
\fcXTick{2}
\fcFullDot{2.5}{3.125}
\rput[br](2.4, 3.225){$(2.5, 3.125)$}
\rput[tl](2.6, 3.025){$Q$}
\fcXTick{2.5}
\rput(1,4){$y= \frac{x^2}2$}
%Function formula: 9/4 x-5/2
\psplot[linecolor=\fcColorTangent, plotpoints=1000]{0.888889}{3.1}{-2.5 x 2.25 mul add }
%Function formula: 1/2 x^{2}
\psplot[linecolor=\fcColorGraph, plotpoints=1000]{-1}{3.1}{x 2 exp 0.5 mul }
\end{pspicture}
\psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1, -5)(3,5)
\psframe*[linecolor=white](-1,-5)(3,5)
\tiny
\psaxes[ticks=none, labels=none]{<->}(0,0)(-1,-0.5)(3.1,4.805)
\fcFullDot{2}{2}
\rput[r](1.9, 2){$(2,2)$}
\rput[tl](2.1, 1.9){$P$}
\fcXTick{2}
\fcFullDot{3}{4.5}
\rput[br](2.9, 4.5){$(3, 4.5)$}
\rput[tl](2.95, 4.3){$Q$}
\fcXTick{3}
\rput(1,4){$y= \frac{x^2}2$}
%Function formula: 5/2 x-3
\psplot[linecolor=\fcColorTangent, plotpoints=1000]{1}{3.1}{-3 x 2.5 mul add }
%Function formula: 1/2 x^{2}
\psplot[linecolor=\fcColorGraph, plotpoints=1000]{-1}{3.1}{x 2 exp 0.5 mul }
\end{pspicture}
}
\end{center}
\caption{A tangent line touches the graph of a function, and is approximated by secant lines \label{figTangentIdeaFor}}
\end{figureFixed}
It is natural to approximate the tangent line using secant lines passing through the point $ P=(2, 2)$ and nearby points $Q=(t,f(t))=(t, \frac{t^2}{2})$ lying on the graph of $f(x)$. The line passing through $P=(2,2) $ and $Q=(t,f(t))$ has slope $m_Q:=\frac{f(t)-2}{t-2}$ and therefore has equation
\[
y-2=m_Q(x-2), \quad\quad \quad\text{where~} m_Q= \left(\frac{f(t)-2}{t-2}\right).
\]
As the equation of the tangent line is $y-2=m(x-2)$, we choose to approximate $m$ by the numbers $m_Q$ as $Q$ gets close to the point $P$. On the other hand, the point $Q=(t,f(t))$ gets closer to $P= (x, f(x))$ as $t$ gets closer to $x$.
In Table \ref{tableTangentIdeaForTable}, we have computed the values of $f(t)$ and $m_Q=\frac{f(t)-2}{t-2}$ for various values of $t$ close to $x=2$. We see that as $t$ gets closer to $2$, $m_Q$ appear to get closer to the number $2$, and indeed, we can say that $m_Q$ ``gets infinitely close to $2$ as $t$ approaches $2$'' and so we can define $m=2$. Then the equation of the equation of the tangent line at $P$ becomes
\[
y-2=2(x-2)\quad ,
\]
which is exactly the equation of the line plotted in blue in Figure \ref{figTangentIdeaFor}.
\begin{tableFixed}
\begin{center}
\begin{tabular}{c|c|c|c|c}
$x$& $f(x)=\frac{t^2}2$& $t-2$ & $f(t)-f(2)$ & $m_Q=\frac{f(t)-f(2)}{t-2}$ \\\hline
0& 0& -2& -2& 1 \\
1& 0.5& -1& -1.5& 1.5 \\
1.5& 1.125& -0.5& -0.875& 1.75\\
1.9& 1.805& -0.1& -0.195& 1.95\\
1.99& 1.98005& -0.01& -0.01995& 1.995\\
1.999& 1.998& -0.001& -0.0019995& 1.9995\\\hline
2.001& 2.002& 0.001& 0.0020005& 2.0005\\
2.01& 2.02005& 0.01& 0.02005& 2.005\\
2.1& 2.205& 0.1& 0.205& 2.05\\
2.5& 3.125& 0.5& 1.125& 2.25\\
3& 4.5& 1& 2.5& 2.5\\
4& 8& 2& 6& 3 \\
\end{tabular}
\end{center}
\caption{ Values of $f(x)$ and slope of line through $P,Q$. \label{tableTangentIdeaForTable}}
\end{tableFixed}
In order to give a strict definition of tangent, we need to define formally what it means for $m_Q$ to ``get infinitely close'' to the number $2$. The colloquial phrase ``to get infinitely close to,'' corresponds to the mathematical notion of taking limits.
\section{Limits}
\subsection{Definition of limit}\label{secDefLimit}
Before we proceed to the formal definition of limit (Definition \ref{defLimit} below) let us explain limits informally.  We say that $f(x)$ tends to $L$ as $x$ tends to $a$ and we write
\[
\lim_{x\to a} f(x)=L
\]
if, no matter how small positive number $\varepsilon$ we pick, we can always choose a small neighborhood of $a$ so that the distance between $f(x)$ and $L$ is less than $\varepsilon$. Given an $\varepsilon$ and $\delta$, let us draw a rectangle centered at $(a,L)$ with base $ 2\delta$ and height $2\varepsilon$  (Figure \ref{figLimitDef} below). Then $\lim_{x\to a}f(x)=L$ means that, no matter how small the height of the rectangle is, one can select its width so small that when the argument $x$ takes the values given by the rectangle base, the graph of $f(x)$ lies entirely in that rectangle (except possibly the point $(a, f(a))$).

%We note that for a smaller $\varepsilon$, one would expect in general a smaller neighborhood around $a$\footnote{ but not necessarily: think of constant functions.}.

\begin{figureFixed}
\optionalDisplay{
\begin{pspicture}(-0.5,-0.5)(3,2.5)
\tiny
\psaxes[labels=none, ticks=none]{<->}(0,0)(-0.6,-0.5)(3,2.5)
\fcLabels{3}{2.5}
\rput[l](2.8,1.7){\tiny $y=f(x)$}

\psplot[linecolor=red]{-0.5}{3}{x x mul 4 div}
\pscircle*(2,1){0.05}
\pscircle*(2,0){0.05}
\pscircle*(0,1){0.05}
\rput[b](2,0.1){\tiny $a$}
\rput[l](0.1,1){\tiny $L$}
\psline[linestyle=dotted](-0.5,0.8)(3, 0.8)
\psline[linestyle=dotted](-0.5,1.2)(3, 1.2)
\psline[linecolor=blue]{<->}(-0.5,0.8)(-0.5,1.2)
\rput[r](-0.6, 1){ $2\varepsilon$}

\psline[linestyle=dotted](1.84,-0.5)(1.84, 2.5)
\psline[linestyle=dotted](2.16,-0.5)(2.16, 2.5)
\psline[linecolor=blue]{<->}(1.84,2.5)(2.16,2.5)
\rput[t](2, 2.4){ $2\delta$}
\end{pspicture}
}
\caption{\label{figLimitDef} $\lim\limits_{x\to a} f(x)=L$ can be interpreted as being able to select a rectangle centered at $(a, L)$ with sides so that for $x$ near $a$, the entire graph of $y=f(x)$ lies inside that rectangle, except possibly at $a$. }
\end{figureFixed}

Formally, the definition of limit is as follows.
\begin{definition}[the $\varepsilon, \delta$-definition of limit] \label{defLimit}
We say that the limit of $f(x)$ as $x$ tends to $a$ exists and equals $L$ and we write
\[
\lim_{x\to a} f(x)=L
\]
if for every $\varepsilon>0$ there exists $\delta>0$ such that for all $x$ with $0<|x-a|<\delta$ it follows that  $|f(x)-L|<\varepsilon$.
\end{definition}
We note that the expression ``the limit of $f(x)$ as $x$ tends to $a$ exists'' is to be regarded as a technical mathematical definition. This definition requires the existence of a quantity $\delta$ as a function of another quantity $\varepsilon $. We stress the fact that the word ``exists'' in the phrase ``the limits exists'' is used strictly as a technical mathematical term. It is not directly related to the use of the word ``exists'' in philosophical or other texts, and should not be interpreted in any way other than as a reference to Definition \ref{defLimit}.
\subsection{A note on the limit notation.}
We note that a proper English language negation of Definition \ref{defLimit} can be given as follows.
\begin{prop}\label{propLimitDoesntExist}
$\lim\limits_{x\to a}f(x) $ does not exist as $x$ tends to $a$ if, for any number $L$, there exists a number $\varepsilon>0$ such that for all $\delta>0$ (no matter how small), there always exists an $x$ with $0<|x-a|<\delta$ so that $|f(x)-L|>\varepsilon$.
\end{prop}
To be completely formal, we have to use the above sentence whenever the limit of $f(x)$ as $x$ tends to $a$ does not exist.


We make an important note about calculus notation: we defined the notation
\[
\lim\limits_{x\to a} f(x)=L
\]
on condition that for every $\varepsilon>0$ there exists a $\delta>0$ with certain properties,  the ``$\varepsilon, \delta$''-definition. In particular, we defined the notation $\lim\limits_{x\to a}f(x)$ only if the limit exists and is equal to some particular number $L$. However, like many other calculus textbooks, we adopt the convention that we can write
\[
\lim\limits_{x\to a}f(x)
\]
even if there exists no number $L$ that satisfies the requirements of Definition \ref{defLimit}. If that is the case, instead of repeating the entire sentence of Proposition \ref{propLimitDoesntExist}, we simply say that ``$\lim\limits_{x\to a}f(x)$ does not exist''. We ask the reader to accept this slight informality in view of its great convenience and universal acceptance. The phrase ``does not exist'' is to be interpreted strictly as a reference to Proposition \ref{propLimitDoesntExist}; the use of the phrase is not directly related to the use of the phrase in philosophical or other texts.



\subsection{Definition of neighborhood}
When discussing limits, it is useful to speak of neighborhoods of points. Informally, a neighborhood of a point $a$ is an arbitrary collection of points that  includes all points sufficiently close to $a$ except possibly $a$. Formally, the definition of a neighborhood is given as follows.

We say that a $J$ is a \emph{punctured interval} around $a$ if $J$ is of the form
\[
J= (a-\varepsilon,a)\cup (a, a+\varepsilon)=\{x| \varepsilon>|x-a|>0\}.
\]
for some positive number $\varepsilon>0$.
\begin{definition}
Let $a$ be a real number. We say that the set $I$ is a \emph{neighborhood of $a$} if it contains a punctured interval around $a$.
\end{definition}
In other words,  $I$  is a neighborhood of $a$  if $J\subset I$ for some set $J$ of the form $ J= (a-\varepsilon,a)\cup (a, a+\varepsilon)$ for some positive $\varepsilon>0$.

In the language of neighborhoods, we can reformulate the definition of limit as follows.

\begin{definition}[reformulation of Definition \ref{defLimit}]
\[\lim\limits_{x\to a} f(x)=L
\]
if for every $\varepsilon>0$ there exists a neighborhood of $a$ such that for all $x$ in that neighborhood the distance between $f(x)$ and $L$ is less than $\varepsilon$.
\end{definition}
\subsection{Examples of limits using the definition}
\begin{example}
Compute the limit
\[\lim\limits_{x\to 1}\frac{x-1}{x^2-1}\]
\end{example}
\begin{solution}
\optionalDisplay{
\psset{xunit=1cm,yunit=1cm}
\begin{pspicture}(-0.5,-0.5)(2,2.5)
\psframe*[linecolor=white](-0.5, -0.5)(2, 2.5)
\tiny
\psaxes[Dy=0.5, labels=none]{<->}(0,0)(-0.5,-0.5)(2,2.5)
\fcYTickWithLabel{0.5}{$0.5$}
\fcXTickWithLabel{1}{$1$}
\psplot[linecolor=red]{-0.5}{2}{1 1 x add div}
\fcHollowDot{1}{0.5}
\end{pspicture}
}%optionalDisplay
\end{solution}
\begin{example}
Compute the limit
\[\lim\limits_{x\to 0}\frac{\sin x}{x}\]
\end{example}
\begin{solution}
\optionalDisplay{
\psset{xunit=1cm,yunit=1cm}
\begin{pspicture}(-2,-0.5)(2.2,1.5)
\psframe*[linecolor=white](-2, -0.5)(2.2, 1.7)
\tiny
\fcAxesStandard{-2}{-0.5}{2}{1.5}
\rput[br](-0.1,1.1){ $1$}
\psplot[linecolor=red]{-2}{2}{ x 57.295779513 mul sin x div}
\fcHollowDot{0}{1}
\end{pspicture}
}%optional display
\end{solution}
\begin{example}
Show the limit
\[
\lim\limits_{x\to 0}\sin \frac{\pi}{x}
\]
does not exist.
\end{example}
\begin{solution}
\optionalDisplay{
\begin{pspicture}(-3,-1.5)(3,1.5)
\psframe*[linecolor=white](-3, -1.5)(3.2, 1.7)
\fcAxesStandard{-3}{-1.5}{3}{1.5}
\tiny
\rput[t](1,-0.1){ $1$}
\psplot[linewidth=0.3pt, linecolor=red, plotpoints=10000]{-3}{-0.01}{3.14159 x div 57.295779513 mul sin}
\psplot[linewidth=0.1pt, linecolor=red, plotpoints=10000]{0.01}{3}{3.14159 x div 57.295779513 mul sin}
\end{pspicture}
}%optionalDisplay
\end{solution}
\section{Continuity}
%In this section we define continuous functions.
A function $f$ from a subset of the real numbers to the real numbers is continuous if, roughly speaking, its graph is a single unbroken curve with no "holes" or "jumps". %The formal definition follows.

%Let  be a function from a subset of the real numbers to the real numbers.
\begin{definition}
We say that $f$ is continuous at $a$ if
\begin{enumerate}
\item $f$ is defined at $a$.
\item $\lim\limits_{x\to a} f(x)=f(a)$.
\end{enumerate}
\end{definition}

\section{Computing limits using algebra}
In the present section, we learn algebraic techniques to evaluate limits quickly. We will start by computing a couple of basic ``building block'' limits by directly using the Definition \ref{defLimit}. Then we will proceed by showing how these building block limits can be used to compute more complex ones using a number of ``limit laws''. We will summarize the acquired knowledge by giving a procedure for computing limits.
\subsection{Limit laws}
Let us compute the limit
\[
\lim_{x\to 2} x\quad .
\]
We start by interpreting the limit geometrically. The function $f(x)=x$ is a straight line passing through the origin and $f(2)=2$. No matter how small $\varepsilon$ is chosen, if one chooses $\delta=\varepsilon$, then the graph of $f(x)=x$ locally ``fits'' (see Figure \ref{figContinuityID}) in a square with height $2\varepsilon$, and width $2\delta$ centered at $(2,2)$, and so $\lim\limits_{x\to 2} x=2$.

\begin{figureFixed}
\optionalDisplay{
\psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-1.000000, -5)(3.500000,5)
\psframe*[linecolor=white](-1.000000,-5)(3.500000,5)
\tiny
\fcAxesStandard{-0.500000}{-0.5}{3.000000}{3} %Function formula: x
\rput(1,1.5){$y=x$}
\psline[linecolor=\fcColorGraph](-0.5, -0.5)(3,3)
\fcFullDot{2}{2}
\fcXTickWithLabel{2}{$2$}
\fcYTickWithLabel{2}{$2$}

\psline[linestyle=dotted](-0.5,2.2)(3, 2.2)
\psline[linestyle=dotted](-0.5,1.8)(3, 1.8)
\psline[linecolor=blue]{<->}(1.5,1.8)(1.5,2.2)
\rput[r](1.4, 2){ $2\varepsilon$}

\psline[linestyle=dotted](1.8,-0.5)(1.8, 3)
\psline[linestyle=dotted](2.2,-0.5)(2.2, 3)
\psline[linecolor=blue]{<->}(1.8,2.5)(2.2,2.5)
\rput[b](2, 2.6){ $2\delta$}

\end{pspicture}
}
\caption{\label{figContinuityID} The continuity of the identity function $f(x)=x$.}
\end{figureFixed}

Let us now show that $\lim\limits_{x\to 2}x=2 $ algebraically, using the definition of limit (Definition \ref{defLimit}). Indeed, this is straightforward: for every $\varepsilon>0$, we can select $\delta=\varepsilon$ so that whenever $|x-2|<\delta$, we have $|f(x)-2|=|x-2|<\varepsilon$ (this is obvious as $\delta=\varepsilon$).

In the previous example, there was nothing special about the choice $x=2 $: the above considerations would have worked equally well  $x=a$ for any other number $a$, and so we have shown that $\lim\limits_{x\to a}x=a $ for all $a$.

We leave it to the reader to reason that for an arbitrary constant $c $, we have that $\lim \limits_{x\to c}=c$. Geometrically this is clear from the fact that the graph of $f(x)=c$ is a straight line; we leave the very easy algebraic consideration to the reader. We summarize our observations in the following theorem.
\begin{theorem}The following basic ``building block'' limit laws hold.
\begin{enumerate}
\item $\lim\limits_{x\to a} x=a$.
\item $\lim\limits_{x\to a} c= c$, where $c$ is an arbitrary constant.
\end{enumerate}
\end{theorem}
In the next theorem, we show how to compose the above basic ``building block'' limits into more complicated limits.
\begin{theorem} Let $f$ and $g$ be functions defined in punctured neighborhoods of $a$ and suppose both limits $\lim\limits_{x\to a} f(x) $ and $\lim\limits_{x\to a} g(x) $ exist. Then the limits on the right hand side exist and the equalities hold.
\[
\begin{array}{rcll}
\displaystyle \lim\limits_{x\to a} c f(x) \pm \lim\limits_{x\to a}d g(x)&=& \displaystyle \lim\limits_{x\to a}c f(x)\pm d g(x) &  c,d \text{ are arbitrary constants}\\~\\
\displaystyle \lim\limits_{x\to a} f(x)\lim\limits_{x\to a}  g(x) &=&\displaystyle  \lim \limits_{x\to a} f(x)g(x)\\~\\
\displaystyle \frac{\lim\limits_{x\to  } f(x)} {\lim\limits_{x\to a}g(x)} &=&\displaystyle  \lim \limits_{x\to a} \frac{f(x)}{g(x)} &\text{ provided }\displaystyle \lim\limits_{x\to a} g(x)\neq 0 \quad .
\end{array}
\]
\end{theorem}
\begin{proofOptional}~We will only prove the limit law $\displaystyle \lim\limits_{x\to a} f(x)\lim\limits_{x\to a}  g(x)= \lim\limits_{x\to a} f(x) g(x)$; the remaining limit laws are left as an exercise to the reader.

Let $\lim\limits_{x\to a}f(x)=L$ and $\lim\limits_{x\to a}g(x)=M$; we aim to show that $\lim\limits_{x\to a} f(x)g(x) $ exists and is equal to $M L$. In order to do that, we must estimate the difference $ |L M - f(x)g(x)|=|L M - (f(x)-L+L)g(x)|$. We can compute that

\begin{equation}\label{eqlimftimesg}
\begin{array}{rcl}
|L M - f(x)g(x)|&=&|L M - (f(x)-L+L)(g(x))|\\&=& |LM-L g(x) - (f(x)-L)g(x)  |\\& \stackrel{\Delta-\text{ineq}}{\leq}& |L||M-g(x)|+|(f(x)-L)g(x)| \\
&=& |L||M-g(x)|+|(f(x)-L)(g(x)-M+M)|\\
&\stackrel{\Delta-\text{ineq}}{\leq}& |L||M-g(x)|+ |f(x)-L||g(x)-M|
\\&&+ |f(x)-L||M|
\end{array}
\end{equation}

Informally, we need to show that for any $\varepsilon>0$ we have that $|LM-f(x)g(x)|<\varepsilon$ for all $x$ sufficiently close to $a$. Consider the inequality \eqref{eqlimftimesg}. As $\lim\limits_{x\to a} g(x)=M$ and $\lim\limits_{x\to a}f(x)=L $, we know that the terms $|M-g(x)|$ and $|f(x)-L|$ become arbitrarily small for all $x$ sufficiently close to $a$. Then all three summands on the right hand side of \eqref{eqlimftimesg} become arbitrarily small, and so $|LM-f(x)g(x)|<\varepsilon$ for all $x$ sufficiently close to $a$.

The above considerations give an informal proof of the theorem; let us now cast them into a formal proof. Let $\varepsilon>0$. As $\lim\limits_{x\to a}f(x)=L$ there exists a number $\delta_1>0$ such that $|f(x)-L| |M|<\frac{\varepsilon}3$ for all $x$ for which $|x-a|<\delta_1$. Similarly as $\lim\limits_{x\to a}g(x)=M$ there exists a number $\delta_2$ such that $|L||g(x)-M| <\frac{\varepsilon}3$. Finally, there exists a number $\delta_3$ such that $|f(x)-L|<1 $ for all  $x$ for which $|x-a|<\delta_3$. Select $\delta $ to be the minimum of $\delta_1, \delta_2, \delta_3$. Then  for all $x$ for which $|x-a|<\delta$, we have that
\[
\begin{array}{rcl}
\displaystyle |L M - f(x)g(x)|&\leq&\displaystyle |L||M-g(x)|+ |f(x)-L||g(x)-M|+ |f(x)-L||M| \\~\\&\leq&\displaystyle \frac{\varepsilon}{3}+\frac{\varepsilon}{3}+\frac{\varepsilon}{3}=\varepsilon\quad .
\end{array}
\]
So far, for any $\varepsilon>0$ , we showed how to select a $\delta>0$ such that $|LM-f(x)g(x)|<\varepsilon$ for all $x$ such that $|x-a|<\delta$, which is precisely what we were aiming to show.
\end{proofOptional}


\subsection{Examples of limits computations with algebra}



\subsection{Algorithm for computing certain limits}


\section{Derivatives}

\subsection{Definition of derivative}\label{secDerivative}
\begin{definition}
We say that a function $f(x)$ is differentiable at a point $a$ if the limit
\begin{equation}\label{eqDefDerivative}
f'(a)\eqdef \lim_{h\to 0} \frac{f(a+h)-f(a)}{h}
\end{equation}
exists. If the limit exists, we say that it equals the derivative of $f$ at the point $a$.
\end{definition}

\subsection{Definition of non-vertical tangent}
\label{secDefTangent}

\subsection{Computing derivatives}
Recall that for a function of a variable $z$, $\frac{\diff }{\diff x}(f(x))$ denotes the derivative of $f(x)$. The notations $\frac{\diff }{\diff x}(f(x))$ and $f'(x)$ are equivalent.

Let $n$ be an integer.
\[
\frac{\diff }{\diff x} (x^n)= n x^{n-1}\quad .
\]
\[
\frac{\diff }{\diff x} (f(x)g(x)) = f'(x)g(x)+f(x)g'(x) \quad .
\]
Let $f(y)$ and $g(x)$ be differentiable functions.  Then
\[
\frac{\diff }{\diff x} \left(f( g(x))\right)= g'(x) f'(g(x)) \quad .
\]

\section{Differentials}
Let $f$ be a differentiable function of a variable $x$. Introduce formal expressions $\diff(f(x))$ (abbreviated as $\diff f$) and $\diff x$ so that they obey the following rule.
\begin{equation}\boxed{
f'(x)\diff x\eqdef \diff(f(x))\quad \quad,
}
\end{equation}
in abbreviated notation
\begin{equation}\boxed{
f'\diff x = \diff f\quad \quad .
}
\end{equation}

$\diff (f(x))$ is called the \emph{differential} of $f$, and expressions of the form $g(x)\diff x$ are called \emph{differential forms}.

Differentials obey the laws
\[
\begin{array}{rcll}
\diff (f+C)&=&\diff f \quad \quad &\mathrm{if~} C \mathrm{~is~constant}\\
\diff (Cf)&=&C\diff f \quad \quad &\mathrm{if~} C \mathrm{~is~constant}\\
\diff (f+g)&=&\diff f+\diff g \quad\quad\quad\quad \quad &(\mathrm{linearity})\\
\diff (fg)&=&g\diff f+f\diff g \quad\quad\quad\quad \quad &(\mathrm{Leibniz~rule})\\
\end{array}
\]
%Try to prove these laws from the definition!
We recall the notation $\displaystyle \int f(x)\diff x$ stands for the class of anti-derivatives of $f(x)$, i.e., for the functions $F(x)$ for which $F'(x) = f(x)$.

The fact that $f(x)+C=\displaystyle \int f'(x)\diff x$ is translated into differential notation as
\begin{equation}\boxed{
\displaystyle \diff \left(\int f(x)\diff x\right)= f(x)\diff x\quad ,
}
\end{equation}
and
\begin{equation}\boxed{
\int \diff f(x)= f(x)+C\quad .
}
\end{equation}




\chapter{Exponents, complex numbers and trigonometric functions: power series approach} \label{chapterExponentsTrigFromPowerSeries}
In Chapter \ref{chapterExponentsTrigFromEuclideanGeometry} we presented a construction of the trigonometric functions and exponents through Euclidean geometry. The present chapter is an alternative exposition of the topics of trigonometric function and exponents by using power series. The two expositions are of course equivalent: we prove that in the end of this chapter.


The power series approach to trigonometry postpones the geometric insight provided by the Euclidean geometry definition for the benefit of providing a much cleaner algebraic and numerical apparatus. In addition the power series approach makes the relation between trigonometry and exponents obvious. For example, Euler's formula holds essentially by definition.

The present chapter also gives answers to important questions such as how to compute (approximations of) the trigonometric and exponent functions using a computer, and is thus closer to applications in our modern computer-equipped world. 





\section{Exponent function, $\sin$ and $\cos$}
\subsection{Definition of $e^x$}
Consider the infinite sum
\begin{equation}\boxed{\label{eqExponentFunctionDefinition}
\begin{array}{rcl}
\displaystyle e^{z}&\eqdef &\displaystyle \sum_{n=0}^{\infty} \frac{z^n}{n!} = 1+z+\frac{z^{2}}{2}+\frac{z^3}{6}+\frac{z^4}{24}+\frac{z^5}{120}+\dots+\frac{z^n}{n!}+\dots\\
\end{array} \quad .
}
\end{equation}
We postpone the precise definition of infinite sums to Section \ref{secSequencesPowerSeries}, we rely on his or her intuition for the time being.
\begin{definition}\label{defNaturalExponent}
\index{exponent function} The function $e^z$ defined by equation \eqref{eqExponentFunctionDefinition} is called the natural exponent function.
\end{definition}
The notation $e^z$ is used to denote exponents however it appears we have used it to denote an infinite sum instead. For example if $z$ is a positive integer, the notation $e^z$ would suggest that 
\[
\begin{array}{rcll|l}
e^{z}&=& \underbrace{e\cdots e}_{z \text{ times} } && z\text{ is integer}>0\\
e^{z+w}&=&\underbrace{e \cdots e}_{z+w \text{ times} }&& z,w\text{ are integers}>0\\
&=&\underbrace{e\cdots  e}_{z \text{ times} }\underbrace{e\cdots  e}_{w \text{ times} }\\
&=& e^z e^w\quad .
\end{array}
\]
However, we just defined $e^x$ via the infinite sum \eqref{eqExponentFunctionDefinition}, rather than the rules of arithmetics. It is then not clear why, if at all, we should have that $e^{z+w}=e^{z}e^{w}$. In other words, it is not clear why the infinite sum
\[
e^{z+w} =\left(\sum\limits_{n=0}^{\infty} \frac{(z+w)^n}{n!}\right)
\]
should be equal to the product of infinite sums
\[
e^{z} e^w=\left(\sum\limits_{n=0}^{\infty} \frac{z^n}{n!}\right)\left(\sum\limits_{n=0}^{\infty} \frac{w}{n!} \right) .
\]
The equality $e^{z+w}= e^{z}e^{w}$ does indeed hold, as we show in the following Theorem \ref{thExponentArgumentsAddWhenMultiplying}. In other words, exponents add when we multiply exponent functions and when $n$ is an integer we have that
\begin{equation}\label{eqExponentToIntegerPowerRespectsArithmetics}
e^{n z}= e^{\underbrace{z+\dots +z}_{n\text{ times}} }=\underbrace{ e^{z}\cdots  e^{z}}_{n \text{ times}}  \quad .
\end{equation}
In this way our definition of  $e^{z}$ using an infinite sum \eqref{eqExponentFunctionDefinition} is consistent with the rules of arithmetics studied in high school.

While the Definition of $e^z$ via the infinite sum \eqref{eqExponentFunctionDefinition} does not immediately reveal the arithmetic properties of the exponent function, it is extremely useful for almost all other purposes. We start with a definition of the number $e$ which simultaneously shows how to get very accurate approximations for it.
\begin{definition}\label{defE} \index{e}\index{Euler!number}\index{Napier!constant}
The number $e$ given by
\[
e\eqdef e^1= 1+ \frac{1}{1} +\frac{1}2 +\frac{1}{3!} +\frac{1}{4!}+\dots \approx 2.71828
\]
is called \emph{Euler's number}, \emph{Napier's constant}, or simply the number $e$.
\end{definition}
From formula \eqref{eqExponentFunctionDefinition} we know how to evaluate $e^z$ for any number (including complex numbers): let us elaborate. Let
\begin{equation}\label{eqExponentSeriesLimit}
\displaystyle f(z,N):=\sum\limits_{n=0}^{N} \frac{z^n}{n!}\quad .
\end{equation}
Then \eqref{eqExponentFunctionDefinition} becomes
\[
e^z\eqdef \displaystyle\lim\limits_{N\to \infty} f(z, N)\quad .
\]
Thus it is clear how to find numerical approximations for $e^z= 1+z+\frac{z^{2}}{2} +\frac{z^3}{6} +\frac{z^4}{24} +\frac{z^5}{120} +\dots+\frac{z^n}{n!}+\dots$: pick a large number $N$, evaluate the sum of the first $N$ summands, discard the remaining ones, and declare the so obtained number, $f(z,N)$, to be an approximation of \eqref{eqExponentFunctionDefinition}. Up to the author's knowledge, this is in fact how computers evaluate the exponent $e^z$. The question of how to pick a sufficiently large $N$ so that the error is sufficiently small will be addressed in Section \ref{secSeriesConvergence}.

The careful reader has already noticed that the equality \eqref{eqExponentFunctionDefinition}, evaluated via \eqref{eqExponentSeriesLimit}, involves a limit which we have not shown to exist. We immediately fix this.
\begin{theorem} The infinite sum \eqref{eqExponentFunctionDefinition} used to define $e^z$ is well-defined, i.e. the limit $\displaystyle\lim_{N\to \infty} f(z, N)$ given in \eqref{eqExponentSeriesLimit} exists for all $z$.
\end{theorem}
We postpone the proof of this theorem to Section \ref{secSeriesConvergence}.

We now give the promised proof of the arithmetic properties of the exponent function.
\begin{theorem}\label{thExponentArgumentsAddWhenMultiplying}
Recall that $e^x$ is defined via \eqref{eqExponentFunctionDefinition}. Then we have that \begin{equation}\boxed{
e^{z} e^{w}=e^{z+w}\quad ,
}
\end{equation}
\end{theorem}
\begin{proofOptional}
\[
\begin{array}{rcl}
\displaystyle
\displaystyle e^{z} e^w&=&\displaystyle  \sum_{n=0}^{\infty} \frac{z^n}{n!} \sum_{m=0}^{\infty} \frac{w^m}{m!} \\ &\eqAttention&  \displaystyle  \sum_{s=0}^{\infty}\sum_{k=0 }^s \frac{z^{k}w^{s-k}}{k! (s-k)!} \\ &=& \displaystyle  \sum_{s=0}^{\infty}\sum_{k=0 }^s   \frac{z^{k}w^{s-k}}{s!} \frac{s!}{k! (s-k)!}\\ &\stackrel{\mathrm{Newton~binomial}}{=}& \displaystyle  \sum_{s=0}^{\infty} \frac{(z+w)^s}{s!}=e^{z+w}\quad .
\end{array}
\]
\end{proofOptional}
\begin{theorem}
\[
e^0=1\quad .
\]
\end{theorem}
\begin{proofOptional}
\[ e^0 = 1+ 0+ \frac{0^2}{2}+ \frac{0^3}{6}+\dots +\frac{0^n}{n!}+\dots = 1\quad .
\]
\end{proofOptional}

\begin{theorem}
$\frac{1}{e^{x}}= e^{-x}$\quad .
\end{theorem}
\begin{proof}
\[e^{-x}e^x=e^0 = 1\quad .
\]
Therefore $e^{-x}=\frac{1}{e^x}.$
\end{proof}

\optionalMaterial We end this section by stating an important alternative to \ref{eqExponentFunctionDefinition} for expressing $e^z$ as a limit.
\begin{theorem}\label{th(1+x/n)^n=e^x}
We have that
\begin{equation}\label{eq(1+x/n)^n=e^x}
\lim_{n\to \infty} \left(1+\frac{z}n\right)^n= e^z\quad .
\end{equation}
The limit holds for arbitrary $z\in \mathbb C$.
\end{theorem}
The limit \eqref{eq(1+x/n)^n=e^x} can be used as an alternative definition of $e^{z}$, but that results in a slightly longer exposition of the theory.

The limit \eqref{eq(1+x/n)^n=e^x} has important theoretical and practical applications. For example, if a compound interest of $ k\%$ is accumulated over $100/k $ compounding periods, the accumulation of the sum of money is
\[
\left(1+\frac{k}{100}\right)^{\frac{100}{k}},
\]
which, if $k$ is small, is an approximation for $e$.

For $z\in \mathbb R$, we will be in position to prove Theorem \ref{th(1+x/n)^n=e^x} in Section \ref{secLHospitalRevisited}.
\subsection{Definition of $\sin x$ and $\cos x$}
\index{sine function} \index{cosine function}

\begin{definition}
The functions $\sin z$ and $\cos z$ are defined as
\begin{equation}\boxed{
\sin z\eqdef \frac{e^{iz}-e^{-iz}}{2i} \quad ,
}
\end{equation}
\begin{equation}\boxed{
\cos z\eqdef \frac{e^{iz}+e^{-iz}}{2}\quad ,
}
\end{equation}
where we recall that $i$ stands for the imaginary unit ($i^2=-1$).
\end{definition}
\begin{theorem}\label{thSinCosMaclaurinSeries} We have the following equalities
\begin{equation}\label{eqSinCosMaclaurinSeries}
\begin{array}{rcl}
\cos x= &=&\displaystyle 1-\frac{x^2}{2}+ \frac{x^4}{4!}- \frac{x^6}{6!} +\dots - \frac{x^{4n+2}}{(4n+2)!}+ \frac{x^{4n+4}}{ (4n+4)!} +\dots\\
&=& \displaystyle \sum_{n=0}^\infty (-1)^n \frac{x^{2n} }{ (2n)!} \\
\sin x& =&\displaystyle  x-\frac{x^3}{3!}+ \frac{x^5}{5!}- \frac{x^7}{7!} +\dots + \frac{x^{4n+1}}{(4n+1 )!}- \frac{x^{4n+3}}{ (4n+3)!} +\dots\\
&=&\displaystyle \sum_{n=0}^{\infty}(-1)^n \frac{x^{2n+1}}{ (2n+1)!} \quad .
\end{array}
\end{equation}
\end{theorem}
\begin{proofOptional}
\[
\begin{array}{rcl}
\displaystyle\cos(x)&=& \displaystyle \frac{e^{ix}+e^{-ix}}{2}= \frac 12 \left(\sum_{i=0}^\infty (ix)^{n}+ \sum_{i=0}^\infty (-ix)^{n}\right)\\
&=&\displaystyle  \phantom{+} \frac12\left( 1+ \cancel{ix} + \frac{(ix)^2}2 +\cancel{\frac{(ix)^3}6}+\dots + \frac{(ix)^{2n}}{(2n)!}+ \cancel{\frac{(ix)^{2n+1}}{(2n+1)!}} +\dots \right)
\\
&&\displaystyle +  \frac12\left( 1-\cancel{ix} + \frac{(-ix)^2}2 +\cancel{\frac{(-ix)^3}6}+\dots + \frac{(-ix)^{2n}}{(2n)!}+ \cancel{\frac{(-ix)^{2n+1}}{(2n+1)!}}+\dots \right)\\
&=&\displaystyle  1+\underbrace{i^2}_{=-1}\frac{x^{2}}2+\underbrace{i^4}_{=1}\frac{x^{4}}{4!}+\dots\\
&=&\displaystyle  1-\frac{x^2}{2}+ \frac{x^4}{4!}- \frac{x^6}{6!} + \dots -\frac{x^{4n+2}}{(4n+2)!}+ \frac{x^{4n+4}}{(4n+4)!}+\dots\\
&=&\displaystyle \sum_{n=0}^\infty (-1)^n\frac{x^{2n}}{(2n)!}
\end{array}
\]


\[
\begin{array}{rcl}
\displaystyle\sin(x)&=&\displaystyle  \frac{e^{ix}- e^{-ix}}{2i} = \displaystyle \frac {-i}{2} \left(\sum_{i=0}^\infty (ix)^{n}- \sum_{i=0}^\infty (-ix)^{n}\right) \\
&=&\displaystyle -\frac{i}{2}\left(\cancel{1} + ix + \cancel{\frac{(ix)^2}2} +{\frac{(ix)^3}6}+\dots + \cancel{\frac{(ix)^{2n}}{(2n)!}}+ {\frac{(ix)^{2n+1}}{(2n+1)!}} +\dots \right)\\
&&\displaystyle  +  \frac{i}{2}\left(\cancel{1} -{ix} + \cancel{\frac{(-ix)^2}2} +{\frac{(-ix)^3}6}+\dots + \cancel{\frac{(-ix)^{2n}}{(2n)!}}+ {\frac{(-ix)^{2n+1}}{(2n+1)!}}+\dots \right)\\
&=&\displaystyle  - \underbrace{i^2}_{=-1} x-\underbrace{i^4}_{=1} \frac{x^3}{3!}- \underbrace{i^{6}}_{=-1} \frac{x^5}{5!} -\dots\\
&=&\displaystyle  x-\frac{x^3}{3!}+ \frac{x^5}{5!} -\frac{x^7}{7!} +\dots +\frac{x^{4n+1}}{(4n+1)!}- \frac{x^{4n+3}}{(4n+3)!}+\dots\\
&=&\displaystyle \sum_{n=0}^{\infty}(-1)^n\frac{x^{2n+1}}{(2n+1)!}\quad .
\end{array}
\]
\end{proofOptional}
\begin{corollary}\label{corSinCosOfRealIsReal}
Let $x $ be a real number. Then $\sin x$ and $\cos x$ are real numbers.
\end{corollary}
\begin{proof}
We look at the power series for $\sin x$ and $\cos x$ given by \eqref{eqSinCosMaclaurinSeries} and see that $i$ does not participate in either expression. Therefore substituting real numbers in the expressions for $\sin x$ and $\cos x$ given by \eqref{eqSinCosMaclaurinSeries} yields a real number.
\end{proof}

\begin{theorem}
\begin{equation}\boxed{\label{eqsinSquaredPlusCosSquared}
\sin^2 z +\cos^2 z= 1\quad .
}
\end{equation}

\end{theorem}
\begin{proofOptional}
\begin{equation*}
\begin{array}{rcl}
\sin^2 z +\cos^2 z&=&\displaystyle  \frac{(e^{iz} + e^{-iz})^2 }{2^2} + \frac{(e^{iz}+e^{-iz})^2}{(2i)^2}=\\
&=&\displaystyle \frac{ \cancel{(e^{iz})^2} + 2 e^{iz}e^{-iz} + \cancel{(e^{-iz})^2}}{4} \\
&&\displaystyle -\frac{(\cancel{(e^{iz})^2}-2e^{iz}e^{-iz} + \cancel{(e^{-iz})^2})}{4} \\
&=& \displaystyle e^{iz}e^{-iz}= e^{iz-iz}=e^0=1\quad .
\end{array}
\end{equation*}
\end{proofOptional}
\begin{corollary}
If $x\in \mathbb R$ then $|\sin x|\leq 1$ and $|\cos x| \leq 1$.
\end{corollary}
\begin{proof}
By  \eqref{eqsinSquaredPlusCosSquared} $\sin^2 x+\cos ^2 x=1$. Since $x$ is a real number, so are $\sin x$ and $\cos x$ (Corollary \ref{corSinCosOfRealIsReal}) and therefore $\sin^2x\leq 1$ and $\cos^2 x$ are positive real numbers. Therefore  $|\sin x|^2\leq 1$ and $|\cos x|^2\leq 1$ and therefore $|\sin x|\leq 1$ and $|\cos x|\leq 1$.
\end{proof}
\subsection{Euler's formula}
\begin{theorem}[Euler's formula]
We have the following equality.
\begin{equation}\boxed{\label{eqEulerExp}
e^{iz}= \cos z+ i\sin z \quad .
}
\end{equation}
\end{theorem}
\begin{proofOptional}
\[
\begin{array}{rcl}
\displaystyle e^{iz}&=&\displaystyle  \frac{2}{2}e^{iz}= \frac{2e^{iz} + e^{-iz} - e^{-iz}}{2} = \frac{e^{iz}+e^{-iz}}2 +\frac{e^{iz}-e^{-iz}}2\\
&=&\displaystyle  \cos z + \frac{i(e^{iz}-e^{-iz})}{2i}\\
&=& \displaystyle \cos z+i \sin z\quad .
\end{array}
\]

\end{proofOptional}

\begin{theorem}\label{thSinCosAngleSum}
\[
\sin(x+ y)=\sin x\cos y +\sin y\cos x
\]
\[
\cos (x+y)= \cos x \cos y - \sin x \sin y
\]
\end{theorem}
Before we proceed with the proof, we show a quick way to remember the above formulas. Suppose $x,y\in \mathbb R$. Then
\[
\begin{array}{rcl}\cos (x+y) + i\sin (x+y)&\stackrel{\mathrm{Euler~formula~}\eqref{eqEulerExp}}{=}& e^{i(x+y)} =e^{ix}e^{iy}\\&=& (\cos x +i \sin y)(\cos y + i \sin y)\\&=& \cos x \cos y - \sin x \sin y\\&& +i(\sin x \cos y+ \sin y \cos x) \quad .\end{array}
\]
Now since both $x,y$ are real the only way to satisfy the above equality is to have the formulas $\sin(x+ y)=\sin x\cos y +\sin y\cos x  $ and $\cos (x+y)=\cos x \cos y - \sin x \sin y$. However, this argumentation works only for $x,y\in \mathbb R$.

To give a proof for arbitrary complex numbers one has to do a slightly longer computation, as the following proof shows.

\begin{proofOptional}[ of Theorem \ref{thSinCosAngleSum}]
This is a straightforward computation:
\[
\begin{array}{l}
\cos x \cos y - \sin x \sin y \\
\displaystyle= \frac{(e^{ix}+e^{-ix})(e^{iy}+e^{-iy})}4 - \frac{(e^{ix}-e^{-ix})(e^{iy}-e^{-iy}) }{4i^2}\\
\displaystyle = \frac{ e^{i(x+y)}+ \cancel{e^{i(y-x)}} +{e^{ -i(x+y)}} + \cancel{e^{i(x-y)}} +e^{i(x+y)}+e^{-i(x+y)}- \cancel{e^{i(x-y)}}- \cancel{e^{ i(y- x)}} }4\\
= \cos(x+y)\quad .\end{array}
\]
The proof for $\sin(x+y)$ is similar:
\[\begin{array}{l}
\sin x \cos y + \cos x \sin y\\
= \displaystyle\frac{(e^{ix}-e^{-ix})(e^{iy}+e^{-iy})}{4i} +\frac{(e^{ix}+e^{-ix})(e^{iy}-e^{-iy}) }{4i}\\
= \displaystyle\frac{ e^{i(x+y)}- \cancel{e^{i(y-x)}} - {e^{-i(x+y)}} + \cancel{e^{i(x-y)}} +e^{i(x+y)}-e^{-i(x+y)}- \cancel{e^{i(x-y)}}+\cancel{e^{i(y-x)}} }4\\
= \displaystyle \sin(x+y)\quad .
\end{array}
\]
\end{proofOptional}




We have that $(e^{iz})^2= e^{2iz}= \cos (2z)+ i\sin(2z)$. On the other hand, $(e^{iz})^2 = (\cos z+i\sin z)^2= \cos^2z+2i\sin z\cos z - \sin^2 z$. Therefore $\cos (2z)+ i\sin(2z)= (\cos^2 z- \sin^2 z)+ i(2\sin z\cos z) $ and therefore we get the formulas
\begin{equation}\label{eqSin2xCos2x}
\sin (2z)=2\sin z \cos z \quad\quad \quad \cos(2z)=\cos^2 z- \sin^2 z.
\end{equation}
There are two equivalent ways of defining $\sin z$ and $\cos z$: one given here, and one studied in some high schools and in alternative expositions of the calculus course. When using the high school approach, one defines that the functions $\sin \varphi$ and $\cos\varphi$ measure the lengths of the legs of the right angle triangle with another angle $\varphi$ and hypotenuse of length 1, as indicated on figure \ref{figUnitCircleSineCosine}. The angle $\varphi$ is defined as the length of the arc of the unit circle colored in red.

The fact that $\varphi$ measures the arc drawn with red in the figure \ref{figUnitCircleSineCosine} is to be taken, for the time being, on ``good faith''. This  will be fixed after studying lengths of curves in Section \ref{secCurveLength}.


\begin{figure}[h]\caption{The $\sin\varphi$ and $\cos\varphi$ as legs of a right angle triangle with other angle $\varphi$.}

\optionalDisplay{
\psset{xunit=3cm,yunit=3cm}
\begin{pspicture}(-2,-2)(2,2)
\psline[linecolor=gray]{->}(-1.5,0)(1.5,0) % x-axis
\psline[linecolor=gray]{->}(0,-1.5)(0,1.5) % y-axis
\rput[l](1.5,0){$\Re z$}
\rput[b](0,1.5){$\Im z$}
\rput[bl](0.03,1.03){$i$}
\rput[bl](1,-0.1){$1$}
\rput[c](0,1){$\bullet$}
\rput[c](1,0){$\bullet$}
\psplot{-1}{0.5}{1 x x mul sub sqrt}

\psplot[linecolor=red]{0.5}{1}{1 x x mul sub sqrt}

\psplot{-1}{1}{1 x x mul sub sqrt -1 mul}
\rput(0.5, 0){\psplot{-0.1}{0}{ 0.01001 x x mul sub sqrt 1 mul}}
\rput[l](0.53, 0.34){$\sin\varphi$ $=\Im \frac{z}{|z|}$}
\rput[t](0.44, -0.03){$\cos\varphi=\Re \frac{z}{|z|}$}

\psline(0,0)(0.5,0.866)
\rput[c](0.5,0.866){$\bullet$}
\rput(0.47,1){$\frac{z}{|z|}$}
\psline[linestyle=dotted](0.5,0.866)(1,1.74)
\rput(1.1,1.81){$z$}
\rput[c](1,1.74){$\bullet$}

\psline(0.5,0)(0.5,0.866)
\rput[l](0.84,0.65){$\varphi$}
\end{pspicture}
} %end optionalDisplay
\label{figUnitCircleSineCosine}
\end{figure} %
\subsubsection{Examples and Exercises}
\begin{problem}
Use the trick used to derive \eqref{eqSin2xCos2x} to derive a formula for $\sin(3z)$ and $\cos (3z)$. Your solution may start like this. ``We have that $(e^{iz})^3=e^{3iz}=\cos (3z)+i\sin(3z)$. On the other hand, $(e^{iz})^3= \dots$''.
\end{problem}
\subsection{Definition of $\pi$}
\label{secDefinitionPi}

The proof of the following theorem is a bit more difficult and we omit it from the current version of this textbook. A proof might be included in a future version of this textbook.
\begin{theorem}\label{thPiExists}
There exists a positive real number $x$ such that \index{$\pi$}
\begin{equation}\boxed{
e^{ix}=i\quad .
}
\end{equation}
\end{theorem}

\begin{definition}
Define $\pi$ to be the smallest positive real number, the existence of which is guaranteed by \ref{thPiExists} for which
\begin{equation}\boxed{
e^{i\frac{\pi}{2} }= i\quad .
}
\end{equation}
\end{definition}



\begin{theorem} The following equalities hold.
\begin{equation}\label{eqe^ipi}
\begin{array}{rcl}
\displaystyle e^{i\pi}&=&-1\\
\displaystyle e^{2i\pi}&=&=1\\
\displaystyle \sin \frac{\pi}2 &=& 1\\
\displaystyle \cos \frac{\pi}2 &=& 0\quad .
\end{array}
\end{equation}
\end{theorem}
\begin{proofOptional}
The first two equalities are a direct computation.
\[
\begin{array}{rcl}
\displaystyle e^{i\pi}&=&\displaystyle  e^{i\frac{\pi}2}e^{i\frac{\pi}2}= i^2= -1\\
\displaystyle e^{2i\pi}&=&\displaystyle  e^{\pi i} e^{\pi i}= (-1)^2=1\quad .
\end{array}
\]
For the last two equalities we compute:
\[
0+ 1*i=i=e^{i\frac{\pi}2}= \cos \frac{\pi}2 + i\sin \frac\pi 2\quad .
\]
As $\pi$ is real, by Corollary \ref{corSinCosOfRealIsReal}, $\sin\frac{\pi}{2}$ and $\cos \frac{\pi}{2}$ are real. Therefore we can compare both sides to see that $0=\cos \frac{\pi}{2}$ and $1=\sin \frac \pi 2$.
\end{proofOptional}

The length of the circumference of a circle of radius $r$ is $2\pi r$ and the area of a circle of radius $r$ is $\pi r^2$. We will prove these formulas very soon.

For $x\in \mathbb R$, graphs of $\sin x$ and $\cos x$ can be found in section \ref{secGraphsTrigFunc}.
\begin{theorem} We have that
\[
\sin \left(x+\frac{\pi}{2}\right)= \cos x \quad .
\]
\end{theorem}

\begin{proof}
\[
\cos\left(x+\frac{\pi}{2}\right)+ i\sin \left(x+\frac{\pi}2\right)=e^{i(x+\frac{\pi}{2})}= e^{ix} \underbrace{e^{ i\frac{\pi}2 }}_{=-1}= ie^{ix}= i(\cos x + i \sin x)= i\cos x- \sin x\quad .
\]
Comparing both sides of the above expression shows that $\cos(x+\frac{\pi}{2})=-\sin x$ and $\sin(x+\frac{\pi}{2})= \cos x$.
\end{proof}
This equality is indicated by the red color in the graphs of $\sin$ and $\cos$ in section \ref{secGraphsTrigFunc}.

Recall that $\overline{a+ib}=a-ib$  (complex conjugation). Then recall \eqref{eqComplexConjugationIsFieldHMM}:
\[
\overline {z w}= \bar z \bar w\quad .
\]

\[
\overline {z^n}= \overline {\underbrace{z\cdots z}_{n\mathrm{~times}}} = \underbrace{\bar z\cdots  \bar z}_{n\mathrm{~times}}= \bar {z}^n
\]

\begin{theorem}
Let $y\in \mathbb R$ (i.e., $y$ is a real number). Then
\begin{equation}\label{eqexponentiyconjugateisinverse}
\overline {e^{iy}}=e^{-iy}= \frac{1}{e^{iy}}\quad .
\end{equation}
\end{theorem}
\[
\overline {e^{iy}}= \sum_{n=0}^\infty \frac{\overbrace{\bar i}^{=-i} \bar y^n }{ n! }= \sum_{n=0}^\infty \frac{(i(-y))^n}{n!}=  e^{-iy}= \frac{1}{e^{iy}}\quad .
\]
\begin{theorem}
Let $y\in \mathbb R$. Then $|e^{iy}|=1$.
\end{theorem}
\begin{proof}
\[
|e^{iy}|^2 \eqdef e^{iy}e^{-iy}\underbrace{ =}_{ \eqref{eqexponentiyconjugateisinverse}} 1 \quad .
\]
For arbitrary $z$ we have that $|z|$ is a non-negative real number and so it follows that $|e^{iy}|=1$.
\end{proof}

\begin{prop}
Let $x\in \mathbb R$. Then $e^x$ is a positive real number.
\end{prop}


\chapter{Logarithms and inverse trigonometric functions}
\subsection{Polar form of complex numbers}
\label{secPolarFormComplexNumbers}

Let $\rho, \varphi\in \mathbb R$ (real numbers). Then the expression
\[
e^{\rho+i\varphi}=\underbrace{e^{\rho}}_{|e^{\rho+i\varphi}|}(\cos \varphi + i\sin \varphi)
\]
is defined to be a polar form of the number $e^{\rho+i\varphi}$. Let $z=a+ib$, where $a, b\in \mathbb R$ (real numbers). Let $w\neq 0$. Suppose $\varphi$ is such a number that
\[
(a+ib)= \underbrace{\sqrt{a^2+b^2}}_{=|z|}(\cos \varphi + i \sin \varphi)\quad.
\]
Then $\varphi$ is called argument of $a+ib$. We write \[\varphi=\arg z\quad .\] In other words
\[z=\underbrace{|z|}_{\sqrt{\Im(z)^2+\Re(z)^2}}\left(\cos(\arg z) + i\sin (\arg z)\right)\quad .
\]
Look at figure \ref{figUnitCircleSineCosine}. Identify the parts of figure \ref{figUnitCircleSineCosine} corresponding to every term you see in the above formula.
Let $e^{\rho+i\phi}$ and $e^{\tau+i\psi}$ be two complex numbers. Then $e^{\rho+i\phi}e^{\tau+i\psi} = e^{\rho+\tau+i(\phi+\psi)}$. Therefore
\begin{equation}\boxed{
\text{Multiplying complex numbers adds arguments and multiplies absolute values.}
}
\end{equation}

\subsection{$\log z$, $\ln x$, $\tan z$, $\arctan z$, $\arcsin x$, $\arccos x$}
Consider the exponent function $e: \mathbb R \to \mathbb R$. Then the range of the exponent function is the positive real numbers. Define $\ln y$ to be the inverse of $e^x$ in this range. In other words, $\ln(y)$ is defined when $y$ is a positive real number. In mathematical notation, $\ln: (0,+\infty)\to \mathbb R$. Then we have that
\begin{equation}\boxed{
\ln (e^x)\eqdef x\quad\quad \quad \quad  e^{\ln y}\eqdef y\quad .
}
\end{equation}

Let $z$ be a complex number for which $\cos z\neq 0$.

\begin{equation}\boxed{
\tan z\eqdef \frac{\sin z}{\cos z}\quad .
}
\end{equation}

Let $x$ be a real number. Consider the tangent function acting in the interval $(-\frac{\pi}{2}, \frac{\pi}{2})$, in mathematical notation, $\tan :(-\frac{\pi}{2}, \frac{\pi}{2})\to \mathbb R$. Then the function $\tan(x)$ has an inverse which is called $\arctan(x)$. In other words,
\[
\tan (\arctan(x))= x\quad .
\]

Let $D$ be the set of complex numbers which are neither negative real numbers nor zero. Consider the exponent function $e: D\to \mathbb C$ which maps $z\in D$ to $e^z$. Then define $\log_{(-\pi,\pi)} z$, or $\log z$ for short, to be the inverse of this function, that is,
\begin{equation}\boxed{
e^{\log z} = z\quad \quad \quad \log (e^w)=w\quad .
}
\end{equation}
The imaginary part of $\log_{(-\pi,\pi)}(z)$ is a number in the interval $(-\pi,\pi)$, i.e.,
\[
-\pi<\Im(\log z)<\pi\quad .
\]

The notation $\ln x$  is used when the argument $x$ is a positive real number. The function $\ln$ is called ``natural logarithm''. The notation $\log z$ is used when the argument $z$ is a complex number. If it so happens that $z$ is a complex number that is also a positive real number, then $\log(z)=\ln(z)$. Therefore the main role of the notation $\log$ and $\ln$ is to remind us of whether we are computing with complex or with real numbers.

In non-mathematical texts, the notation $\log_{10} x\eqdef \frac{\ln x}{\ln 10}$ is sometimes used to denote ``logarithm base 10'' (similar notation can be used for other bases as well). In mathematical texts, the notation $\log_{10} x$ is not accepted and should be avoided. Even when writing papers in the natural sciences, where the use of the notation $\log_{10} x$ is acceptable, I  recommend avoiding this notation.

\begin{lemma}
Let $\varphi\in (-\frac{\pi}{2} ,\frac{\pi}{2})$ (i.e., $\varphi$ is a real number in the interval $(0,\frac{\pi}{2})$). Then
\begin{equation}\label{eqSinCosViaTan}
\begin{array}{rcl}
\sin \varphi &=& \frac{ \tan \varphi}{\sqrt  {1+\tan^2 \varphi  }}\\
\cos \varphi &=& \frac{1}{\sqrt  {1+\tan^2 \varphi  }}\quad .\\
\end{array}
\end{equation}
\end{lemma}
\begin{proof}
We compute that
\[\frac{1}{1+\tan^2 \varphi} = \frac{1}{1+\frac{\sin^2 \varphi}{\cos^2\varphi}}= \frac{1}{\frac{\sin^2\varphi+\cos^2\varphi}{\cos^2\varphi}}= \cos^2{\varphi}\quad ,
\]
and as both sides are positive real numbers we can take the square root on both sides to get the second equality in \eqref{eqSinCosViaTan}. The fact that both sides are positive real numbers is important: we have not defined square root for negative real numbers or for arbitrary complex numbers. The remaining equality is more tricky:
\begin{equation}\label{eqSinViaTanTemp}
\sin \varphi = \pm \sqrt{1-\cos^2\varphi}= \pm \sqrt{1-\frac{1}{1+\tan^2\varphi}}= \pm \sqrt{\frac{\tan^2\varphi}{1+\tan^2\varphi}}=
\frac{\pm (\pm \tan\varphi)}{\sqrt{1+\tan^2\varphi}}\quad ,
\end{equation}
where we have used that for an arbitrary real number $t$ we have that $|t|=\sqrt{t^2}=\pm t$. In the interval $\varphi\in (-\frac{\pi}{2} ,\frac{\pi}{2})$, $\sin \varphi$ and $\tan \varphi$ have the same sign. Therefore in \eqref{eqSinViaTanTemp} the expression $\pm(\pm \tan\varphi)$ has the same sign as $\tan \varphi$ and we can omit the $\pm$ signs.
\end{proof}

\begin{theorem}\label{thLogarithmComplex}
\index{logarithm!complex} Let $z$ be a complex number such that $\Re z>0$. Then
\begin{equation}\boxed{
\log z = \ln |z| + i\arctan \left(\frac{\Im z}{\Re z}\right)\quad .
}
\end{equation}
\end{theorem}
\begin{proofOptional}
~We aim to prove that $e^{\log z}=z$. Set
\[
\arctan \left(\frac{\Im z}{\Re z}\right)\quad .
\]
By definition $\varphi\in  \left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$ and therefore we may apply \eqref{eqSinCosViaTan} as follows.
\[
e^{\ln |z| + i\varphi}= e^{\ln |z|} (\cos\varphi+i\sin\varphi )=|z|(\cos\varphi+i\sin\varphi ) \stackrel{\eqref{eqSinCosViaTan}}{=}
|z| \left(\frac{1}{\sqrt{1+\tan^2\varphi}} +i \frac{\tan \varphi}{\sqrt{1+\tan^2\varphi}} \right)\quad .
\]
On the other hand, as $\varphi\in  \left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$, we have that $\tan\varphi= \tan\left(\arctan\left(\frac{\Im z}{\Re z}\right)\right)= \frac{\Im z}{\Re z}$. Therefore
\[
\begin{array}{rcl}
\displaystyle e^{\ln |z| + i\varphi}&=&\displaystyle \underbrace{|z|}_{\sqrt{(\Re z)^2+(\Im z)^2}} \left(\frac{1}{\sqrt{1+\frac{(\Im z)^2}{(\Re z)^2}}} +i \frac{\frac{\Im z}{\Re z}}{\sqrt{1+\frac{(\Im z)^2}{(\Re z)^2}}} \right) \\
&=&\displaystyle \cancel{\sqrt{(\Re z)^2+(\Im z)^2}} \left(\frac{|\Re z|}{\cancel{\sqrt{(\Re z)^2+(\Im z)^2}}} +i \frac{|\Re z|\frac{\Im z}{\Re z}}{\cancel{\sqrt{(\Re z)^2+(\Im z)^2}}} \right) \\
&=&\displaystyle|\Re z | + i|\Re z| \frac{\Im z}{\Re z}\quad .
\end{array}
\]
Caution to the last equality: we cannot cancel $|\Re z| $ with $\Re z$ if the two expressions have different sign. However, we requested in the start that $z$ was such a number that $\Re z>0$, and therefore $|\Re z|=\Re z $. Therefore
\[
e^{\ln |z| + i\varphi}=\Re z+i\Im z = z\quad ,
\]
which completes the proof.
\end{proofOptional}

\subsubsection{Examples and Exercises}
\begin{problem}
\input{../../modules/complex-numbers/homework/plot-complex-number-find-principal-argument-1}
\end{problem}
\input{../../modules/complex-numbers/homework/plot-complex-number-find-principal-argument-1-solutions}

\begin{problem}
\input{../../modules/complex-numbers/homework/frac-linear-transformations-preserving-unit-circle}
\end{problem}

\begin{problem}
\input{../../modules/inverse-trig/homework/trig-evaluated-on-inverse-trig-1}
\end{problem}

\input{../../modules/inverse-trig/homework/trig-evaluated-on-inverse-trig-1-solutions}

\begin{problem}
\input{../../modules/inverse-trig/homework/trig-evaluated-on-inverse-trig-2}
\end{problem}
\input{../../modules/inverse-trig/homework/trig-evaluated-on-inverse-trig-2-solutions}

\begin{problem} \input{../../modules/inverse-trig/homework/trig-evaluated-on-inverse-trig-3}
\end{problem}
\input{../../modules/inverse-trig/homework/trig-evaluated-on-inverse-trig-3-solutions}


\begin{problem}
\input{../../modules/inverse-trig/homework/artan-sum-problem}
\end{problem}



\section{Plots of trigonometric functions and their inverses} \label{secGraphsTrigFunc}
Let $x\in \mathbb R$ ($x$- real number). Then $\sin x$ and $\cos x$ can be drawn as follows.

\optionalDisplay{
\psset{xunit=0.5cm,yunit=0.5cm}
\begin{pspicture*}(-10,-3)(10,3)
\fcAxesStandard{-9.5}{-2.5}{9.5}{2.5}
\rput[l](9.5,0){$x$}
\rput[b](0,2.5){$y$}
\rput(4,1.6){$y=\sin x$}
\rput[lt](1,0){1}
\psline(1,-0.1)(1,0.1) % x unit mark
\rput[bl](0,1){1}
\psline(-0.1,1)(0.1,1) % y unit mark

\psplot[linecolor=\fcColorGraph]{-9.5}{0}{x 180 mul 3.1415 div sin}
\psplot[linecolor=\fcColorGraph]{1.571}{9.5}{x 180 mul 3.1415 div sin}

\psplot[linecolor=green,linewidth=1.5pt]{0}{1.571}{x 180 mul 3.1415 div sin}
\end{pspicture*}


\begin{pspicture*}(-10,-3)(10,3)
\fcAxesStandard{-9.5}{-2.5}{9.5}{2.5}
\rput[l](9.5,0){$x$}
\rput[b](0,2.5){$y$}
\rput(4,1.6){$y=\cos x$}
\rput[lt](1,0){1}
\psline(1,-0.1)(1,0.1) % x unit mark
\rput[bl](0,1){1}
\psline(-0.1,1)(0.1,1) % y unit mark
\psplot[linecolor=\fcColorGraph]{-9.5}{-1.571}{x 180 mul 3.1415 div cos}
\psplot[linecolor=\fcColorGraph]{0}{9.5}{x 180 mul 3.1415 div cos}

\psplot[linecolor=green,linewidth=1.5pt]{-1.571}{0}{x 180 mul 3.1415 div cos}

\end{pspicture*}

 Let $x\in \mathbb R$ (i.e., let $x$ be real). Then $e^x$ and $\ln x$ may be plotted as

\psset{xunit=0.5cm,yunit=0.5cm}
\begin{pspicture*}(-3,-10)(10,10)
\fcAxesStandard{-3}{-3}{9.5}{9.5}
\rput[l](9.5,0){$x$}
\rput[b](0,9.5){$y$}
\rput(2,7.6){$y=e^x$}
\rput[lt](1,0){1}
\psline(1,-0.1)(1,0.1) % x unit mark
\rput[br](0,1){1}
\psline(-0.1,1)(0.1,1) % y unit mark
\fcFullDot{1}{2.71828}
\rput[l](1.1,2.71828){$e\approx 2.71828$}

\psplot[linecolor=\fcColorGraph]{0}{3}{ 2.7182818284 x exp}
\psplot[linecolor=\fcColorGraph]{-3}{0}{1 2.7182818284 div x -1 mul exp}
\psplot[linecolor=\fcColorGraph]{0.0497870684}{10}{  x ln}
\rput(7.0,2.3){$y=\ln x$}
\psline[linestyle=dashed, linecolor=blue](-3,-3)(9.5,9.5)
\end{pspicture*}

Let $x\in \mathbb R$. Then $\tan x$ can be plotted as follows.


\psset{xunit=0.5cm,yunit=0.5cm}
\begin{pspicture*}(-5,-10)(10,10)
\fcAxesStandard{-5}{-3}{5}{9.5}
\rput[l](5.5,0){$x$}
\rput[b](0,9.5){$y$}
\rput[t](1,-0.1){1}
\psline(1,-0.1)(1,0.1) % x unit mark
\rput[lb](1.570796327,0.1){$\frac{\pi}2$}
\psline(1.570796327,-0.1)(1.570796327,0.1) % pi/2 unit mark
\rput[br](0,1){1}
\psline(-0.1,1)(0.1,1) % y unit mark

\multido{\r=-1.570796327+1.570796327}{3}{%
\put(\r,0){\psplot[linecolor=\fcColorGraph]{-1.552}{1.552}{ 180 x mul  3.1415 div tan} }
\put(\r,0){\psline[linestyle=dashed](1.570796327,-10)(1.570796327,10) }
}
\end{pspicture*}


\psset{xunit=0.5cm,yunit=0.5cm}
\begin{pspicture*}(-5,-5)(5,5)
\fcAxesStandard{-5}{-3}{4.5}{4.5}
\rput[l](4.5,0){$x$}
\rput[b](0,4.5){$y$}
\rput[t](1,-0.1){1}
\psline[linecolor=gray](1,-0.1)(1,0.1) % x unit mark
\rput[lb](0.1, 1.570796327){$\frac{\pi}2$}
\psline[linecolor=gray](-0.1,1.570796327)(0.1,1.570796327) % pi/2 unit mark
\rput[br](0,1){1}
\psline[linecolor=gray](-0.1,1)(0.1,1) % y unit mark

\psplot[linecolor=\fcColorGraph]{-5}{5}{ x ATAN}
\psline[linecolor=blue, linestyle=dashed](-5,1.570796327)(5,1.570796327)
\psline[linecolor=blue, linestyle=dashed](-5,-1.570796327)(5,-1.570796327)
\rput(3,0.5){$y=\arctan x$}
\end{pspicture*}
} %end of optionalDisplay

\section{Derivatives of trigonometric, exponent functions and their inverses}

\begin{theorem}The function $e^z$ is differentiable and
\begin{equation}\boxed{\label{eqDerivativeexponent}
\frac{\diff}{\diff z}\left( e^z\right)= e^z
}
\end{equation}
\end{theorem}
The fact that $e^z$ is differentiable follows from the more general Theorem \ref{thDifferentiatingIntegratingPowerSeriesNonFormal} the proof of which we postpone to Section \ref{secSequencesPowerSeries}. More precisely, Theorem \ref{thDifferentiatingIntegratingPowerSeriesNonFormal} states that, under certain conditions, differentiating infinite sums is the same as first differentiating each summand and then adding. Assuming that theorem, the algebra behind proving that $\frac{\diff}{\diff z}\left( e^z\right)=e^z$ is straightforward, as we show below.

\begin{proofOptional}[~of \eqref{eqDerivativeexponent}]
\[
\begin{array}{rcl}
\displaystyle \frac{\diff}{\diff z}e^z &=&\displaystyle  \frac{\diff}{\diff z}\left( \sum_{n=0}^{\infty} \frac{z^n}{n!}\right)\\
&\underbrace{\eqAttention}_{\substack{\text{Caution: sum is infinite}\\\text{Use Theorem }\ref{thDifferentiatingIntegratingPowerSeriesNonFormal} }} &\displaystyle \sum_{n=0}^{\infty} \frac{\diff}{\diff z} \left( \frac{z^n}{n!}\right)\\
&=&\displaystyle \sum_{n=1}^{\infty} \frac {nz^{n-1}}{n!}\\
&=&\displaystyle \sum_{n=1}^{\infty} \frac {z^{n-1}}{(n-1)!} \\
&=&\displaystyle \sum_{m=0}^{\infty} \frac {z^{m}}{m!}\\
&=&e^z\quad .
\end{array}
\]
\end{proofOptional}

\begin{theorem}
The derivative of $\ln x$ is given by
\begin{equation}\boxed{
(\ln x)' = \frac{1}x
}
\end{equation}
\end{theorem}
\begin{proof}
\[
\begin{array}{rcll|l}
e^{\ln x}&=& x  &&\displaystyle  \frac {\diff}{\diff x}\\
(e^{\ln x})(\ln x)' &= &1\\
 (\ln x)' &=& \displaystyle \frac{1}x\quad .
\end{array}
\]
\end{proof}

\begin{theorem}
Let $x\in \mathbb R$ (i.e., $x$ is real) and $a\in \mathbb R$, $a\neq 0$. Suppose in addition $x>0$. Then
\begin{equation}\label{eqXtotheAthDerivative}
(x^a)'=ax^{a-1}\quad .
\end{equation}
\end{theorem}
\begin{proof} As $x=e^{\ln x}$ we have that
\[
x^a=  e^{a\ln x}\quad .
\]
Therefore
\[
(x^{a})'=(e^{a\ln x})'=ae^{a\ln x} (\ln x)' =a x^{a}\frac{1}{x} = ax^{a-1} \quad .
\]
\end{proof}
\begin{theorem}
The derivatives of $\sin x, \cos x, \tan x, \cot x$ are given by
\begin{equation}\boxed{
\begin{array}{rcl}
(\sin x)'&=&\displaystyle \cos x\\
(\cos x)'&=&-\sin x\\
(\tan x)'&=&\displaystyle \frac{1}{\cos ^2 x}\\
(\cot x)'&=&\displaystyle -\frac{1}{\sin ^2 x}\quad .
\end{array}
}
\end{equation}
\end{theorem}
\begin{proof}
\[
\begin{array}{rcl}
\displaystyle (\sin x)'&=&\displaystyle \left(\frac{ e^{ix}- e^{-ix}}{2i}\right)' =\frac{ie^{ix}- (-i)e^{-ix}}{2i} =\frac{e^{ix} +e^{-ix}}2=\cos x\\
\displaystyle(\cos x)' &=&\displaystyle \left(\frac{e^{ix} +e^{-ix}}{2}\right)' =\frac{ie^{ix}- ie^{-ix}}{2} = -\frac{e^{ix} -e^{-ix}}{2i}=-\sin x \\
\displaystyle (\tan x)' &=&\displaystyle  \frac{(\sin x)'\cos x- \sin x (\cos x)'}{ \cos x^{2}} = \frac{\sin^2x+ \cos^2x}{\cos^2x} =\frac{1}{\cos^2x} \quad .\\
\end{array}
\]
The derivative of $\cot x$ is computed in a similar fashion and we leave it to the reader.
\end{proof}
\begin{theorem}
The derivatives of $\Arcsin x, \Arccos x, \Arctan x$ are given by 
\[
\begin{array}{rcl}
(\Arcsin x)'&=&\displaystyle \frac{1}{\sqrt{1-x^2}} \\
(\Arccos x)'&=&\displaystyle -\frac{1}{\sqrt{1-x^2}}\\
(\Arctan x)'&=&\displaystyle \frac{1}{1+x^2}\quad .\\
\end{array}
\]
\end{theorem}
\begin{proof}
\[
\begin{array}{rcl}
\sin(\arcsin x)&=&\displaystyle  x  \quad\quad\left|\frac{\diff}{\diff x}\right. \\
(\arcsin x)'\cos (\arcsin x) &=& 1\\
(\arcsin x)'\sqrt{1-\sin^2(\arcsin x) } &=& 1\\
(\arcsin x)' &=&\displaystyle \frac{1}{\sqrt{1-x^2}}\quad .
\end{array}
\]
\[
\begin{array}{rcl}
\cos(\arccos x)&=&\displaystyle  x \quad\quad\left|\frac{\diff}{\diff x}\right. \\
-(\arccos x)'\sin (\arccos x) &=& 1\\
-(\arccos x)'\sqrt{1-\cos^2(\arccos x) } &=& 1\\
(\arccos x)' &=&-\frac{1}{\sqrt{1-x^2}}\quad .
\end{array}
\]
\[
\begin{array}{rcl}
\tan(\arctan x)&=& x \quad\quad\left|\frac{\diff}{\diff x}\right. \\
-(\arctan x)'\frac{1}{ (\cos{\arctan x})} &=& 1 \quad \quad \quad \mathrm{use~} \frac{1}{\cos^2x}= 1+\tan^2x\\
-(\arctan x)'(1+\tan^2(\arctan x) &=& 1\\
(\arctan x)' &=&\frac{1}{1+x^2}\quad .
\end{array}
\]

\end{proof}


\textbf{Optional material.} (!)Holomorphic functions. We say that a function $f$ is \emph{holomorphic} in a neighborhood of  $z$ if the limit
\[
f'(z)\eqdef \lim_{\substack{h\to 0\\ h\in \mathbb C}} \frac{ f(z+h)-f(z)}{h}
\]
exists.

You may notice that the above looks very similar to the definition that $f$ be differentiable in a neighborhood of $x$. The difference between ``differentiable'' and ``holomorphic'' is that holomorphic functions are required to have limits no matter from which direction in the complex plane does $h$ approach $0$. On the other hand, for a function to be differentiable, we only need to have limit when $h$ approaches $0$ along the real axis (either from the left, negative side, or from the right, positive side). Thus it is ``easier'' for a function to be differentiable, than it is to be holomorphic. All holomorphic functions, when restricted to the reals, are differentiable, but it is easy to construct real functions that are differentiable but cannot be extended to a holomorphic function (one such function is given in problem \ref{probDifferentiableNonAnalyticFunctionExample}).

\textbf{Note.} You are not required to remember this definition and you will not be tested on it. In order to fully understand this definition you must study the subject of complex analysis. Here is the Wikipedia page:

\url{http://en.wikipedia.org/wiki/Complex_analysis} \quad .

\optionalMaterial
\[\begin{array}{l}
\text{The derivatives of holomorphic functions are computed} \\
\text{by the same rules as those for real functions.} \\
\end{array}
\]
This means you can use identities such as $(ix)'=i$. $(e^{ix})'=ie^{ix}$, $(\cos (ix))'= -i\sin(ix)$. Of the functions we studied so far, $e^z, \sin z, \cos z$ are holomoprhic functions everywhere, the functions $\arctan z$, $\log z$ are holomorphic in neighborhoods of the values of $z$ for which they are defined and continuous, and the functions $\Im z, \Re z, |z|$ are not holomorphic.




Definition of hyperbolic sine, hyperbolic cosine and hyperbolic tangent.

Derivative of $\sinh z$, $\cosh z$, $\tanh z$, $\coth z$.

Derivative of $ \arcsinh z$, $ \arccosh z$, $ \arctanh z$.
%\end{comment}
\chapter{Techniques of integration}
\section{Integration by parts}
Integration by parts can be formulated using differential forms as
\begin{equation}\boxed{
\int f \diff g+ \int g \diff f = fg\quad,
}
\end{equation}
or alternatively as
\begin{equation}\boxed{
\int f(x) g'(x)\diff x = fg- \int g(x) f'(x)\diff x \quad .
}
\end{equation}
Combining the above with the fundamental theorem of Calculus yields the definite integral version
\begin{equation}\boxed{
\int\limits_{x=a}^{x=b} f(x) g'(x)\diff x = \left.fg\phantom\int\right|_{x=a}^{x=b} - \int\limits_{x=a}^{x=b} g(x) f'(x)\diff x \quad.
}
\end{equation}
We now discuss when and how to use integration by parts. Suppose we already know the integral of $g(x)$, say $\diff (G(x))=g(x)\diff x$. Suppose for some reason we consider the integral of $f'(x)\diff x$ to be easier than the integral of $f(x)\diff x$. If we want to integrate $f(x)g(x)\diff x= f(x)\diff (G(x))$, then we may use integration by parts to get
\[
\int f(x)\underbrace{g(x)\diff x}_{=\diff (G(x))}= f(x)g(x)- \int G(x) \underbrace{f'(x)\diff x}_{=\diff (f(x))}\quad .
\]
If the function $G(x)$ is not too complicated, then $f'(x)G(x)\diff x$ may be easier to integrate than $f(x)g(x)\diff x$. If that is the case, integrating by parts pays off, and we should use the technique.

There is no 100\% recipe when integration by parts makes the problem easier. However, with practice and experience, on small examples, one can quickly judge when integration by parts helps.
\subsection{Examples and exercises}

\begin{problem}
\input{../../modules/integration-by-parts/homework/integration-by-parts2}
\end{problem}
\input{../../modules/integration-by-parts/homework/integration-by-parts2-solutions}

\begin{problem}
\input{../../modules/integration-by-parts/homework/integration-by-parts}
\end{problem}
\input{../../modules/integration-by-parts/homework/integration-by-parts-solutions}
\begin{problem}
\input{../../modules/integration-by-parts/homework/integrate-x-power-n-e-power-x}
\end{problem}
\input{../../modules/integration-by-parts/homework/integrate-x-power-n-e-power-x-solution}
\section{Integration of rational functions} \label{secIntegrationRationalFunctions}


\subsection{The theory behind integrating rational functions}
Every rational function may be represented as a sum of a polynomial and a proper fraction. A fraction is proper if its numerator is a polynomial of smaller degree than the denominator.

A non-proper function is transformed to a proper one through polynomial
division.

\begin{theorem}(The fundamental theorem of algebra). Every polynomial $D(x)= a_0x^n+a_{1}x^{n-1}+\dots + a_n$ can be factored over the complex numbers into linear terms
\begin{equation}\label{eqFundamentaTheoremAlgebra}
D(x)=a_0(x-x_1)(x-x_2)\dots (x-x_n),
\end{equation}
where $x_1,\dots, x_n$ are the roots of $D(x)$. The roots of $D(x)$ are complex numbers.
If all coefficients $a_i$ of the polynomial $D(x)= a_0x^n+a_{1}x^{n-1}+\dots + a_n$ are real numbers, and $z$ is a non-real root of $D(x)$, then the complex conjugate $\bar z$ of $z$ is also a root of $D(x)$.
\end{theorem}

Therefore the terms  $(x-z)(x-\bar z)$ appear in the factorization \eqref{eqFundamentaTheoremAlgebra}. The terms $(x-z)(x-\bar z)$ may be combined to
\begin{equation}\label{eqCombineConjugateRoots}
(x-z)(x-\bar z)= x^2- (z+\bar z)x + z\bar z = x^{2}+ (\underbrace{-2\Re z}_{c_j})x+ \underbrace{(\Re z)^2+(\Im z)^2}_{d_j}.
\end{equation}
The discriminant of the above quadratic polynomial is negative since it equals $4(\Re z)^2- 4((\Re z)^2+(\Im z)^2)= -4(\Im z)^2<0$.
If all coefficients $a_i$ of the polynomial $D(x)= a_0x^n+a_{1}x^{n-1}+\dots + a_n$ are real numbers, then $D(x)$ can be factored in the form
\begin{equation}\label{eqFactorOverReals}
D(x)= a_0(x-x_1)^{p_1}\dots (x-x_k)^{p_k} (x^2+c_1x+d_1)^{q_1}(x^2+c_2x+d_2)\dots (x^2+c_lx +d_l)^{q_k},
\end{equation}
where $x_1, \dots, x_k$ are real numbers, $c_1, \dots, c_l$ and $d_1,\dots, d_l$ are real numbers, $p_1,\dots, p_k, q_1, \dots, q_l$ are positive integers, and there are no repeating terms. Furthermore $c_j^2-4d_j<0$ and each of the terms $x^2+c_j x+d_j$ is formed by combining two  roots of $D(x)$ as indicated in \eqref{eqCombineConjugateRoots}.

Let $\frac{N(x)}{D(x)}$ be a proper fraction, and let $D(x)$ be factorized as indicated in \eqref{eqFactorOverReals}. Then $\frac{N(x)}{D(x)}$ can be written in the form
\begin{equation}\label{eqSplitPF}
\begin{array}{rcl}
\displaystyle\frac{N(x)}{D(x)}&=&\displaystyle \left (\frac{A_{11}}{x-x_1}+\dots +\frac{A_{1p_1}}{(x-x_1)^{p_1}}\right) +\dots + \left (\frac{A_{k1}}{x-x_k}+\dots +\frac{A_{kp_k}}{(x-x_1)^{p_k}}\right) \\
&&\displaystyle+\left(\frac {B_{11}x+C_{11}}{x^2+c_1x+d_1}+\dots +\frac {B_{1q_1}x+C_{1q_1}}{(x^2+c_1x+d_1)^{q_1}} \right)+\dots
\\&&\displaystyle +
\left(\frac {B_{l1}x+C_{l1}}{x^2+c_lx+d_l}+\dots +\frac {B_{lq_l}x+C_{lq_l}}{(x^2+c_lx+d_l)^{q_l}} \right)\\ \\
&&(\mathrm{in~}\sum\mathrm{~notation}):\\ \\
&=&\displaystyle \sum_{j=1}^{k}\sum_{s=1}^{p_k} \frac{A_{js}}{(x-x_j)^s}+\sum_{j=1}^{l}\sum_{s=1}^{q_j} \frac{B_{js}x+C_{js}}{(x^2+c_jx+d_j)^s}\quad .
\end{array}
\end{equation}
The constants $A_{js}, B_{js}, C_{js}$ are real numbers and may be extracted by clearing the denominators in the above identity and comparing the coefficients in front of $x$.
Using linear substitutions, integrals of the form
\[
\displaystyle\int \frac{A}{(x-x_j)^s}\diff x,
\]
and
\[
\int \frac{Bx+C}{(x^2+c_jx+d_j)^s}\diff x
\]
can be transformed to combinations of integrals of the forms:
\[
\int \frac{1}{y^n}\diff y,
\]
\[
\int \frac{1}{(y^2+1)^n}\diff y,
\]
\[\int \frac{y}{(y^2+1)^n}\diff y.
\]
Each of these ``building block'' integrals has different behavior for $n=1$ and for $n\geq 2$. We solve each of the ``building block'' integrals below.

\subsubsection{Integrals of the form $\displaystyle\int \frac{1}{(x+a)^n}\diff x$.}
For $n=1$ we get
\begin{equation}\label{eqBuildingBlockInt1base}
\int \frac{1}{x}\diff x=\ln |x| +C\quad .
\end{equation}
Let $n> 1$. Then
\begin{equation}\label{eqBuildingBlockInt1N}
\int \frac{1}{x^n}\diff x=\int x^{-n}\diff x= -\frac{1}{(n-1)x^{n-1}}+C \quad .
\end{equation}
\subsubsection{Integrals of the form $\displaystyle\int \frac{x}{(1+x ^2)^n} \diff x $.}
If we use the substitution $y=x^2+1$, we get $\diff y=2x\diff x $ and so $\displaystyle \int \frac{x}{(1+x ^2)^n} \diff x = \displaystyle\int \frac{\frac12 \diff y}{y^n} $, i.e., our integral is transformed to an integral of the form \eqref{eqBuildingBlockInt1base} or  \eqref{eqBuildingBlockInt1N}. In other words, for $n=1$ we have
\[
\int \frac{x}{x^2+1}\diff x= \int \frac{\frac{1}2 \diff\left(x^2+1\right)}{x^2+1}= \frac{1}{2}\ln \left(x^2+1\right) +C\quad .
\]
\subsubsection {Integrals of the form $\displaystyle\int \frac{1}{(1+x ^2)^n} \diff x $.}
Set 
\begin{equation}\label{eqBuildingBlock3N}
J(n)=\int \frac{1}{(1+x ^2)^n}\diff x\quad .
\end{equation}
We are looking for a formula for $J(n)$. For $n=1$ we can recall that the derivative of $\Arctan x$ is $\displaystyle\frac{1}{1+x^2}$, in other words
\[
J(1)=\int \frac{1}{x^2+1}\diff x= \Arctan x+C\quad .
\]
For $n>1, n\in \mathbb Z$, finding a formula for $J(n)$ is considerably more challenging than the preceding ``building blocks''. There is a trick to solving \eqref{eqBuildingBlock3N}: we integrate $\displaystyle J(n-1)= \int \frac{1}{(1+x ^2)^{n-1}}\diff x$ by parts:

$
\begin{array}{r@{~}c@{~}l@{}l@{}|l}
J(n-1)&=&\displaystyle \int \frac{1}{(1+x ^2)^{n-1}}\diff x &&\text{int. by parts}\\
&=&\displaystyle \frac{x}{(1+x ^2)^{n-1}}-\int x \diff \left(\frac{1}{(1+x ^2)^{ n - 1 } } \right)\\
&=&\displaystyle\frac{x}{(1+x ^2)^{n-1}}- \int 2(-n+1)\frac{x^2}{(1+x^2)^n}\diff x\\
&=&\displaystyle\frac{x}{(1+x ^2)^{n-1}}+2(n-1) \int \frac{(1+x^2-1)}{(1+x^2)^n}\diff x\\
&=&\displaystyle\frac{x}{(1+x ^2)^{n-1}} +2(n-1)\int \frac{1}{(1+x^2)^{n-1}}\diff x \\&&\displaystyle -2(n-1) \int \frac{1}{(1+x^2)^n}\diff x\\
&=&\displaystyle  \frac{x}{(1+x ^2)^{n-1}} +2(n-1)J(n-1)-2(n-1)J(n).
\end{array}
$

\noindent Transfer the term involving $J(n)$ to the left hand side, and the term involving $J(n-1)$ to the right hand side to obtain that
\[
\begin{array}{rcl}
\displaystyle (2n-2)J(n)&=&\displaystyle \frac{x}{(1+x ^2)^{n-1}}+ (2n-3)J(n-1)\\
\displaystyle J(n)&=&\displaystyle \frac{x}{(2n-2)(1+x ^2)^{n-1}}+ \frac{2n-3}{2n-2}J(n-1)
\end{array}
\]
In this way we expressed the integral $J(n)$ using a simpler one, $J(n-1)$. In a similar fashion, we may express $J(n-1)$ via $J(n-2)$, and so on, until we reach $\displaystyle J(1)= \int \frac{1}{(1+x^2)}\diff x=\arctan x +C$. Our answer may be summarized as
\[
\begin{array}{rcl}
\displaystyle \int \frac{1}{(1+x^2)^n}\diff x&=& \displaystyle  \frac{x}{(2n-2)(1+x ^2)^{ n-1}}+\frac{2n-3}{2n-2} J(n-1)\\
&=&\displaystyle \frac{x}{(2n-2)(1+x ^2)^{n-1}}\\
&&\displaystyle +\frac{2n-3}{2n-2} \left(\frac{x}{(2n -4)(1 +x^2 )^{n -2}} +\frac{ 2n-5 }{2n-4} J(n-3) \right)\\
&=&\dots\\ \\
&&\mathrm{using~}\Sigma \mathrm{~notation:}\\ \\
&=&\displaystyle \frac{(2n-3 )(2n-5)\dots 1}{(2n-2)(2n-4)\dots 2}\arctan x\\
&&\displaystyle + 
\frac{1}{(2n-2)}\frac{x}{ \left(1+x^2\right)^{n-1}}\\
&&\displaystyle + 
\sum_{s=2}^{n} \frac{(2n-3) }{(2n-2)}\dots \frac{(2n-2s+1)}{(2n-2s+2)} \frac{1}{(2n-2s)}\frac{x}{ \left(1+x^2\right)^{n-s}}
\quad .
\end{array}
\]

\subsection{Algorithm for integrating rational functions}
In this section we summarize the algorithm for integrating rational functions $\int \frac{N(x)}{D(x)}\diff x$.
\begin{itemize}
\item If the degree of $N(x)$ is greater than the degree of $D(x)$, use polynomial division with remainder to transform to proper fraction. In other words, by the algorithm of long division, obtain $Q(x)$ and $R(x)$ such that $N(x)= Q(x)D(x)+R(x)$ and the degree of $R(x)$ is smaller than the degree of $D(x)$. Then use
\[
\frac{N(x)}{D(x)}=Q(x) +\frac{R(x)}{D(x)}\quad .
\]
\item Factor $D(x)$, i.e., produce the decomposition \eqref{eqFactorOverReals}. In many problems, $D(x)$ will be presented to you in already factored form.
\item Split $\frac{R(x)}{D(x)}$ into partial fractions, i.e., write it in the form \eqref{eqSplitPF}. To do so one needs to do the following.
\begin{itemize}
\item[Step 1] Clear denominators on both sides in equation \eqref{eqSplitPF}.
\item[Step 2] Uncover parenthesis and group all terms according to powers of $x$. 
\item[Step 3] Form a system of equations by comparing the coefficients of $x$ on both sides of the resulting equality.
\item[Step 4] The resulting system is linear; solve using standard methods.
\item \textbf{Remark.} Shortcuts to Steps 2-4 can be obtained by substituting well-chosen values for $x$. We will demonstrate the technique on examples.
\end{itemize}
\item Using linear substitutions, transform each partial fraction into a multiple of one of the ``building block'' integrals $\displaystyle \int \frac{1}{(1+y^2)^n}\diff y$, $\displaystyle \int \frac{y}{(1+y^2)^n}\diff y$, $\displaystyle \int \frac{1}{(y+C)^n}\diff y$, $n\geq 2$.
\item Solve the ``building blocks'' integrals as we did earlier in this section.
\item Collect the ``building blocks'' to a single answer.
\item Check your answer with one of the following techniques.
\begin{itemize}
\item Differentiate your answer (by hand or by computer algebra system) and check that you get the original integrand.
\item Use a computer algebra system to get an answer and compare.
\end{itemize}
On a timed exam, carry out this step only if done with the other problems.
\end{itemize}
\optionalMaterial A few notes on factoring polynomials are in order.
\begin{itemize}
\item There are algorithms for factoring a polynomial $D(x)$ over the rational numbers.
\item There is a ``closed form'' formula for finding the roots of an arbitrary polynomial of degree 4 or less using radicals and the arithmetic operations. Here, ``closed form'' formula means it is written with finitely many radicals (second, third, \dots roots), addition, subtraction, division and multiplication.
\item A polynomial of degree 5 will in general have roots  that cannot be written using finitely many arithmetic and radical operations. For concrete polynomials, there may be exceptions. All examples in the present textbook will fall under this exception.
\item One can ``solve'' an arbitrary polynomial equation of degree 5 or more by allowing operations involving additional (transcendental) functions. These functions may be described, for example, using infinite power series, just as we will later come to describe the radical function $\sqrt{1+z}$ (see \eqref{eqNewtonBinomialGeneralized}). However, studying the properties of these transcendental functions - for example, whether one such function may be used for two different polynomials or how to systematically implement them using computer algebra systems - are still an area of active mathematical research. The branch of mathematics that deals with the question of solving polynomial equations is called ``Galois Theory''.
\item The present textbook does not cover the material alluded to in the preceding item. Therefore we will be able to perform the algorithm for integration of rational functions only when the denominator can be (easily) factored into radicals. We assure the reader that all examples in this textbook were selected to be doable by hand by the average student. 
\end{itemize}
\subsection{Examples and exercises}
\subsubsection{Linear Substitutions leading to Building block integrals}
\begin{problem}
\input{../../modules/partial-fractions/homework/integration-rational-functions-building-blocks}
\end{problem}
\input{../../modules/partial-fractions/homework/integration-rational-functions-building-blocks-solutions}

\begin{problem}
\input{../../modules/partial-fractions/homework/integration-rational-functions-building-block-II-and-III-parametric}
\end{problem}
\input{../../modules/partial-fractions/homework/integration-rational-functions-building-block-II-and-III-parametric-solution}


\begin{problem}
\input{../../modules/partial-fractions/homework/integration-rational-functions-building-block-III-b-parametric}
\end{problem}
\input{../../modules/partial-fractions/homework/integration-rational-functions-building-block-III-b-parametric-solution}
\subsubsection{Partial fractions: Quadratic term in the denominator}
\begin{problem}
\input{../../modules/partial-fractions/homework/integration-rational-functions-quadratics-in-denominator-1}
\end{problem}
\input{../../modules/partial-fractions/homework/integration-rational-functions-quadratics-in-denominator-1-solutions}
\subsubsection{Partial fractions: complete algorithm}
\begin{problem}
\input{../../modules/partial-fractions/homework/integration-rational-functions-complete-1}
\end{problem}
\input{../../modules/partial-fractions/homework/integration-rational-functions-complete-1-solutions}

\subsubsection{A large example illustrating the full algorithm}
\begin{problem}
\input{../../modules/partial-fractions/homework/integration-rational-functions-complete-large-example-1}
\end{problem}
\input{../../modules/partial-fractions/homework/integration-rational-functions-complete-large-example-1-solution}


\section{Trigonometric integrals}\label{secTrigIntegrals}
\subsection{\optionalMaterial General method for solving rational trigonometric integrals}\label{secTrigIntegralsWith2Arctan}
Let $R$ be a rational function in two variables. Integrals of the form
\[
\int R(\sin \theta,\cos \theta )\diff \theta
\]
can be transformed to integrals of rational functions via the substitution $\theta= 2\arctan t$. To see how $\cos \theta$ and $\sin \theta$ are expressed via $t$ we need a little algebra. First we recall the formulas
\begin{equation}\label{eqsin2zViaTanzcos2zViaTanz}
\begin{array}{rcll|l}
\cos (2z)&=& \displaystyle \cos^2 z- \sin^{2}z\\
&=&\displaystyle \frac{\cos^{2}z- \sin^{2}z }{ \cos^2z + \sin^2 z}\\
&=&\displaystyle \frac{\left(\cos^{2}z- \sin^{2 } z\right) \frac{1}{\cos^2 z}}{ \left( \cos^2z+\sin^2z \right) \frac{1}{ \cos^2 z}}\\
&=&\displaystyle \frac{1-\tan^2 z}{1+\tan^2z}\quad ,\\~\\
\sin(2z)&=& 2\sin z \cos z\\
&=&\displaystyle \frac{2\sin z \cos z}{\cos^2z+\sin^2z}\\
&=&\displaystyle \frac{2\sin z \cos z}{\left(\cos^2z+\sin^2z\right)} \frac{\frac{ 1}{\cos^2z}}{ \frac{ 1}{\cos^2z}}\\
&=&\displaystyle \frac{2\tan z}{1+\tan^2z}
\end{array}
\end{equation}
We are ready to express $\cos \theta$ and $\sin \theta$ via $t$:
\[
\begin{array}{rcll|l}
\cos \theta&=& \cos \left(2\arctan t\right)&&\text{use formula \eqref{eqsin2zViaTanzcos2zViaTanz}}\\
&=&\displaystyle \frac{1- \tan^2(\arctan t)}{ 1 + \tan^2( \arctan t)}\\
&=&\displaystyle  \frac{1- t^2}{1+t^2}\quad ,\\~\\
\sin \theta&=& \sin \left(2\arctan t\right)&&\text{use formula \eqref{eqsin2zViaTanzcos2zViaTanz}}\\
&=&\displaystyle \frac{2\tan(\arctan t)}{1+\tan^2(\arctan t)}\\
&=&\displaystyle  \frac{2t}{1+t^2}\quad .
\end{array}
\]
The differential $\diff \theta$ can expressed via $\diff t$ from $\theta=2\arctan t$. The substitution $\theta=2\arctan t$ can be summarized as:
\begin{equation}\label{eqTrigIntegralsWeistrassSub}
\begin{array}{rcl}
\cos \theta&=&\displaystyle  \frac{1- t^2}{1+t^2} \\
\sin \theta&=&\displaystyle \frac{2t}{1+t^2}\\
\diff \theta&=&\displaystyle \frac{2}{t^2+1}\diff t \\
t&=&\displaystyle \tan \left(\frac {\theta}{2}\right)
\end{array}
\end{equation}
\subsubsection{\optionalMaterial Examples and exercises}
\begin{problem}
\input{../../modules/trig-integrals/homework/rationalizing-substitution-1}
\end{problem}

\input{../../modules/trig-integrals/homework/rationalizing-substitution-1-solutions}



\subsection{\optionalMaterial Method II for solving rational trigonometric integrals}
\textbf{To be written.}


\subsection{Trigonometric integrals: quick ad hoc methods}

\section{Integrals of the form $\displaystyle \int R(x, \sqrt{ax^2+bx+c}) \diff x$}
Let $R$ be a rational function in two variables. We recall that a rational function in two variables is simply a quotient of two polynomials in two variables. An example of a rational function in two variables would be $R(z,w)= \frac{zw+1}{z+w}$.

Let $ay^2+by+c$, $a\neq 0$ be a quadratic that is not a perfect square and that takes on positive values for some real $y$. Then $\sqrt{ay^2+by+c})$ can be transformed using linear substitutions (by ``completing the square'') to a multiple of one of the following radicals: $\sqrt{x^2+1}$, $\sqrt{-x^2+1}$,  $\sqrt{x^2-1}$. In Section \ref{secEulerIntegralsAndLinearSubstitutions} we shall exercise such linear substitutions to transform our integral to one of the forms
\begin{equation}\label{eqIntegralQuadraticRadicalX}
\int R(x, \sqrt{x^2+1})\diff x,\qquad  \int R(x, \sqrt{-x^2+1})\diff x, \qquad  \int R(x, \sqrt{x^2-1})\diff x\quad .
\end{equation}

The above three integrals can transformed to trigonometric rational integrals using the trigonometric substitutions from the table below. These substitutions are called \emph{trigonometric  substitutions}. In turn, in Section \ref{secTrigIntegrals} we studied that trigonometric integrals are transformed to rational integrals via the substitution $\theta=2\arctan t$. If we compose the  trigonometric substitution with $\theta=2\arctan t$ we get a substitution which directly transforms integrals of the form \eqref{eqIntegralQuadraticRadicalX} to integrals of rational functions. The so obtained rationalizing substitutions are called \emph{Euler substitutions}, and we list them below the trigonometric substitutions in the order in which they are obtained.

\noindent\begin{tabular}{|l|l|l|l|}
\hline
Expression & Substitution& Variable range & Relevant identity\\\hline
\multirow{2}{*}{$\sqrt{x^2+1}$} & $x = \tan \theta$ &  $ \theta\in \left(-\frac{\pi}{2} , \frac{\pi}{2}\right)$ & $1 + \tan^2 \theta = \sec^2 \theta$\\
&$x=\cot \theta$ &$ \theta\in (0, \pi) $ & $1+\cot^2\theta =\csc^2\theta $ \\ \hline
\multirow{2}{*}{ $\sqrt{-x^2+1 }$} & $x = \sin \theta$ &  $ \theta\in \left[ -\frac{\pi}{2} ,\frac{\pi}{2}\right]$ & $1 - \sin^2 \theta = \cos^2 \theta$\\
& $x = \cos \theta$ & $\theta\in (0,\pi)$& $1-\cos^2\theta=\cos^2\theta$ \\\hline
\multirow{2}{*}{$\sqrt{x^2-1}$} &$x=\csc \theta$ &$\theta\in \left[0, \frac{\pi}{2} \right) \cup \left[ \pi, \frac{3\pi}{2}\right)$ &  $\csc^2\theta-1=\cot^2\theta $\\
& $x = \sec \theta$ &
$\theta\in \left[0, \frac{\pi}{2}\right)\cup \left[\pi, \frac{ 3 \pi}{2}\right)$
& $\sec^2\theta - 1 = \tan^2\theta$\\
\hline
\multicolumn{4}{c}{Euler substitution by applying in addition $\theta=2\arctan t$}\\
\hline
Expression & Substitution  & Variable range & Relevant identity\\
\hline
\multirow{2}{*}{$\sqrt{x^2+1}$} & $ x =\frac{2t}{1-t^2}$ & $-1< t< 1$ & see \eqref{eqsqrtx2plus1Euler1}\\
&$ x=\frac{1}{2} \left(\frac{1}{t}-t\right)$ & $0<t $ & see \eqref{eqsqrtx2plus1Euler2}  \\ \hline
\multirow{2}{*}{ $\sqrt{-x^2+1 }$} & $x=\frac{2t}{1+t^2} $ & $-1\leq t\leq 1 $ & see \eqref{eqsqrt1minusxsquaredE1} \\
& $x =\frac{1-t^2}{1+t^2} $ & $0<t$& see \eqref{eqsqrt1minusxsquaredE2}  \\\hline
\multirow{2}{*}{$\sqrt{x^2-1}$} &$x=\frac{1}{2}\left(\frac{ 1}{t}+t\right)$ & $t\in (-\infty, -1)\cup [0,1)$& see \eqref{eqsqrtxsquareminus1E1}\\
& $x =\frac{1+t^2}{1-t^2} $ & $t \in (-\infty,-1)\cup [0,1)$ &see \eqref{eqsqrtxsquareminus1E2} \\\hline
\end{tabular}

To the student we note that to learn the material in this section it suffices to memorize the three pairs of trigonometric substitutions $x=\tan \theta, x= \cot \theta$; $x=\sin \theta, x= \cos \theta$; $x=\sec \theta, x=\csc \theta$, and to memorize the substitution $\theta=2\arctan t$. In other words, one only needs to memorize the first and second column of the first three rows in the table above. Indeed, the formulas in the fourth column appear naturally while simplifying radical expression from the first column. In turn, the third column is a natural consequence of the other three: let us explain why. Indeed, while the entries of the third column may be chosen in another way without significant changes in the theory, among all possible choices, the one above is arguably the simplest (``most natural'') one. For example, for the first substitution, $x=\tan \theta$, we have that
\[
\sqrt{x^2+1}= \sqrt{ 1+\tan^\theta}= \sqrt{\frac{1}{\cos^2 \theta}}=\pm \sec \theta\quad .
\]
It is then most natural to select the range for $\theta$ so that the sign choice in the above equality is $+$. Among all ranges with that property, the range $-\frac{\pi }{2}< \theta < \frac{\pi }{2}$ is the only one that is symmetric around $0$ and consists of a single interval of length $\pi$ (i.e., is as large as possible). We leave the considerations for choice of the variable ranges in the remaining rows of the table to the reader.

Finally we note that the Euler substitutions - the last three rows of the table - are a consequence of the trigonometric substitutions and need not be memorized. We shall show in full detail why that is so by the end of the present section.

We start the technical part of this section by exercising the familiar procedure of completing the square to transform integrals of the form $\int R(x,\sqrt{ax^2+bx+c}) \diff x$ to integrals of the forms $\int R(x, \sqrt{x^2+1})\diff x$,  $\int R(x, \sqrt{-x^2+1})\diff x$, $\int R(x, \sqrt{x^2-1})\diff x$.
\subsection[Transforming radicals of quadratics to basic form]{Examples and Exercises: Linear substitutions transforming $\displaystyle \sqrt{ax^2+bx+c}$ to $\displaystyle \sqrt{u^2+1}, \sqrt{-u^2 +1}, \sqrt{u^2-1} $} \label{secEulerIntegralsAndLinearSubstitutions}
Suppose we have a quadratic that is not a perfect square and is not strictly negative. In the present section we demonstrate how to transform the radical of such a quadratic to a multiple of one of the three forms $\sqrt{u^2+1}$, $\sqrt{-u^2 +1}$, $\sqrt{u^2-1} $. The transformation consists in completing the square, which was already exercised in Section \ref{secIntegrationRationalFunctions}. A student confident in this technique may skip this section.

Due to the elementary nature of this section we skip all theory and solve a few examples instead.
\subsubsection{Examples and Exercises}

\begin{problem}
\input{../../modules/trig-substitution/homework/prepare-radical-of-quadratic-for-integration-complete-square}
\end{problem}
\input{../../modules/trig-substitution/homework/prepare-radical-of-quadratic-for-integration-complete-square-solutions}

\subsection{Integrals of the form $\displaystyle \int R(x,\sqrt{x^2+1})\diff x$}
We recall that in this section $R$ stands for a rational function in two variables. The two trigonometric substitution which transform
\[
\int R(x, \sqrt{x^2+1})\diff x
\]
to integral of rational function are $x=\tan \theta$ and $x=\cot \theta$. We study both in the following two sections. Students interested in practical applications may want to read only one of the sections, say, Section \ref{secTrigSubtanTheta} (and its subsection \ref{secTrigSubtan(2arctant)}). Students interested in a deeper understanding of the material may want to read one of the sections, say,  Section \ref{secTrigSubtanTheta} (and its subsection \ref{secTrigSubtan(2arctant)}), and then derive the other Section \ref{secTrigSubcotTheta} on their own, and finally compare their work with the exposition in Section \ref{secTrigSubcotTheta}.
\subsubsection{The trigonometric substitution $x=\tan \theta$, $\displaystyle \theta\in \left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$ }\label{secTrigSubtanTheta}
\input{../../modules/trig-substitution/homework/trig-and-euler-substitution-theory-case1-tan-exposition-part-1}

\subsubsection{The Euler substitution $x=\tan \theta$, $\theta=2\arctan t$, $-1<  t< 1$} \label{secTrigSubtan(2arctant)}
\input{../../modules/trig-substitution/homework/trig-and-euler-substitution-theory-case1-tan-exposition-part-2}


As demonstrated in the present section, it is sufficient to memorize the substitution $x=\tan (2\arctan t)$ in order to derive the equalities above. An alternative way to memorize the Euler substitution is through the equality
\[
\sqrt{x^2+1}=\frac{x}{t}-1\quad .
\]
\begin{problem} 
\input{../../modules/trig-substitution/homework/euler-substitution-theory-alternative-case1-tan}
\end{problem}


\subsubsection{The trigonometric substitution $x=\cot \theta$, $\theta \in \left(0, \pi\right)$}\label{secTrigSubcotTheta}
We challenge the interested student who has read Section \ref{secTrigSubtanTheta} to first try to derive the trigonometric substitution $x=\cot \theta$ and the corresponding Euler substitution $x=\cot (2\arctan t)$  on her or his own, and only then to read the rest of this section.

\input{../../modules/trig-substitution/homework/trig-and-euler-substitution-theory-case1-cot-exposition-part-1}



\subsubsection{The Euler substitution $x=\cot \theta$, $\theta=2\arctan t$, $t>0$}
\input{../../modules/trig-substitution/homework/trig-and-euler-substitution-theory-case1-cot-exposition-part-2}



As demonstrated in the present section, it is sufficient to memorize the substitution $x=\cot (2\arctan t)$ in order to derive the equalities above. An alternative way to memorize the Euler substitution is through the equality
\[
\sqrt{x^2+1}=x+t\quad .
\]
\begin{problem} 
\input{../../modules/trig-substitution/homework/euler-substitution-theory-alternative-case1-cot}
\end{problem}
\input{../../modules/trig-substitution/homework/euler-substitution-theory-alternative-case1-cot-solution}

\subsubsection{Summary of the case $\sqrt{x^2+1}$}
Our considerations so far are summarized in the following theorem. We recall that in the present section $R$ stands for a rational function of two variables. 
\begin{theorem}
The substitutions \eqref{eqEulerSub-case1-tan(2arctant)} and \eqref{eqEulerSub-case1-cot(2arctant)} both transform all integrals of the form $ \int R(x, \sqrt{ x^2+1})\diff x$ to integrals of rational functions.
\end{theorem}
We note that the Euler substitution \eqref{eqEulerSub-case1-cot(2arctant)} corresponding to $x=\cot \theta$ appears to lead to a slightly more convenient for a human algebra; we may show a slight preference for \eqref{eqEulerSub-case1-cot(2arctant)}  over \eqref{eqEulerSub-case1-tan(2arctant)}. This is of course mostly a matter of taste, as both substitutions ultimately lead to the same indefinite integrals (up to addition of a constant).
\subsubsection{Examples and Exercises}

\begin{problem}
\input{../../modules/trig-substitution/homework/integration-euler-substitution-case1}
\end{problem}
\input{../../modules/trig-substitution/homework/integration-euler-substitution-case1-solutions}

\begin{problem}
\input{../../modules/trig-substitution/homework/integration-euler-substitution-case1-integral-radical-quadratic}
\end{problem}


\subsection{Integrals of the form $\displaystyle \int R(x,\sqrt{-x^2+1})\diff x$}
We recall that in this section $R$ stands for a rational function in two variables. The two trigonometric substitution which transform
\[
\int R(x, \sqrt{-x^2+1})\diff x
\]
to integral of rational function are $x=\sin \theta$ and $x=\cos \theta$.

\subsubsection{The trigonometric substitution $x=\sin \theta$, $\theta \in \left[-\frac{\pi}{2},-\frac{\pi}{2} \right]$}
We challenge the interested student who has read Section \ref{secTrigSubtanTheta} to first try to derive the trigonometric substitution $x=\sin \theta$ and the corresponding Euler substitution $x=\sin (2 \arctan t)$  on her or his own, and only then to read the rest of this section.


\input{../../modules/trig-substitution/homework/trig-and-euler-substitution-theory-case1-sin-exposition-part-1}

\subsubsection{The Euler substitution $x=\sin \theta$, $\theta=2\arctan t$ , $-1\leq t\leq 1$}
\input{../../modules/trig-substitution/homework/trig-and-euler-substitution-theory-case1-sin-exposition-part-2}




As demonstrated in the present section, it is sufficient to memorize the substitution $x=\sin (2\arctan t)$ in order to derive the equalities above. An alternative way to memorize the Euler substitution is through the equality
\[
\sqrt{-x^2+1}=1-xt\quad .
\]
\begin{problem}
\input{../../modules/trig-substitution/homework/euler-substitution-theory-alternative-case1-sin}
\end{problem}

\subsubsection{The trigonometric substitution $x=\cos \theta$, $\theta\in\left[0, \pi\right] $}
We challenge the interested student who has read Section \ref{secTrigSubtanTheta} to first try to derive the trigonometric substitution $x=\cos \theta$ and the corresponding Euler substitution $x=\cos (2\arctan t)$  on her or his own, and only then to read the rest of this section.

\input{../../modules/trig-substitution/homework/trig-and-euler-substitution-theory-case1-cos-exposition-part-1}

\subsubsection{The Euler substitution $x=\cos \theta$, $\theta=2\arctan t$, $ t\geq 0 $}
\input{../../modules/trig-substitution/homework/trig-and-euler-substitution-theory-case1-cos-exposition-part-2}

As demonstrated in the present section, it is sufficient to memorize the substitution $x=\sin (2\arctan t)$ in order to derive the equalities above. An alternative way to memorize the Euler substitution is through the equality
\[
\sqrt{-x^2+1}=(1-x)t\quad .
\]
\begin{problem}
\input{../../modules/trig-substitution/homework/euler-substitution-theory-alternative-case1-cos}
\end{problem}
\input{../../modules/trig-substitution/homework/euler-substitution-theory-alternative-case1-cos-solution}

\subsection{Integrals of the form $\displaystyle \int R(x, \sqrt{x^2 -1}) \diff x$}
We recall that in this section $R$ stands for a rational function in two variables. The two trigonometric substitution which transform
\[
\int R(x, \sqrt{x^2-1})\diff x
\]
to integral of rational function are $x=\sec \theta$ and $x=\csc \theta$.

\subsubsection{The trigonometric substitution $x=\csc \theta$, $\theta \in \left[0, \frac{\pi}{2}\right)\cup \left[\pi, \frac{3\pi}{2}\right)$}
We challenge the interested student who has read Section \ref{secTrigSubtanTheta} to first try to derive the trigonometric substitution $x=\csc \theta$ and the corresponding Euler substitution $x=\csc (2\arctan t)$  on her or his own, and only then to read the rest of this section.

\input{../../modules/trig-substitution/homework/trig-and-euler-substitution-theory-case1-csc-exposition-part-1}



\subsubsection{The Euler substitution $x=\csc \theta$, $\theta=2\arctan t$, $ t\in (-\infty, -1) \cup \left[0, 1 \right) $}

\input{../../modules/trig-substitution/homework/trig-and-euler-substitution-theory-case1-csc-exposition-part-2}


As demonstrated in the present section, it is sufficient to memorize the substitution $x=\csc (2\arctan t)$ in order to derive the equalities above. An alternative way to memorize the Euler substitution is through the equality
\[
\sqrt{x^2-1}=\frac{1}{t}-x\quad .
\]
\begin{problem}
\input{../../modules/trig-substitution/homework/euler-substitution-theory-alternative-case1-csc}
\end{problem}
\subsubsection{The trigonometric substitution $x=\sec \theta$, $\theta \in \left[0, \frac{\pi}{2}\right)\cup \left[\pi, \frac{3\pi}{2}\right) $}
We challenge the interested student who has read Section \ref{secTrigSubtanTheta} to first try to derive the trigonometric substitution $x=\sec \theta$ and the corresponding Euler substitution $x=\sec (2\arctan t)$  on her or his own, and only then to read the rest of this section.

\input{../../modules/trig-substitution/homework/trig-and-euler-substitution-theory-case1-sec-exposition-part-1}

\subsubsection{The Euler substitution $x=\sec \theta$, $\theta = 2\arctan t$, $ t\in (-\infty, -1) \cup \left[0, 1 \right) $}

\input{../../modules/trig-substitution/homework/trig-and-euler-substitution-theory-case1-sec-exposition-part-2}


As demonstrated in the present section, it is sufficient to memorize the substitution $x=\sec (2\arctan t)$ in order to derive the equalities above. An alternative way to memorize the Euler substitution is through the equality
\[
\begin{array}{rcl}
\sqrt{x^2-1}&=&(x+1)t\quad .
\end{array}
\]
\begin{problem}
\input{../../modules/trig-substitution/homework/euler-substitution-theory-alternative-case1-sec}
\end{problem}
\input{../../modules/trig-substitution/homework/euler-substitution-theory-alternative-case1-sec-solution}



\subsection{Examples and Exercises}
\subsubsection{Trig or Euler substitution, solutions use trig substitution}

\begin{problem}
\input{../../modules/trig-substitution/homework/trig-substitution-sin-cos-1}
\end{problem}
\input{../../modules/trig-substitution/homework/trig-substitution-sin-cos-1-solutions}

\subsubsection{Trig or Euler substitution, solutions use Euler substitution}
\begin{problem}
\input{../../modules/trig-substitution/homework/integration-euler-substitution-case1}
\end{problem}
\input{../../modules/trig-substitution/homework/integration-euler-substitution-case1-solutions}

\begin{problem}
\input{../../modules/trig-substitution/homework/integration-euler-substitution-case1-integral-radical-quadratic}
\end{problem}

\begin{problem}
\input{../../modules/trig-substitution/homework/integration-euler-substitution-case2}
\end{problem}
\input{../../modules/trig-substitution/homework/integration-euler-substitution-case2-solutions}


\begin{problem}
\input{../../modules/trig-substitution/homework/integration-euler-substitution-case3}
\end{problem}

\section{Examples and Exercises}
\begin{problem}~
\begin{enumerate}
\item
\[
\int \frac{1}{\sqrt{x^2+1}}\diff x\quad .
\]
Carry out the Euler substitution: $x=\frac12\left(\frac{1}{t}- t\right)$. You do not need to solve the integral.
\item
\[
\int \frac{1}{\sqrt{x^2+1}}\diff x\quad .
\]
Carry out the substitution: $x=\frac12\left(e^s- e^{-s}\right)$. You do not need to solve the integral.
\item Solve the integral
\[
\int \frac{1}{\sqrt{x^2+1}}\diff x\quad .
\]
\end{enumerate}
\end{problem}



\chapter{Sequences, limits, formal power series}\label{secSequencesPowerSeries}
\section{Sequences, limits of sequences}
\begin{definition}
\index{sequence} A sequence is an infinite list of numbers
\[
a_1, a_2, a_3, \dots\quad .
\]
\end{definition}
As usual, we may choose letters other than the letter $a$ to denote a sequence. The number $a_n$ is called the $n^{th}$ term of the sequence. A sequence can be thought of as a function from the non-negative integers to the real numbers $a: \mathbb Z_{>0}\to \mathbb R$. The term $a_n$ can be thought of as the value $a(n)$ of the function $a$ at $n$. A similar definition can be given for sequences of complex numbers.

In our lectures, in the notation $a_n$, we will informally call $n$ the index of $a_n$. We are allowed to relabel sequences to start at an index other than 1.

A sequence starting with index $1$ is usually denoted as $\{a_n\}$ or, to be more specific, $\{a_n\}_{n=1}^\infty$. A sequence starting at $n=3$ would be denoted as $\{a_n\}_{n=3}^\infty$; a sequence starting at $-1$ would be denoted as $\{a_n\}_{n=-1}^\infty$; and so on.

Sequences can be given by directly producing a formula for the $n^{th}$ term, for example $a_n:=\frac{1}{n}$.

Sequences can be also given recursively, for example $a_1:=1$, $a_2:=1$, $a_n:=a_{n-1}+a_{n-2}$. Another example would be $a_1:=1$, $a_{n+1}= \frac{a_n}{2}+\frac{N}{2a_n}$ for any $N>0$.

We say that a sequence $\{a_n\}$ has limit $L$ and we write
\[
\lim a_n\to L
\]
if the terms $a_n$ get arbitrarily close to $L$ as $n$ increases. More precisely,

\importantText{
\begin{tabular}{l}
$\{a_n\}$ has limit $L$ if for every $\varepsilon>0$ we can choose an index $N$ \\
depending on $\varepsilon$, such that $|a_n-L|<\varepsilon$ whenever $n>N$.
\end{tabular}
}

\index{convergent!sequence}\index{divergent!sequence} If a sequence $\{a_n\}$ has limit, we say it \emph{converges} (or is \emph{convergent}), else we say it \emph{diverges} (or is \emph{divergent}).

If $\lim\limits_{x\to \infty} f(x)=L $, then the sequence $\{f(n)\}$ converges to $L$.

We say that
\[
\lim \limits_{n\to \infty} a_n=+\infty
\]
if for every positive number $M$ there is an index $N$ depending on $M$ such that $a_n>M$ whenever $n>N$.

Note that even though we write $\lim \limits_{n\to \infty} a_n=+\infty$, the sequence $\{a_n\}$ is divergent (this terminology is rather unfortunate, but can be justified as preferable when studying a field of mathematics called ``Topology'').

If $\{a_n\}$ and $\{b_n\}$ are convergent sequences, and $c$ is constant, then
\begin{equation}\boxed{
\begin{array}{rcl}
\lim \limits_{n\to \infty} (a_n+b_n)&=&\lim \limits_{n\to \infty} a_n+ \lim \limits_{n\to \infty} b_n\\
\lim \limits_{n\to \infty} (a_n-b_n)&=&\lim \limits_{n\to \infty} a_n- \lim \limits_{n\to \infty} b_n\\
\lim \limits_{n\to \infty} (ca_n)&=&c\lim \limits_{n\to \infty} a_n\\
\lim \limits_{n\to \infty} (a_nb_n)&=&\left(\lim \limits_{n\to \infty} a_n\right) \left(\lim \limits_{n\to \infty} b_n\right)\\
\lim \frac{a_n}{b_n}&=&\frac{\lim \limits_{n\to \infty} a_n}{\lim \limits_{n\to \infty} b_n} \quad\quad\quad \mathrm{if~}\lim \limits_{n\to \infty} b_n\neq 0 \mathrm{~and~} b_n\neq 0\mathrm{~for~all~}n\quad .
\end{array}
}
\end{equation}
\importantText{
If $\lim\limits_{n\to \infty} a_n=L$ and $f$ is a function that is continuous at $L$, then
$\lim \limits_{n\to \infty} f(a_n)=f(L)$.
}

The sequence $\{r^n\}$ is convergent if $1< r\leq 1$ for $r$- real, and divergent for other values of $r$.
\begin{equation}\boxed{\label{eqLimitRtoTheNth}
\lim\limits_{n\to \infty}r^n= \left\{\begin{array}{ll} 0 & \mathrm{if~} -1<r<1\\ 1& \mathrm{if~} r=1\end{array} \right.\quad .
}
\end{equation}
A sequence $\{a_n\}$ is increasing if $a_n<a_{n+1}$ for all $n$. A sequence is decreasing if $a_n>a_{n+1}$ for all $n$. A sequence is monotonic if it is either increasing or decreasing. \index{sequence!monotonic} \index{sequence!increasing} \index{sequence!decreasing}

A sequence $\{a_n\}$ is bounded above by $M$ if $a_n<M$ for all $n$. A sequence is bounded below by $M$ if $a_n>M$ for all $n$. If a sequence is bounded above and below it is called a bounded sequence. \index{sequence!bounded, below and above}

\begin{theorem}\label{thMonotonicSequenceThereom}\index{sequence!monotonic sequence theorem}
 (Monotonic sequence theorem). Every bounded monotonic sequence is convergent.
\end{theorem}
\section{A couple of tools}
\subsection{L'Hospital's rule}
\begin{theorem}
\index{theorem!mean value} Mean value theorem (Review). Let $f:[a,b]\to \mathbb R$ (i.e., $f$ is a function from the interval $[a,b]$ to the reals), such that $f$ is continuous in the entire interval $[a,b]$, and differentiable in the open interval $(a,b)$. Then there exists a number $c\in (a,b)$ such that
\[
f'(c)=\frac{f(b)-f(a)}{b-a}\quad,
\]
or equivalently,
\[
(b-a)f'(c)=f(b)-f(a)\quad .
\]
\end{theorem}

\begin{theorem} \index{L'Hospital's rule}
(L'Hospital's rule). Let $I$ be an open interval and let $a\in I$. Let $f$ and $g$ be functions that are differentiable in the interval $I\setminus \{a\}$ ($I$ minus the point $a$). Suppose either
\[
\lim_{x\to a} f(x)= \lim_{x\to a}g(x)=0
\]
or
\[
\lim_{x\to a} f(x)= \lim_{x\to a}g(x)=\pm \infty\quad .
\]
Suppose in addition
\[
\lim_{x\to a} \frac{f'(x)}{g'(x)}
\]
exists and $g'(x)\neq 0$ for all $x\in I\setminus \{a\}$. Then
\[
\lim_{x\to a}\frac{f(x)}{g(x)}= \lim_{x\to a} \frac{f'(x)}{g'(x)}\quad .
\]
\end{theorem}

L'Hospital's rule applies for $a=\infty$, i.e., $x\to \infty$.

Proof of L'Hospital's rule in the case that $\lim\limits_{x\to a}g(x)=0 $. For $x$ sufficiently close to $a$ we have by the Mean value theorem that $f(x)-f(a)=f(x)=  f'(c_x) (x-a) $, where $c_x$ is some point in the interval $(a,x)$ and depending on $x$. Similarly, $g(x)- g(a)= g'(\bar c_x)(x-a)$ for some other point $\bar c_x\in (a,x)$. Therefore
\[
\lim\limits_{x\to a}\frac{f(x)}{g(x)}=\lim\limits_{x\to a}\frac{f'(c_x) \cancel{(x-a)}}{ g'(\bar c_x)\cancel{(x-a)}} = \lim\limits_{x\to a}\frac{f'(c_x) }{ g'(\bar c_x)}=\lim\limits_{x\to a}\frac{f'(x) }{ g'(x)}\quad ,
\]
where the very last equality follows from the fact that for every $x$, $c_x$ is in the interval $(a,x)$, and therefore $c_x$ (as a function of $x$) tends to $a$ as $x$ tends to $a$. For the very last equality we are also using the fact that $f'(x)$ and $g'(x)$ are continuously differentiable in a small neighborhood of $a$ (where?).

The above explanation does not deal with the case of $f(x), g(x)\to\pm \infty $ or with $x\to \infty$. A detailed proof in those cases can be found at Wikipedia's page.
\url{http://en.wikipedia.org/wiki/L'H%C3%B4pital's_rule}

A strategy for finding a limit $\lim\limits_{x\to a}F(x)$. The strategy is not guaranteed to work.
\begin{itemize}
\item Step 1. Important(!) Check whether $F(x)$ is well-defined at $x=a$ by plugging in $x=a$. If $F(a)$ is well defined, or is already known to tend to $\pm \infty$, the limit is simply $F(a)$.
\item Step 2. From now on we assume that $F(x)$ is not well-defined at $x=a$, and we do not know its limit. $F(x)$ can be of the form $\frac{0}{0}$, $\frac{\pm \infty }{\pm \infty}$, $ \infty-\infty$, etc.

Here, the terminology ``of the form $\frac{0}{0}$ or $\frac{\pm\infty}{\pm \infty}$'' is used colloquially; such wording should be avoided in scientific texts (including the natural sciences).
\item Step 3. Try to rewrite $F(x)$ as a quotient of two functions $\frac{f(x)}{g(x)}$ such that $f(x)$ and $g(x)$ are well-defined, or have known limits $f(x), g(x)\to \pm \infty$ as $x\to a$.
\begin{itemize}
\item If $F(x)$ is of the form $f(x)g(x)$ you may try rewriting as $\frac{f(x)}{\frac{1}{g(x)}}$.
\item If $F(x)$ is of the form $f(x)\pm g(x)$ you may try rewriting as $\frac{(f(x)\pm g(x))(f(x)\mp g(x))}{f(x)\mp g(x)}$.
\end{itemize}
Note there is no fixed recipe for rewriting $F$.
\item Step 4. Check whether $\frac{f(a)}{g(a)}$ is of the form $\frac{0}{0}$, or $\frac{\pm\infty}{\pm \infty}$. If so, use L'Hospital's rule. Go back to Step 1.
\end{itemize}
Note that in the above strategy, you might end up going in an infinite cycle. If, in your judgment, you are going in an infinite cycle, try transforming $F(x)$ algebraically.
\subsection{Examples of L'Hospital's rule}
\[
\lim\limits_{x\to 0} \underbrace{(x-1)\ln x}_{\mathrm{~of~the~form~}(-1)*(-\infty)} =+\infty\quad .
\]

\[\begin{array}{rcl}
\underbrace{\lim\limits_{x\to 0} x\ln x}_{\mathrm{~of~the~form~}0*(-\infty)} &=&\underbrace{\lim\limits_{x\to 0} \frac{\ln x}{\frac{1}{x}}}_{\mathrm{~of~the~form~}(\pm\infty)/(\pm\infty)}\stackrel{\mathrm{L'Hospital}}{=}\lim\limits_{x\to 0} \frac{(\ln x)'}{\left(\frac{1}{x}\right)'}\\&=& \lim\limits_{x\to 0} \frac{\frac{1}{x}}{\frac{1}{x^2}}=  \lim\limits_{x\to 0} x =0
\end{array}
\]

\[
\begin{array}{rcl}
\lim\limits_{x\to 0} \underbrace{\frac{\sin x}x}_{\mathrm{~of~the~form~}0/0}\stackrel{\mathrm{L'Hospital}}{=} \lim\limits_{x\to 0} \frac{(\sin x)'}{x'}= \lim\limits_{x\to 0} \frac{\cos x}{1}= 1\quad .
\end{array}
\]

\[
\begin{array}{rcl}
\underbrace{\lim\limits_{x\to \infty} \frac{e^x}{x^2}}_{\mathrm{~of~the~form~}\pm \infty/\pm\infty} &\stackrel{\mathrm{L'Hospital}}{=} & \underbrace{\lim\limits_{x\to \infty} \frac{e^x}{2x}}_{\mathrm{~of~the~form~}\pm \infty/\pm\infty}\stackrel{\mathrm{L'Hospital}}{=}\lim\limits_{x\to \infty} \frac{e^x}{2}= +\infty
\end{array}
\]

Let $n\in \mathbb Z_{\geq 0}$ (i.e., $n$ is a non-negative integer). Prove by induction that
\begin{equation}\label{eqLimiteMinusXtimesxtoNth}
\lim\limits_{x\to \infty} x^ne^{-x} = \lim\limits_{x\to \infty}\frac{ x^n}{e^{x}}=0\quad .
\end{equation}
\begin{solution}
For the base of the induction we have that $\lim\limits_{x\to \infty}\frac{1}{e^{x}}=0$ as $e^x\to +\infty$. Suppose we have already proven by induction that $ \lim\limits_{x\to \infty}\frac{x^{n-1}}{e^{x}}=0$. We aim to prove that $ \lim\limits_{x\to \infty}\frac{x^{n}}{e^{x}}=0$. Indeed, we compute
\[
\begin{array}{rcl}
\underbrace{\lim\limits_{x\to \infty} \frac{x^n}{e^x}}_{\mathrm{~of~the~form~}\pm \infty/\pm\infty} &\stackrel{\mathrm{L'Hospital}}{=} & \lim\limits_{x\to \infty} \frac{nx^{n-1}}{e^x}=n\left( \lim\limits_{x\to \infty} \frac{x^{n-1}}{e^x}\right) \stackrel{\mathrm{by~induction~hypothesis}}{=}0\quad .
\end{array}
\]
which proves the induction step and completes the proof.
\end{solution}

\[
\begin{array}{rcl}
\lim\limits_{x\to \infty} \frac{(\ln x)}{\sqrt{x}} &\stackrel{\mathrm{L'Hospital}}{=} & \lim\limits_{x\to \infty} \frac{(\ln x)'}{(\sqrt{x})'}=
 \lim\limits_{x\to \infty} \frac{1/x}{1/2x^{-1/2}}= \lim\limits_{x\to \infty} \frac{2}{\sqrt{x}}=0
\end{array}
\]

\[
\begin{array}{rcl}
\lim\limits_{x\to 0}\limits \frac{\tan x- x}{x^3}&\stackrel{\mathrm{L'Hospital}}{=} &\lim\limits_{x\to 0} \frac{\frac{1}{\cos^2 x}-1}{3x^2}=
\lim\limits_{x\to 0} \frac{1}{\cos^2x}\frac{1-\cos^2x}{3x^2}\stackrel{\mathrm{see~the~remark!}}{=}\underbrace{\lim\limits_{x\to 0} \frac{1}{\cos^2x}}_{=1} \lim\limits_{x\to 0} \frac{1-\cos^2x}{3x^2} \\
&\stackrel{\mathrm{L'Hospital}}{=}&\lim\limits_{x\to 0}\frac{2\sin x\cos x }{6x}= \lim\limits_{x\to 0}\frac 13\cos x\lim\limits_{x\to 0}\frac{\sin x}{x}\stackrel{\mathrm{L'Hospital}}{=}\frac 13\lim\limits_{x\to 0}\cos x= \frac 13\quad .
\end{array}
\]
\textbf{Remark.} In general we do not have the right to split off limits: it may happen that $\lim\limits_{x\to a} f(x)g(x)$ exists but $ \left(\lim\limits_{x\to a} f(x)\right) \left(\lim\limits_{x\to a} g(x)\right)$ doesn't. The easiest example for this is $1=\lim\limits_{x\to 0} 1 =  \lim\limits_{x\to 0} \frac{x}{x}$. This expression cannot be split as $\left(\lim\limits_{x\to 0} \frac{1}{x}\right)\left(\lim\limits_{x\to 0} x \right)$: indeed, the latter expression is not well-defined.

However, if $f(a)\neq 0, \pm \infty$ and $f$ is continuous in a small neighborhood of $a$, then we have the right to write $\lim\limits_{x\to a} f(x)g(x)= \left(\lim\limits_{x\to a} f(x)\right) \left(\lim\limits_{x\to a} g(x)\right)= f(a)\left(\lim\limits_{x\to a} g(x)\right)$.

\subsection{Improper integrals} \label{secImproperIntegrals}
\index{improper integral} Improper integrals are the combination of definite integrals and limits.

Type I: infinite intervals.

\begin{itemize}
\item Suppose $\int\limits_{t=a}^{x}f(t)\diff t$ exists for all $t\geq a$. We define
\[
\int\limits_{a}^{\infty} f(t)\diff t\eqdef  \lim
\limits_{x\to \infty}\int\limits_{t=a}^{x}f(t)\diff t\quad,
\]
provided the above limit exists.
\item Similarly, suppose $\int\limits_{t=x}^{a}f(t)\diff t$ exists for all $t\leq a$. We define
\[
\int\limits_{-\infty}^{a} f(t)\diff t\eqdef  \lim
\limits_{x\to \infty}\int\limits_{t=x}^{a}f(t)\diff t\quad,
\]
provided the above limit exists.
\item Suppose $\int\limits_{0}^{\infty}f(t)\diff t$ and $\int\limits_{-\infty}^{0}f(t)\diff t$  exist. We define
\[
\int\limits_{-\infty}^{\infty} f(t)\diff t\eqdef  \int\limits_{0}^{\infty}f(t)\diff t+\int\limits_{-\infty}^{0}f(t)\diff t
\]
provided the above limit exists.
\end{itemize}
In case the limits defining the above improper integrals do not exist, we call the improper integrals \emph{divergent}. Else, if the limits exist, we call them \emph{convergent}. \index{convergent!improper integral} \index{divergent!improper integral}

Type II: integrating discontinuous functions.
\begin{itemize}
\item Suppose $f:[a,b)\to \mathbb R$ ($f$ is a function from $[a,b)$ to the reals) is continuous in $[a,b)$, but discontinuous at $b$. We define
\[
\int\limits_{a}^{b}f(t)\diff t\eqdef \lim\limits_{\substack{x\to b \\ x<b}}\int\limits_{a}^{x} f(t)\diff t\quad ,
\]
provided the limit exists.
\item Similarly, suppose $f:(a,b]\to \mathbb R$ ($f$ is a function from $(a,b]$ to the reals) is continuous in $(a,b]$, but discontinuous at $a$. We define
\[
\int\limits_{a}^{b}f(t)\diff t\eqdef \lim\limits_{\substack{x\to a \\ x>a}}\int\limits_{x}^{b} f(t)\diff t\quad ,
\]
provided the limit exists.
\end{itemize}

The comparison theorem. Suppose $f(x)\geq g(x)\geq 0$.
\begin{itemize}
\item If $\int\limits_{a}^{b}f(x)\diff x$ is convergent, so is $\int\limits_{a}^{b}g(x)\diff x$. Here, we allow $ b=\infty$ or $a=-\infty$, as well as we allow improper integrals of type II.
\item If $\int\limits_{a}^{b}g(x)\diff x$ is divergent, so is $\int\limits_{a}^{b}f(x)\diff x$. Here, we allow $ b=\infty$ or $a=-\infty$, as well as we allow improper integrals of type II.
\end{itemize}

\subsection{Examples of improper integrals}
Let $p\neq 1$. Then
\begin{equation}\label{eqIntegralxTopthPowerToInfty}
\int\limits_{x=1} \frac{1}{x^{p}} \diff x = \left.-\frac{x^{-p+1}}{-p+1}\right|_{x=1}^{\infty}= \lim\limits_{x\to\infty}
\frac{1}{(p-1) x^{p-1}}+\frac{1}{p-1} =\doublebrace{\infty}{p<1}{\frac{1}{p-1}}{p>1} \quad .
\end{equation}

\begin{equation}\label{eqIntegral1overxToToInfty}
\int\limits_{x=1}^{\infty} \frac{1}x \diff x= \left.\ln x \right|_{x=1}^{\infty}= \infty\quad .
\end{equation}

\begin{equation}\label{eqGamma(1)}
\begin{array}{rcl}
\int\limits_{0}^{\infty}e^{-x}\diff x&=& \left.- e^{-x}\right|_{x=0}^{x=\infty}=-\lim\limits_{x\to \infty}e^{-x}  - (-e^{-0})\\&=& -\lim \limits_{x\to \infty} \frac{1}{e^{x}} +1=1
\end{array}
\end{equation}


\[
\begin{array}{rcl}
\int\limits_{0}^{\infty}xe^{-x}\diff x&=& \left.-xe^{-x} - e^{-x}\right|_{x=0}^{x=\infty}=\lim\limits_{x\to \infty}\left( -xe^{-x} -e^{-x}\right) - (-0e^{-0}-e^{-0})\\&=& 1-\lim \limits_{x\to \infty} \frac{x+1}{e^{x}}\stackrel{\mathrm{L'Hospital}}{=}1-\lim \limits_{x\to \infty} \frac{1}{e^{x}} =1
\end{array}
\]

More generally, we can compute $\Gamma (n):=\displaystyle\int\limits_{0}^{\infty} x^ne^{-x}\diff x$. $\Gamma$ is the so called \emph{gamma function}, invented by L. Euler. As we compute below, in case $n$ is an integer, $\Gamma(n)=(n-1)!$. In case $n$ is not an integer, $\Gamma(n)$ may still be defined, and thus $\Gamma$ is a generalization of the notion of factorial. Suppose $n>0$ is an integer. Then
\[
\begin{array}{rcl}
\Gamma (n+1)&:=&\underbrace{\int\limits_{0}^{\infty} x^{n}\underbrace{e^{-x}\diff x}_{=-d (e^{-x})}}_{\mathrm{integrate~by~parts}}=\left. -x^{n}e^{-x}\right|_{x=0}^{\infty}+ n\underbrace{\int\limits_{0}^{\infty}x^{n-1} e^{-x}\diff x}_{=\Gamma(n)}\\
&=& \underbrace{\lim\limits_{x\to \infty} -\frac{x^{n}}{e^x}}_{=0\mathrm{~using~\eqref{eqLimiteMinusXtimesxtoNth}}}+ 0^ne^{-0}+n\Gamma(n)= n\Gamma(n)\\
&=& n(n-1)(n-2)=\dots =n(n-1)(n-2)\dots 2*1* \Gamma(1)= n!\quad .
\end{array}
\]
For the very last equality we used the fact that we already computed $\Gamma(1)$ in \eqref{eqGamma(1)}.

Determine whether the integral below is convergent.
\[
\int\limits_{0}^{\infty}e^{-x^2}\diff x
\]
\begin{solution}
We cannot evaluate the above integral using the techniques studied so far. However  for $x\geq 1$ we have that $x^2\geq x$ and therefore $e^{x^2}\geq e^{x}>0$ and finally $e^{-x^2}= \frac{1}{e^{x^2}}<\frac{1}{e^x}= e^{-x}$. We have that
\[
\int\limits_{0}^{\infty}e^{-x^2}\diff x= \int\limits_{0}^{1}e^{-x^2}\diff x +\int\limits_{1}^{\infty}e^{-x^2}\diff x\quad .
\]
The first integral $\int\limits_{0}^{1}e^{-x^2}\diff x$ is proper and well defined, as $e^{-x^2}$ is a continuous function in the interval $[0,1]$ (and everywhere else).  We already showed that $ \int\limits_{0}^{\infty}e^{-x}\diff x$ is convergent; we leave it to the reader to show that $ \int\limits_{1}^{\infty}e^{-x}\diff x$ is convergent too (this is in fact obvious). As $e^{-x}>e^{-x^2}$ for $x>1$, we apply the comparison theorem to get that $\int\limits_{0}^{\infty}e^{-x^2}\diff x<\int\limits_{1}^{\infty}e^{-x}\diff x$ is convergent.
\end{solution}

\section{Formal power series and operations with them} \label{secFormalPowerSeries}
\index{series!formal power} A \emph{formal power series} is an expression of the form
\begin{equation}\label{eqFormalPowerSeriesDef}
(a_0 + a_1x+a_2x^2+\dots +a_nx^n+\dots)= \sum_{n=0}^{\infty} a_n x^n \quad .
\end{equation}

\index{series!formal power at a point} A \emph{formal power series in $x-p$}, or \emph{formal power series at $p$} is an expression of the form
\begin{equation}\label{eqFormalPowerAtPSeriesDef}
(a_0 + a_1(x-p)+a_2(x-p)^2+\dots +a_n(x-p)^n+\dots)= \sum_{n=0}^{\infty} a_n (x-p)^n \quad .
\end{equation}
The theory of formal power series in $y=x-p$ is identical with the theory of formal power series in the indeterminate $y$ and we will not discuss it any further. Formal power series in $y=x-p$ are used when defining Taylor series (see Definition \ref{defTaylorSeries}).

The formal power series \eqref{eqFormalPowerSeriesDef} is to be thought as alternative way of writing the infinite sequence $a_1, a_2, \dots, a_n,\dots $. The ``infinite sum'' in the definition of formal power series does not imply that the sum converges in any way, or that any limits exist. We stress the fact that the expression  \eqref{eqFormalPowerSeriesDef} is  formal - $x$  is an indeterminate, rather than a number. This analogous to the fact that in a polynomial in $x$, the variable $x$ is an indeterminate.

We may or may not be allowed to plug in numbers for $x$ in the expression \eqref{eqFormalPowerSeriesDef}, depending on whether the corresponding sum is convergent. However, the formal power series are defined independent of any convergence issues.

We may (formally) add, subtract, multiply, integrate, differentiate formal power series. With additional assumptions, we may be able to divide power series, as well as carry out formal substitutions replacing $x$ by a formal power series.

Adding, subtracting and multiplying formal power series is carried out by ``uncovering the infinite brackets'', much in the way that finite expressions are added, subtracted and multiplied.
\[
\sum_{n=0}^\infty a_n x^n\pm \left(\sum_{n=0}^\infty b_n x^n\right)\eqdef \sum_{n=0}^\infty (a_n+b_n)x^n\quad .
\]
\[
\begin{array}{r@{~}c@{~}l}
\displaystyle\left(\sum_{n=0}^\infty a_n x^n\right) \left(\sum_{n=0}^\infty b_n x^n\right) &=&
(a_0 + a_1x+a_2x^2+\dots )(b_0 + b_1x+b_2x^2+\dots )\\&\eqdef&
a_0b_0 + (a_0b_1+a_1b_0)x + (a_0b_2+a_1b_1+a_2b_0)x^2+\dots \\
&&+(a_0 b_n +a_1b_{n-1}+\dots + a_{n-1}b_1 +a_nb_0 )x^n+\dots \\
&=&\displaystyle \sum_{n=0}^{\infty} \left(\sum_{k=0}^n a_k b_{n-k}  \right)x^n\quad .
\end{array}
\]

Formal power series can be (formally) differentiated and integrated.
\[
\begin{array}{rcl}
\displaystyle
\frac{\diff}{\diff x}\left(\sum_{n=0}^{\infty} a_n x^n\right) &=& \displaystyle \frac{\diff}{\diff x} \left(a_0 +a_1x+a_2x^2+\dots \right)\\
&\eqdef& a_1+2 a_2 x+ 3a_3x^2 +\dots\\
&=&\displaystyle \sum_{n=1}^{\infty} na_nx^{n-1}\quad .
\end{array}
\]

\[
\begin{array}{rcl}
\displaystyle \int \left(\sum_{n=0}^{\infty} a_n x^n\right)\diff x &=& \displaystyle \int (a_0 +a_1x+a_2x^2+\dots )\diff x\\
&\eqdef& C+ a_0x +a_1\frac{x^2}{2}+a_2\frac{x^3}3+\dots\\
&=&\displaystyle C+\sum_{n=1}^{\infty} \frac{a_n}{n+1}x^{n+1}\quad .
\end{array}
\]

\textbf{(!) Optional material. This point does not require material outside of this course, but you may find it technically challenging. You will not be tested on this material}. Let $g(x)$ be a formal power series without a constant term, that is, $g(x)$ is of the form $g(x)=\sum_{n=1}^\infty b_n x^n=  b_1x+b_2x^2+\dots$. Let $f(x)=\sum_{n=0}^{\infty} a_n x^n$ be a formal power series. Then we can carry out substitutions $f(g(x))$, much in the way that substitutions in usual ``finite'' polynomials are carried out. Details follow.

\[
\begin{array}{rcl}
\displaystyle
\left(\sum_{n=1}^{\infty} b_n x^n\right)^k &=&   (b_1 x+ b_2x^2+b_3x^3+\dots)^k\\
&=&\displaystyle \sum_{n=k}^{\infty}\underbrace{\left( \sum_{\substack{\mathrm{all~possible~indices}\\ k\geq i_1, i_2,\dots, i_n\geq 0\\
\mathrm{for~which}\\i_1+2i_2+3i_3+\dots + ni_n =n  \\ \mathrm{and} \\ i_1+\dots +i_n=k}}\binom{k}{i_1, i_2, i_3, \dots, i_n}  b_{1}^{i_1}\dots b_{n}^{i_n} \right)}_{=: c_{k,n}} x^n,
\end{array}
\]
where $\binom{k}{i_1, i_2, i_3, \dots, i_n}\eqdef  \doublebrace{\frac{k!}{i_1!i_2!\dots i_n!}}{\mathrm{if~}i_1+\dots i_n=k}{0}{\mathrm{otherwise}}$. Let $c_{k,n}$ be the numbers defined as indicated in the preceding formula. Then
\[
\begin{array}{rcl}
f(g(x)) &\eqdef&\displaystyle a_0+ \sum_{k=1}^\infty a_k\left(\sum_{j=1}^{\infty} b_j x^j\right)^k = a_0+\sum_{k=1}^\infty a_k \left(\sum_{n=k}^{\infty} c_{k,n}x^n \right)\\
&=&\displaystyle a_0+\sum_{n=1}^{\infty} \left(\sum_{k=1}^n a_k c_{k,n} \right)x^n\quad .
\end{array}
\]
The above formulas would be cumbersome to work with by hand except for the smallest of examples. In particular, all examples done in this course will use shortcuts that circumvent the preceding formulas. However, the preceding formulas may be needed for a complicated problem arising in practice/outside of the scope of this course. Furthermore, the formulas are well-suited for implementation in computer algebra systems.

\section{Maclaurin series as formal power series}\label{secMaclaurinSeries}
\begin{definition}\label{defMaclaurinSeries}
\index{series!Maclaurin} Let $f(x)$ be a function that is infinitely differentiable at $0$. Recall that $f^{(n)}(x)$ denotes the $n^{th}$ derivative of $f(x)$. Then the formal power series
\begin{equation}\label{eqMacLaurinDef}
\begin{array}{rcl}
\maclaurin (f(x))&\eqdef&\displaystyle \sum_{n=0}^{\infty}  \frac{f^{(n)} (0)}{n!} x^n\\
&=& \displaystyle f(0)+f'(0)x+ \frac{f''(0)}{2!}x^2 + \frac{f'''(0)}{3!} x^3 + \dots + \frac{f^{(n)}(0)}{n!}x^n+\dots
\end{array}
\end{equation}
is called the Maclaurin series of $f(x)$.
\end{definition}
There is no standard notation for Maclaurin series; we use the notation $\maclaurin (f(x))$.


\begin{definition}\label{defTaylorSeries}
\index{series!Taylor} Let $f(x)$ be a function that is infinitely differentiable at the point $p$. Then the formal power series
\begin{equation}\label{eqTaylorDef}
\begin{array}{rcl}
\taylor_p (f(x))&\eqdef&\displaystyle \sum_{n=0}^{\infty}  \frac{f^{(n)} (p) }{ n!} (x-p)^n\\&=& \displaystyle f(0)+f'(0)(x-p)+\frac{f''(0)}{2!}(x-p)^2 +\dots+ \frac{f^{(n)}(0)}{n!}(x-p)^n+\dots
\end{array}
\end{equation}
is called the Taylor series of $f(x)$ at $p$. The Maclaurin series are a partial case of the Taylor series at the point $0$.
\end{definition}
There is no standard notation for Taylor series at $p$; we use the notation $\taylor_p(f(x))$.

The motivation for the definition of Maclaurin series is that, with some conditions on the function $f(x)$, given a number $a$ with sufficiently small absolute value, $f(a)$ equals the limit of the sum of the Maclaurin series of $f(x)$ when we carry out the substitution $x=a$. An analogous statement holds for Taylor series.

In order to make the above point precise, we need to establish
\begin{enumerate}
\item under what conditions we can plug in a number in place of $x$ in \eqref{eqMacLaurinDef} and
\item under what conditions does the sum \eqref{eqMacLaurinDef} equal the function $f(x)$ (when we substitute values for $x$).
\end{enumerate}
This is one of the central goals of Section \ref{secSeriesConvergence} .

In Section \ref{secFormalPowerSeries}, we learned how to carry out operations with formal power series. Therefore we may compute with Maclaurin series independent of proving properties 1) and 2) above. We postpone the investigation of problem 1)  to section \ref{secSeriesConvergence}. We shall not discuss problem 2) in this course (an illustration of the issue is given by Problem \ref{probDifferentiableNonAnalyticFunctionExample}). In the rest of this section, we will learn instead how to compute with Maclaurin series.

When computing Maclaurin series, we can use the fact that performing operations with functions and then taking Maclaurin series amounts to the same result as first taking the Maclaurin series and then performing the corresponding operation with the formal power series. The operations on power series were defined in the most natural fashion. Therefore, if all functions were equal to their Maclaurin series, it would seem obvious that performing an operation on them and then taking the Maclaurin series amounts to the same thing as first taking the Maclaurin series and then performing the operation. However this remains true even if the functions involved are not equal to their Maclaurin series. 
\[
\maclaurin (f(x)\pm g(x))= \maclaurin (f(x))\pm \maclaurin(g(x))
\]
\[
\maclaurin (f(x)* g(x))= \maclaurin (f(x))* \maclaurin(g(x))
\]
If $f(x)g(x)=1$ for all $x$ in a small interval $I$ containing $0$, then
\[
\maclaurin (f(x)*g(x))= 1\quad,
\]
as well as
\[
\maclaurin\left(\frac{1}{f(x)}\right)= \maclaurin(g(x))\quad .
\]
The following two properties will discussed in more detail in \ref{thDifferentiatingIntegratingPowerSeriesNonFormal}.
\[
\maclaurin \left(\frac{\diff f}{\diff x}(x)\right)= \frac{\diff }{\diff x} \left( \maclaurin(f(x)) \right)\quad .
\]
\[
\maclaurin \left(\int f(x)\diff x \right) = \int\left( \maclaurin( f(x) ) \right) \diff x \quad .
\]
\subsubsection{The Maclaurin series of frequently used functions}
We already studied the Maclaurin series of $e^x, \sin x, \cos x$ in \eqref{eqSinCosMaclaurinSeries} and \eqref{eqExponentFunctionDefinition}.

\[
\begin{array}{r@{~}c@{~}l}
(1-x)(1+x+x^2+\dots +x^n+\dots) &=& (1+\cancel{x}+\cancel{x^2}+\dots +\cancel{x^n} +\dots)\\
&& -(\phantom{1+}\cancel{x}+\cancel{x^2}+\cancel{x^3}+\dots + \cancel{x^n}+\dots)\\
&=& 1\quad .
\end{array}
\]
Therefore
\begin{equation}\label{eqInfiniteGeometricProgression}
\maclaurin \left( \frac{1}{1-x}\right)= 1+x+x^2+\dots \quad .
\end{equation}

\begin{problem}
\input{../../modules/power-series/homework/newton-binomial-integer-generalized}
\end{problem}
\input{../../modules/power-series/homework/newton-binomial-integer-generalized-solution}

More generally, us let solve the following.
\begin{problem}
\input{../../modules/power-series/homework/newton-binomial-generalized}
\end{problem}
\input{../../modules/power-series/homework/newton-binomial-generalized-solution}

As we shall soon learn, $\maclaurin \left( (1+x)^q\right)_{|x=c} =(1+c)^q $   for all $c\in (-1, 1)$.

The power series of $\arctan x$.
\[
\begin{array}{rcl}
\frac{\diff}{\diff x}\maclaurin (\arctan x)&=& \displaystyle \maclaurin\left( \frac{\diff}{\diff x}(\arctan x)\right)= \maclaurin \left(\frac{1}{1+x^2}\right) \\
&=&\displaystyle \maclaurin \left(\frac{1}{1-(-x^2)}\right)\\
& \stackrel{\mathrm{use~} \eqref{eqInfiniteGeometricProgression} }{=} & 1+(-x^2)+(-x^2)^2+(-x^2)^3+\dots+ (-x^2)^n+\dots
\\ &=& 1-x^2+x^4-x^6+\dots+ (-1)^nx^{2n}+\dots \quad .
\end{array}
\]
Therefore
\[
\begin{array}{rcl}
\maclaurin (\arctan x)&=&\displaystyle \int( 1-x^2+x^4-x^6+\dots (-1)^nx^{2n} + \dots )\diff x \\
&=&\displaystyle  \underbrace{\arctan(0)}_{=0}+ x- \frac{ x^3}3 + \frac{ x^5}5 -\frac{ x^7}7+\dots +(-1)^n\frac{x^{2n+1}}{2n+1}+\dots\\
&=&\displaystyle  \sum_{n=0}^{\infty} (-1)^n\frac{x^{2n+1}}{2n+1}\quad .
\end{array}
\]
The power series of $\ln (1-x)$, $\ln(1+x)$.
\[
\begin{array}{rcl}
\displaystyle \frac{\diff}{\diff x}\maclaurin(\ln (1-x))&=& \displaystyle \maclaurin \left(\frac{\diff}{\diff x}(\ln (1-x))\right) = \maclaurin \left( - \frac{ 1}{1-x}\right)\\
&=& \displaystyle -1-x-x^2-\dots -x^n-\dots
\end{array}
\]
Therefore
\[
\begin{array}{rcl}
\displaystyle \maclaurin(\ln (1-x) )&=&\displaystyle \int (-1-x-x^2-\dots -x^n - \dots)\diff x \\&=&\displaystyle \underbrace{\ln (1-0)}_{=0} - x- \frac{ x^2 }{ 2}- \frac{x^3}{3}-\dots - \frac{x^n}{n}-\dots= -\displaystyle\sum_{ n=1} \frac{ x^n}{n} \quad .
\end{array}
\]
Therefore
\[
\maclaurin(\ln (1+x) )=\sum_{n=1}(-1)^{n+1}\frac{x^n}{n}\quad .
\]

\[
\begin{array}{rcl}
\frac{\diff}{\diff x}\maclaurin(\arcsin{x})&=&\displaystyle \maclaurin \left( \frac{\diff}{\diff x}(\arcsin{x})\right)= \maclaurin \left( \frac{ 1}{ \sqrt{ 1-x^2 }}\right)\\
&=&
\maclaurin\left( (1+(-x^2))^{-\frac{1}2}\right)\\
&\stackrel{\eqref{eqNewtonBinomialGeneralized}}{=}&\displaystyle 1-\frac{1}{2 }(- x^{2})+ \left(\frac{-\frac{1}{2}(-\frac{1}2-1)}{2!}\right)(-x^2)^2+ \dots\\
&&\displaystyle + \left(\frac{-\frac{1}{2}(-\frac{1}2-1)\dots (- \frac{1}2 -n+1) }{n!}\right)(-x^2)^n+\dots
\\
&=& \displaystyle\sum\limits_{n=0}^{\infty} (-1)^n \binom{-\frac{1}{2}}{n} x^{ 2n} \quad .
\end{array}
\]
Therefore
\[
\maclaurin(\arcsin x)=\int \maclaurin\left(\frac{1}{\sqrt{1-x^2}}\right) \diff x =\underbrace{0}_{=\arcsin 0} +\sum_{n=1}^{\infty} (-1)^n \binom{-\frac{1}{2}}{n} \frac{x^{2n+1}}{2n+1}\quad .
\]

\subsubsection{Examples and Exercises}
\begin{problem}
Compute the Maclaurin series of the following.
\begin{multicols}{2}
\begin{enumerate}
\item $\displaystyle \maclaurin (\arctan x) $.
\item $\displaystyle \maclaurin (\sqrt{1+x^2})$.
\item $\displaystyle \maclaurin (\arccos 2x)$.
\item $\displaystyle \maclaurin(\ln (1+x^2))$.
\item Recall from \eqref{eqExponentFunctionDefinition} the series $\displaystyle \maclaurin (e^x)$.
\item Recall from \eqref{eqSinCosMaclaurinSeries} the series $\displaystyle \maclaurin (\sin x)$.
\item Recall from \eqref{eqSinCosMaclaurinSeries} the series $\displaystyle \maclaurin (\cos x)$.
\end{enumerate}

\end{multicols}
\end{problem}

\begin{problem}\label{probDifferentiableNonAnalyticFunctionExample}
 \textbf{This problem is of higher difficulty.}
Let $f(x)$ be defined as
\[
f(x):=\doublebrace{e^{-\frac{1}{x^2}}}{\mathrm{if~} x>0}{0}{\mathrm{otherwise.}}
\]
\begin{enumerate}
\item Prove that if $R(x)$ is an arbitrary rational function,
\[
\lim\limits_{\substack{x\to 0\\ x>0}} R(x)e^{-\frac{1}{x^2}}=0
\]
\item Prove that $f(x)$ is differentiable at $0$ and $f'(0)=0$.
\item Prove that the Maclaurin series of $f(x)$ are 0 (but $f(x)$ is clearly a non-zero function).
\end{enumerate}

\end{problem}
\begin{problem}\textbf{This problem is of higher difficulty.} Prove the properties of Maclaurin series given in the last item of Section \ref{secMaclaurinSeries}.
\end{problem}

\subsubsection{L'Hospital's rule and limits revisited}\label{secLHospitalRevisited}
\index{L'Hospital's rule}
Suppose we want to compute $\lim\limits_{x\to 0} \frac{f(x)}{g(x)}$ when $f(0)= 0$ and $g(0)=0$.
L'Hospital's rule teaches us that we should differentiate $f(x)$ and $g(x)$ until we establish whether the limit exists or not. Alternatively, we may write the Maclaurin series $\maclaurin (f(x))$, $\maclaurin (g(x))$. For sufficiently small $x$ and sufficiently well-behaved functions $f(x)$ and $g(x)$ (which include all functions studied in this course), $f(x)$ and $g(x)$ will equal their Maclaurin series (when we substitute values for $x$). Therefore we can substitute $f(x)$ and $g(x)$ by their Maclaurin series, cancel the common power of $x$, and evaluate the limit. This is often easier and quicker than L'Hospital's rule, in particular for problems given on Calculus exams (this statement is valid not only with the current instructor!). In the following items we give examples.

As we shall see in section \ref{secSeriesConvergence}, for small enough values of $|x|$, $\arcsin x$ equals its Maclaurin series. Therefore, for small $x$, we have that $\arcsin x = x+\frac{1}6x^3 + \frac{3}{40}x^5+\dots $. Therefore for small $x$
\[
\arcsin x- x- \frac{1}6x^3= x^5 \left(\frac{3}{40}+x^2(\dots)\right)\quad .
\] Therefore
\[
\lim\limits_{x\to 0} \frac{\arcsin x- x- \frac{1}6x^3}{x^5}= \lim\limits_{x\to 0} \frac{ \cancel{x^5} \left(\frac{3}{40}+x^2(\dots)\right)}{\cancel{x^5}} =  \lim\limits_{x\to 0}  \left(\frac{3}{40}+x^2(\dots)\right) =\frac{3}{40}\quad .
\]

Let us ``spice up'' the preceding problem:
\[
\lim\limits_{x\to 0} \frac{\arcsin x- x- \frac{1}6x^3}{\sin^{5} x}= \lim\limits_{x\to 0} \frac{x^5 \left(\frac{3}{40}+x^2(\dots)\right)}{(x-\frac{x^3}6+\dots)^5}=  \lim\limits_{x\to 0} \frac{\cancel{x^5} \left(\frac{3}{40}+x^2(\dots)\right)}{\cancel{x^5}(1-x^2(\dots))}= \frac{3}{40}\quad .
\]

\begin{equation}\label{eqLhospitalExample1}
\lim\limits_{x\to 0}\frac{\ln(1+px)}{x}=\lim \limits_{x\to 0} \frac{xp- \frac{x^2p^2}{2}+\dots  }{x}= \lim \limits_{x\to 0} p- \frac{xp^2}{2}+\dots = p\quad .
\end{equation}

\begin{equation}\label{eqLhospitalExample2}
\lim\limits_{x\to 0}(1+px)^{\frac1x}= \lim\limits_{x\to 0} \left(e^{\ln (1+px)}\right)^{\frac1x} = e^{\lim\limits_{x\to 0}\frac{1}x\ln (1+px)} \stackrel{\eqref{eqLhospitalExample1}}{=}e^{p}\quad .
\end{equation}

We are now in position to prove \eqref{eq(1+x/n)^n=e^x}.
\[
\lim\limits_{n\to \infty} \left(1+\frac{p}{n}\right)^n= \lim_{\substack{n\to \infty \\ x=\frac{1}{n}\\x\to 0}} \left(1+xp\right)^{\frac{1}{x}} \stackrel{\eqref{eqLhospitalExample2}}{=}e^p
\]
\subsubsection{Examples and Exercises}
\begin{problem}
Compute the limits. You may use L'Hospital's rule. For the limits for which $x\to 0$, you are allowed to alternatively use the (not yet proved fact) that whenever $x$ belongs to a small enough neighborhood of 0, analytic functions equal their Maclaurin series (all functions given below are analytic in a small enough neighborhood of 0). You may of course also use any other method that works.
\begin{enumerate}
\item $\lim\limits_{x\to 0} \frac{x^2-1}{x^2-x}$.
\item $\lim\limits_{x\to 0} \frac{\sin 4 x}{\tan 5x}$.
\item $\lim\limits_{x\to 0} \frac{\sin x - x +\frac{x^3}{6}}{x^5}$.
\item $\lim\limits_{x\to \infty} \sqrt{1+x^2} - x$.
\item $\lim\limits_{x\to 0} \frac{\cos n x - \cos mx }{x^2}$.
\item $\lim\limits_{x\to 0} \frac{\arctan x - x+ \frac{x^3}{3} }{(\sin x)^5}$.
\item $\lim\limits_{x\to 1}\left(\frac{x}{x-1}- \frac{1}{\ln x} \right)$.
\end{enumerate}
\end{problem}

\section{Series (non-formal), convergence}\label{secSeriesConvergence}
\subsection{Series, definition and key examples}
Let $\{a_k\}_{k=1}^\infty$ be an infinite sequence. Define $S_n= a_1+a_2+a_3+\dots+a_n$. We say that $S_n$ is the $n^{th}$ partial sum of the sequence $\{a_k\}_{k=0}^\infty$.\index{sum!partial of a sequence}

Let $\{a_k\}_{k=1}^\infty$ be an infinite sequence, and $S_n$ be the partial sum of the sequence as defined above. Provided that the limit $\lim\limits_{n\to\infty} S_n$ exists, we define the infinite sum $\sum\limits_{k=1}^\infty a_k $ as
\begin{equation}\boxed{
\sum_{k=1}^{\infty}a_n\eqdef \lim\limits_{n\to \infty} S_n\quad .
}
\end{equation}
We furthermore say that the infinite sum $\sum\limits_{k=1}^{\infty}a_n$ \emph{converges} (or is \emph{convergent}). If the above limit does not exist, we may still write $\sum\limits_{k=1}^{\infty}a_n$, but we say that the sum \emph{diverges} (or is \emph{divergent}). \index{convergent!series (infinite sum)}\index{divergent!series (infinite sum)}

We have already used infinite series to define $e^x$ in \eqref{eqExponentFunctionDefinition}. We have not proved yet that the series \eqref{eqExponentFunctionDefinition} is convergent (for all $x$). In the coming sections we fix this.

\begin{theorem}\label{thSummandsConvergentSeriesTendToZero}
Suppose $\sum\limits_{k=1}^\infty a_k$ is convergent. Then $\lim\limits_{k\to \infty } a_k=0$.
\end{theorem}

The converse of Theorem \ref{thSummandsConvergentSeriesTendToZero} is not true: for example, $\lim\limits_{n\to \infty} \frac{1}{n}=0$, but
\begin{equation}\label{eqHarmonicSeries}
\sum_{n=1}^{\infty}\frac{1}n= 1+\frac{1}2+\frac13+\frac14+\dots +\frac1n+\dots=\infty\quad .
\end{equation}
To prove the above, let $S_n:=1+\frac{1}2+\dots +\frac{1}n$ denote the $n^{th}$ partial sum. Then we have that
\begin{equation*}
\begin{array}{rcl}
S_{2^n-1}&=&1+\frac{1}2+\frac13 +\dots +\frac{1}{2^n-1}\\
&=& 1+ \underbrace{\left(1/2+ 1/3\right)}_{\mathrm{2~summands}} +\underbrace{(1/4 +1/5+1/6+1/7)}_{\mathrm{4~summands}}+\dots \\
&&+ \underbrace{(1/2^{n-1}+1/(2^{n-1}+1)+\dots +1/(2^{n}-1)}_{2^{n-1}\mathrm{~summands}}\\
&>& 1+ \underbrace{\left(1/4+1/4\right)}_{\mathrm{2~summands}} +\underbrace{(1/8+1/8+1/8+1/8)}_{\mathrm{4~summands}}+\dots \\
&& + \underbrace{(1/2^{n}+\dots +1/2^{n})}_{2^{n-1}\mathrm{~summands}}\\
&=&1+\underbrace{  1/2+\dots +1/2}_{n-1\mathrm{~summands}}= \frac{n+1}2\quad .
\end{array}
\end{equation*}
Therefore $\lim\limits_{n\to \infty} S_{n}=\infty $. The series \eqref{eqHarmonicSeries} is called \emph{harmonic series}.\index{series!harmonic}

Suppose $|z|<1$. Then $\sum_{n=0}^{\infty} z^n$ is convergent and we have
\begin{equation}\boxed{
\label{eqGeometricProgressionSeries}
\frac{1}{1-z}=1+z+z^2+z^3+\dots +z^n+\dots= \sum_{n=0}^{\infty} z^n\quad.
}
\end{equation}
If $|z|\geq 1$ the summand $z^n$ does not tend to $0$ and therefore the \emph{geometric series} \eqref{eqGeometricProgressionSeries} is divergent. \index{series!geometric}

Let us prove \eqref{eqGeometricProgressionSeries}. We have that
\[(1-z)(1+z+z^2+\dots +z^{n})= 1- z^{n-1}\quad .
\]
Therefore for $z\neq 1$ we have that
\[
S_n\eqdef 1+z+z^2+\dots +z^n=\frac{1-z^{n-1}}{1-z}\quad ,
\]
where $S_n$ is the partial sum of the geometric series \eqref{eqGeometricProgressionSeries}. By \eqref{eqLimitRtoTheNth} we know that for $|z|<1$ then $\lim\limits_{n\to \infty} z^n=0$ and therefore
\[
\sum_{n=0}^{\infty}z^n=\lim\limits_{n=0} \frac{1-z^n}{1-z}= \frac{1}{1-z} \quad.
\]

In some high schools students study \emph{periodic decimal notation}\index{periodic!decimal notation} such as, for example, $1.\overline{17}= 1.1717171717\dots$. We are in a position to give a mathematically rigorous definition of this notation; we do that on two examples, and leave the generalization to the reader.
\[
\begin{array}{rcl}
1.\overline{17}&\eqdef&  1.171717\dots\eqdef 1+ \frac{17}{100}+ \frac{17}{100^2}+\dots +\frac{17}{100^n}+\dots\\
&=&1+\frac{17}{100}\left(1+\frac{1}{100}+\frac{1}{100^2}+\dots \right)\stackrel{\eqref{eqGeometricProgressionSeries}}{=} 1+\frac{17}{100}\frac{1}{\left(1-\frac{1}{100}\right)}=1+\frac{17}{\cancel{100}}\frac{\cancel{100}}{100-1}= 1+\frac{17}{99}= \frac{116}{99}\\
0.\overline{9}&\eqdef&  0.99999999\dots \eqdef \frac{9}{10}+ \frac{9}{10^2}+\dots +\frac{9}{10^n}+\dots\\
&=&\frac{9}{10}\left(1+\frac{1}{10}+\frac{1}{10^2}+\dots \right)\stackrel{\eqref{eqGeometricProgressionSeries}}{=} \frac{9}{10}\frac{1}{\left(1-\frac{1}{10}\right)}=\frac{\cancel{9}}{\cancel{10}}\frac{\cancel{10}}{\cancel{9}}= 1\quad .
\end{array}
\]

Compute the sum $\sum\limits_{n=2}^{\infty}\frac{1}{n(n-1)}$. Let $S_k= \sum \limits_{k=2}^{n} \frac{1}{k(k-1)}$ be the partial sum of the series. We have that $\frac{1}{k(k-1)}= \frac{1}{k-1}-\frac{1}k$ and therefore
\[
S_n= \left(\frac11-\cancel{ \frac12}\right)+ \left(\cancel{\frac12} -\cancel{ \frac13} \right) +\dots + \left(\cancel{\frac1{n-2}} -\cancel{\frac{1}{n-1}} \right)+ \left(\cancel{\frac1{n-1}}-\frac1n\right)
=1-\frac{1}n\quad .
\]
Therefore
\[
\sum\limits_{n=2}^{\infty}\frac{1}{n(n-1)}= \lim\limits_{n\to \infty} S_n = \lim \limits_{n \to \infty} 1-\frac{1}n=1\quad .
\]

There is no general technique for summing up arbitrary series ``in closed form''. We can carry out the summation explicitly only for several classes of hand-picked examples.


Suppose $\sum a_n$ and $\sum b_n$ are convergent. Then so is the sum $\sum (ca_n + db_n)$, where $c, d$ are arbitrary constants, and we we have that
\begin{equation}\label{eqLinearCombinationConvergentSeriesIsConvergent}
\sum (ca_n+db_n) = c\sum a_n +d\sum b_n\quad .
\end{equation}
The fact that $\sum a_n$ and $\sum b_n$ are divergent implies nothing about the convergence of $\sum (ca_n+db_n) $. Can you give a very simple example where $\sum a_n$ and $\sum b_n$  are both divergent but $ \sum (a_n+b_n)$ is convergent?

\begin{theorem}\label{thSeriesComparisonTest} (The comparison test)\index{series!comparison test}. Let $\{a_n\}$ and $\{b_n\}$ be non-negative sequences (i.e., $a_n\geq 0$, $b_n\geq 0$).
\begin{itemize}
\item Suppose $b_n\geq a_n$ for all $n$ and $\sum b_n$ is convergent. Then $\sum a_n$ is also convergent.
\item Suppose  $b_n\leq a_n$ for all $n$ and $\sum b_n$ is divergent. Then $\sum a_n$ is also divergent.
\end{itemize}
\end{theorem}

Example. Prove $\sum_{n=0}^{\infty}\frac{1}{2^n+n}$ converges.

\begin{proof}
We have that $\frac{1}{2^n+n}\leq\frac{1}{2^n}$. On the other hand $\sum_{n= 0}^{\infty} \frac{1}{2^n}=2$ and therefore the sum converges.
\end{proof}


\subsection{Series - the integral test}
\begin{theorem} \label{thIntegralConvergenceTest} (The integral test).\index{series!integral convergence test} Suppose $f:[1, \infty)\to \mathbb R$ is a decreasing positive function, i.e., $f(x)\geq 0$ and $f(x)\leq f(y)$ whenever $x<y$. Then the series
\[
\sum_{n=1}^{\infty} f(n)
\]
converges if and only if the improper integral
\[
\int\limits_{x=1}^{\infty}f(x)\diff x
\]
converges.
\end{theorem}
The theorem remains valid if the summation starts at an arbitrary index $a$ if we replace the interval of integration with $[a, \infty)$.

We recall improper integrals were defined in Section \ref{secImproperIntegrals}.

An informal proof of Theorem \ref{thIntegralConvergenceTest} can be given by considering the following picture.
\optionalDisplay{
\psset{xunit=0.6cm,yunit=0.6cm}

\begin{pspicture}(-1,-1)(9.5,6)

\rput(5.5,4){$\underbrace{ \int\limits_{1}^\infty f(x) \diff x}_{\mathrm{region~under~curve}} \geq\underbrace{ \sum\limits_{n=2}^{\infty} f(n)}_{\mathrm{blue~region}}$}

\psaxes[labels=x, ticks=x]{<->}(0,0)(-1,-1)(9,6)

\pscustom[fillstyle=solid, linestyle=none,fillcolor=gray!30]{
\psplot{1}{9}{ 5 x  div}
\psline[linestyle=none](9, 0)(1,0)
}
\psplot[linewidth=1pt]{1}{9}{ 5 x  div}

\multido{\ra=1.0+1.0}{8}{%
\psframe[fillstyle=solid, fillcolor=cyan!40](\ra,0)(! \ra\space 1 add \space 5 \ra\space 1 add div )
}
\end{pspicture}
\begin{pspicture}(-1,-1)(9,6)
\psaxes[labels=x, ticks=x]{<->}(0,0)(-1,-1)(9,6)
\rput(5.5,4){$\underbrace{ \int\limits_{1}^\infty f(x) \diff x}_{\mathrm{region~under~curve}}\leq\underbrace{ \sum\limits_{n=1}^{\infty} f(n)}_{\mathrm{blue~region}}$}

\multido{\ra=1.0+1.0}{8}{%
\psframe[fillstyle=solid, fillcolor=cyan!40](\ra,0)(! \ra\space 1 add \space 5 \ra\space div )
}
\psplot[linewidth=1pt]{1}{9}{ 5 x  div}
\end{pspicture}
} %optionalDisplay

As an exercise to the reader, we suggest he or she write up a formal proof. Where do we use the fact that $f(x)$ is decreasing? Does your proof use the Monotonic sequence Theorem \ref{thMonotonicSequenceThereom} and where?

Let $p\in \mathbb R$. Prove that
\begin{equation}\label{eqSumnToThePthConverges}
\sum\limits_{n=1}^{\infty} \frac{1}{n^p}= \doublebrace{\mathrm{converges}}{\mathrm{for~}p>1}{\mathrm{diverges}}{\mathrm{for~}p\leq 1}\quad .
\end{equation}
\begin{solution}
For $p\geq 0$ we have that $\lim_{n\to \infty} \frac{1}{n^p}$ diverges as $x\to \infty$, and the statement follows from Theorem \ref{thSummandsConvergentSeriesTendToZero}. Suppose now $p< 0$. Define $f(x)\eqdef x^{p}$. Then as $p<0$ we have $f'(x)= px^{p-1}<0$ and so $f$ is decreasing and Theorem \ref{thIntegralConvergenceTest} applies. By \eqref{eqIntegralxTopthPowerToInfty} and \eqref{eqIntegral1overxToToInfty}  the integral $\int\limits_{x=1}^{\infty} x^{-p}\diff x$ converges for $p>1$ and the statement follows.
\end{solution}

Establish whether
\[ \sum\limits_{n=2}^{\infty}\frac{1}{n\ln n}
\]
converges.

\begin{solution}
Consider $\frac{1}{x\ln x}$. We have that $\displaystyle f'(x)= -\frac{1}{x^2\ln x}- \frac{1}{x^2(\ln x)^2}<0$ for $x>2$. Therefore the sum converges if and only if $\int\limits_{x=2}^{\infty}\frac{1}{x\ln x}\diff x$ converges. We have that
\[ \int\limits_{x=2}^{\infty}\frac{1}{x\ln x}\diff x= \int\limits_{x=2}^\infty \frac{1}{\ln x}\diff(\ln x) = \left.\ln (\ln x)\right|_{x=2}^{\infty}= \lim\limits_{x\to \infty} \ln (\ln x ) - \ln (\ln 2)=\infty
\]
and the integral diverges.
\end{solution}

\subsection{Absolute convergence}
\begin{definition}
A series $\sum_{n=1}^\infty a_n$ is defined to be \emph{absolutely convergent} if $\sum_{n=1}^\infty |a_n|$ is convergent. \index{convergent!absolutely convergent series} If a series is convergent but not absolutely convergent, we say that the series is \emph{conditionally convergent}. \index{convergent!condionally convergent series}
\end{definition}

\begin{theorem} \label{thAbsoluteConvergenceImpliesConvergence}
If a series is absolutely convergent then it is also convergent.
\end{theorem}

\begin{proof}
We have that $0\leq a_n+|a_n|\leq 2|a_n|$. By the definition of absolutely convergent series $\sum |a_n|$ is convergent and so is $\sum 2|a_n|$. Therefore by the comparison test $\sum a_n+|a_n|$ is convergent as well. Therefore $\sum a_n= \sum (a_n+|a_n|)-\sum |a_n|$  is convergent by \eqref{eqLinearCombinationConvergentSeriesIsConvergent}.
\end{proof}

Prove that
\[
\sum_{n=1}^{\infty} \frac{\cos n}{n^2}
\]
is convergent.

\begin{solution}
$\sum\limits_{n=1}^{\infty} \frac{|\cos n |}{n^2} \leq \sum\limits_{n=1}\frac{1}{n^2}$. The latter is convergent by \eqref{eqSumnToThePthConverges}. As $\frac{|\cos n|}{n^2}\leq \frac{1}{n^2}$ the statement follows from Theorem \ref{thSeriesComparisonTest}.
\end{solution}

\begin{theorem}\label{thSeriesTwoSeriesRatioTest}
(The limit comparison test)\index{series!limit comparison test}. Suppose $a_n\geq 0$ and $b_n\geq 0$ for all $n$. Then if
\[\lim\limits_{n\to \infty} \frac{a_n}{b_n}= c
\]
for some number $c$, then either both series $\sum a_n$ and $\sum b_n $ converge or both diverge.
\end{theorem}

\begin{theorem}\label{thRatioTest}(The ratio test). \index{series!ratio test} Suppose $a_n\neq 0$ for $n\geq 2$.
\begin{itemize}
\item If $\lim\limits_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|= L<1$ exists then the series $\sum\limits_{n=1}^\infty a_n$ is absolutely convergent.
\item If $\lim\limits_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|= L>1$ exists or $\lim\limits_{n\to\infty} \left|\frac{a_{n+1}}{a_{n}}\right| =\infty$, then the series $\sum\limits_{n=1}^\infty a_n$ is divergent.
\end{itemize}
\end{theorem}
Note that if $\lim\limits_{n\to\infty}\left|\frac{a_n}{a_{n+1}}\right|=1$, we cannot use the above theorem to deduce anything about the convergence of $\sum a_n$.

Note that the root test supposes that the limit $\lim\limits_{n\to\infty} \left|\frac{a_{n+1}}{a_{n}}\right| =L$ exists. This is a potential pitfall. To illustrate why, suppose that $\lim\limits_{n\to\infty} \left|\frac{a_{n+1}}{a_{n}}\right| =L<1$ exists. Now define $b_n:=\doublebrace{a_n}{\mathrm{if~}n~\mathrm{is~even}}{0}{\mathrm{otherwise}}$. Then $\lim \limits_{n\to\infty} \left|\frac{b_{n+1}}{b_{n}}\right|$ is not defined for even $n$ and equals 0 for odd $n$. At the same time, as $\sum |b_n|<\sum |a_n|$, the series $\sum b_n$ is absolutely convergent (comparison test) and therefore convergent. In this way $\sum b_n$ is a convergent series for which the ratio test does not apply. However, we were able to prove that $\sum b_n$ converges with a combination of the ratio test and careful use of inequalities (comparison test).

\begin{proof}[ of Theorem \ref{thRatioTest}]
Suppose first $\lim\limits_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|= L<1 $. Let $M$ be an arbitrary number with $L<M<1$ (for example, $M=\frac{1+L}2$ works). As $\lim\limits_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|=L$ for sufficiently large $n$ we have that $\left|\frac{a_{n+1}}{a_{n}} \right|< M$. More precisely, there exists an index $N$ such that for all $n\geq N$ we have that
\[
|a_{n+1}|< Ma_{n}\quad .
\]
Therefore
\[
|a_{N+k}|<M |a_{N+k-1}|< M^2 |a_{N+k-2}|<\dots <M^k |a_N|\quad .
\]
Therefore
\[
\sum_{n\geq N} |a_{n}|<\sum_{k=0}^{\infty} |a_N|M^{k}= \frac{|a_N|}{1-M}\quad ,
\]
where the last equality was proved in \eqref{eqGeometricProgressionSeries} using $0<M<1$. Our criterion now follows from Theorem \ref{thSeriesComparisonTest} and the fact that the finitely many summands $a_1, \dots, a_{N-1}$ do not affect the convergence of the series.

Suppose next $\lim\limits_{n\to\infty}\left|\frac{a_{n+1}}{a_{n}}\right|= L>1 $. We note that the proof is very similar to the preceding one, only inequalities must be reversed.

Let $M$ be an arbitrary number with $L>M>1$ (for example, $M=\frac{1+L}2$ works). As $\lim\limits_{n\to\infty} \left| \frac{a_{n+1}}{a_{n}} \right|=L$ for sufficiently large $n$ we have that $\left|\frac{a_{n+1}}{a_{n}} \right|> M$. More precisely, there exists an index $N$ such that for all $n\geq N$ we have that
\[
|a_{n+1}|> Ma_{n}\quad .
\]
Therefore
\[
|a_{N+k}|>M |a_{N+k-1}|> M^2 |a_{N+k-2}|>\dots >M^k |a_N|\quad .
\]
Therefore
\[
\sum_{n\geq N} |a_{n}|>\sum_{k=0}^{\infty} |a_N|M^{k}= \lim_{k\to \infty} \frac{|a_N|(M^k-1)}{M-1}=\infty.
\]
Our criterion now follows from Theorem \ref{thSeriesComparisonTest}.
\end{proof}

\index{exponent function} It is time to fix a debt long due: we will now prove
\[
e^x\eqdef \sum_{n=0}^\infty \frac{x^n}{n!}
\]
defined in \ref{eqExponentFunctionDefinition} is convergent for all $x$.

We are using the ratio test \ref{thRatioTest}. For $x=0$, the series is equal to 1, and we have nothing to prove; suppose $x\neq 0$.  We have that
\[
\frac{a_{n+1}}{a_{n}}= \frac{\frac{x^{n+1}}{(n+1)!}}{\frac{x^{n}}{n!}} = \frac{x}{n+1}\quad .
\]
Now $\lim\limits_{n\to \infty} \frac{a_{n+1}}{a_{n}}= \lim\limits_{n\to \infty} \frac{x}{n+1}=0$.

\begin{theorem}\label{thRootTest}(The root test). \index{series!root test}
\begin{itemize}
\item If $\lim\limits_{n\to\infty}\sqrt[n]{|a_n|}= L<1$, then the series $\sum\limits_{n=1}^\infty a_n$ is absolutely convergent.
\item If $\lim\limits_{n\to\infty}\sqrt[n]{|a_n|}= L>1$ or $\lim\limits_{n\to\infty}\sqrt[n]{|a_n|}=\infty$, then the series $\sum\limits_{n=1}^\infty a_n$ is divergent.
\end{itemize}
\end{theorem}
Note again that if $\lim\limits_{n\to\infty}\sqrt[n]{|a_n|}= 1$, we cannot use the above theorem to deduce anything about the convergence of $\sum a_n$.
\subsection{Radius of convergence, convergence of Taylor and Maclaurin series}\label{secTaylorSeriesRadiusConvergence}
In section \ref{secFormalPowerSeries}, we discussed formal power series $\sum a_n x^n$, where $x$ is an indeterminate. We saw that formal power series share the same notation as (non-formal) power series, only the notion of convergence does not apply to them. In this section, we discuss the convergence of the series obtained by substituting a number in place of the indeterminate $x$. In other words, in this section we treat the indeterminate of a formal power series as a number.

\begin{theorem}\label{thRadiusConvergence} Let $\sum\limits_{n=0}^\infty a_n x^n$ be non-formal power series (i.e., $x$ is a number, rather than indeterminate). Then one of the three mutually exclusive possibilities holds.
\begin{itemize}
\item The series converges only when $x=0$.
\item The series is absolutely convergent for all $x$.
\item There is a positive number $R$ such that the series is absolutely convergent whenever $|x|<R$ and diverges whenever $|x|>R$.
\end{itemize}
\end{theorem}
This theorem follows from the ratio test, the comparison test and the fact that absolute convergence implies convergence. We do not present the proof, however an attentive student should be able to carry out the proof on his/her own.

\begin{definition}\label{defRadiusConvergence} \index{radius of convergence}
If the third alternative of Theorem \ref{thRadiusConvergence} holds, the number $R$ is called \emph{radius of convergence} of $\sum\limits_{n=0}^\infty a_n x^n$. If the series converges only for $x=0$, we say that the radius of convergence is $0$, and if the series converges for all $x$, we say that the radius of convergence is $\infty$.
\end{definition}

Consider the power series $\sum\limits_{n=0}^\infty a_n x^n$ with radius of convergence $R$.  Suppose $\lim\limits_{n\to\infty} \frac{a_{n+1}}{a_n}$ is well defined. By the ratio test (Theorem \ref{thRatioTest}), we have that  $\sum\limits_{n=0}^\infty a_n x^n$ converges whenever $\lim\limits_{n\to\infty} \left|\frac{xa_{n+1}}{a_n}\right|<1$ and diverges whenever $\lim\limits_{n\to\infty} \left|\frac{xa_{n+1}}{a_n}\right|>1$. On the other hand, by the definition of radius of convergence, $\sum\limits_{n=0}^\infty a_n x^n$ converges whenever $|x|<R$ and diverges whenever $|x|>R$. Therefore
\[
\lim\limits_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right|=\frac{1}{R}\quad,
\]
whenever the above limit is defined.

Theorem \ref{thRadiusConvergence} holds as stated for complex numbers $x\in \mathbb C$, (the absolute value of a complex number was defined in \ref{secComplexNumbers}), with an almost identical proof. We challenge the reader to attempt such a proof on his/her own.

The complex numbers $x$ for which $|x|<R$ form a disk of radius $R$, which additionally motivates the use of the word ``radius''.

We note that Theorem \ref{thRadiusConvergence} says nothing about the convergence of $\sum a_n x^n$ when $|x|=R$. In fact the series $\sum a_n x^n$ may converge for all complex $x$ with $|x|=R$ (even though it diverges for all $x$ with $|x|>R$), it may converge for no values of $x$ for which $x=|R|$, or it may converge for some $x$ with $x=|R|$ and fail to converge for other $x$ with $|x|=R$. An example of the first case is given by $\sum\limits_{n=1}^\infty  \frac{x^n}{n^2}$, of the second case by  $\sum\limits_{n=0}^\infty  nx^n$, and of the third case by $\sum\limits_{n=1}^\infty  \frac{x^n}{n} $ (what is the radius of convergence in each case?).

We recall the Taylor of $f(x)$ at $p$ and the Maclaurin series of $f(x)$ (=Taylor series of $f(x)$ at 0) from Definitions \ref{defMaclaurinSeries} and \ref{defTaylorSeries}:
\[
\maclaurin (f(x))= \sum\limits _{n=0}^\infty \frac{f^{(n)}(0)}{n!}x^n\quad\quad\quad \taylor_p(f(x))=\sum\limits_{n=0}^\infty \frac{f^{(n)}(p)}{n!}(x-p)^n\quad .
\]

\begin{theorem}\label{thDifferentiatingIntegratingPowerSeriesNonFormal}
Suppose the power series $\sum\limits_{n=0}^\infty a_n x^n$ has radius of convergence $R$, and for $|x|<R$, let $f(x):=\sum a_n x^n$.
\begin{itemize}
\item[(a)] $\sum\limits_{n=0}^\infty a_n n x^{n-1}$ and $\sum\limits_{n=0}^\infty a_n\frac{x^{n+1}}{n+1}$ have the same radius of convergence $x=R$ as the starting series.
\item[(b)] $f'(x)=\sum\limits_{n=0}^\infty a_n n x^{n-1}$ and $\displaystyle\int f(x)\diff x= C+\sum\limits_{n=0}^\infty a_n\frac{x^{n+1}}{n+1} $.
\end{itemize}
\end{theorem}
In other words, this theorem proves that taking Maclaurin series and then differentiating is the same as first differentiating and then taking Maclaurin series.

Theorem \ref{thDifferentiatingIntegratingPowerSeriesNonFormal} (a) can be proved using the ratio test. The pitfalls of the ratio test described in the remarks after \ref{thRatioTest} make a formal proof somewhat tedious (although certainly within reach of a careful student). While we propose such a proof as an exercise to the more motivated students, we shall argue the validity of  Theorem \ref{thDifferentiatingIntegratingPowerSeriesNonFormal} only on condition that $\lim\limits_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right|$ is well defined and exists. Indeed, in the remarks after the definition of radius of convergence, we showed that if $\lim\limits_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right|$ exists it necessarily equals $1/R$. At the same time, $\lim\limits_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right| = \lim\limits_{n\to\infty} \left|\frac{a_{n+1}(n+1)}{a_n n}\right|= \lim\limits_{n\to\infty} \left|\frac{a_{n+1}n}{a_n (n+1)}\right|$ and (a) follows by the ratio test.

\begin{proof}[ of Theorem \ref{thDifferentiatingIntegratingPowerSeriesNonFormal}(b)]
By the definition of $f'(x)$, for $x<R$, we have that
\begin{equation}\label{eqThDifferentiatingPowerSeriesProofeq1}
\begin{array}{rcl}
f'(x)&=&\displaystyle\lim\limits_{\substack{h\to 0\\|x+h|<R}} \frac{f(x+h)-f(x)}{h} = \lim\limits_{\substack{h\to 0\\|x+h|<R}} \frac{\sum\limits_{n=0}^{\infty} a_n\left( (x+h)^n-x^n \right) }{h} \\
&=& \displaystyle\lim\limits_{\substack{h\to 0\\|x+h|<R}} \frac{\sum\limits_{n=0}^{\infty} a_n\left( \cancel {x^n}+ hx^{n-1}n+ h^2(\binom{n}{2} x^{n-2}+\binom{n}{3} x^{n-3}h+\dots +h^{n-2})-  \cancel{x^n}\right) }{h}
\\
&=&\displaystyle\lim\limits_{\substack{h\to 0\\|x+h|<R}} \sum\limits_{n=0}^{\infty}\left(na_n x^{n-1}  +ha_n\left(\binom{n}{2} x^{n-2}+h\binom{n}{3}x^{n-3}h+\dots +h^{n-2}\right) \right)\\
&=&\sum\limits_{n=0}^{\infty}na_n x^{n-1}  +\displaystyle\lim\limits_{\substack{h\to 0\\|x+h|<R}}h\sum\limits_{n=0}^{\infty} a_n\left(\binom{n}{2} x^{n-2}+h\binom{n}{3}x^{n-3}h+\dots +h^{n-2}\right)
\end{array}
\end{equation}
where in the very last equality we use the fact that by (a), the series $\sum\limits_{n=0}^{\infty}\left(na_n x^{n-1}\right)$ is convergent and therefore we can split the series in two summands. Recall from section \ref{secNewtonBinomialReview} that $\binom{n }{k}= \frac{n(n-1)\dots (n-k+1)}{k!}$ and therefore for $n\in \mathbb Z_{\geq 0}$ we have $n^2\binom{n-2}{k}\geq\binom{n}{k+2}$. As $|x|<R$, for small enough $|h|$ we have that $|x|+|h|<\frac{|x|+R}{2}$. Assume $|x|+|h|<\frac{|x|+R}{2}$. Therefore
\[\begin{array}{rcl}
\left|\binom{n}{2} x^{n-2}+h\binom{n}{3}x^{n-3}h+\dots +h^{n-2}\right|&\leq&  n^2\left( |x|^{n-2}+
\binom{n-2}{1}|h||x|^{n-3}|h|+\dots+\binom{n-2}{k}|h|^kx^{n-2-k} \dots +|h|^{n-2}\right) \\
&=&n^2 (|x|+|h|)^{n-2}<\left(\frac{|x|+R}{2}\right)^{n-2}\quad .
\end{array}
\]
By applying (a) twice we have that $\sum\limits_{n=0}^{\infty} n^2a_n\left(\frac{|x|+R}{2}\right)^{n-2} $ is convergent and equals some number $M$. Therefore by the comparison test, so is
\[
\sum\limits_{n=0}^{\infty} a_n\left(\binom{n}{2} x^{n-2}+h\binom{n}{3}x^{n-3}h+\dots +h^{n-2}\right)\leq \sum\limits_{n=0}^{\infty} n^2a_n\left(\frac{|x|+R}{2}\right)^{n-2}=M\quad .
\] Therefore
\[
\left|h\sum\limits_{n=0}^{\infty} a_n\left(\binom{n}{2} x^{n-2}+h\binom{n}{3}x^{n-3}h+\dots +h^{n-2}\right)\right|< h M
\] and therefore the limit on the last line of \eqref{eqThDifferentiatingPowerSeriesProofeq1} equals 0.

This proves that $f'(x)=\sum\limits_{n=0}^\infty a_n n x^{n-1}$. The fact that $\displaystyle\int f(x)\diff x= C+\sum\limits_{n=0}^\infty a_n\frac{x^{n+1}}{n+1} $ now follows immediately: we already proved that we can differentiate the right hand side one term at a time, and the result is the left hand side by direct computation.
\end{proof}

\begin{theorem}[Taylor's inequality] Let $f$ be an $(N+1)$ times differentiable function. Suppose for $|x-a|<d$, $f^{(N+1)}(x)\leq M$. Then
\[
\left|f(x)-\sum\limits_{n=0}^N \frac{f^{(n)}(a)}{n!}(x-a)^n\right| \leq \frac{M}{(n+1)!} |x-a|^{n+1}
\]
for $|x-a|\leq d$.
\end{theorem}
\begin{proof}
For $|t-a|< d$ we have
\[
f^{(N+1)}(t) \leq  M\quad.
\]
Apply $\int\limits_{a}^{x} \bullet \diff t $. We apply the fundamental theorem of calculus and replace $x$ by $t$ to get
\[
f^{(N)}(t)-f^{(N)}(a)\leq (t-a) M
\]
or
\[
f^{(N)}(t)\leq f^{(N)}(a)+(t-a) M
\]
Apply again $\int\limits_{a}^{x} \bullet \diff t $ and replace $x$ by $t$ to get
\[
f^{(N-1)}(t)\leq f^{N-1}(a) +f^{(N)}(a)(t-a)+ M\frac{(t-a)^2}2\quad .
\]
Suppose by induction we have proved that
\[
f^{(N-k)}(t)\leq f^{N-k}(a) +f^{(N-k+1)}(a)(t-a)+ f^{N-k+2}\frac{(t-a)^2}2+\dots +f^{N}\frac{(t-a)^{k}}{k!}+ M\frac{(t-a)^{k+1}}{(k+1)!} \quad .
\]
We apply $\int\limits_{a}^{x} \bullet \diff t $ and replace $x$ by $t$ to get
\[
f^{(N-k-1)}(t)\leq f^{N-k-1}(a) +f^{(N-k)}(a)(t-a)+ f^{N-k+1}\frac{(t-a)^2}2+\dots +f^{N}\frac{(t-a)^{k+1}}{(k+1)!}+ M\frac{(t-a)^{k+2}}{(k+2)!} \quad .
\]
In this way by induction we get that
\[
\left(f(x)-\sum\limits_{n=0}^N \frac{f^{(n)}(a)}{n!}(x-a)^n\right)\leq \frac{M}{(n+1)!} |x-a|^{n+1} \quad.
\]
In an analogous fashion we can prove that
\[
\left(f(x)-\sum\limits_{n=0}^N \frac{f^{(n)}(a)}{n!}(x-a)^n\right)\geq -\frac{M}{(n+1)!} |x-a|^{n+1} \quad.
\]
which completes the proof of the theorem.
\end{proof}



\chapter{Applications of integration}
\section{Volumes of solids of revolution}
A solid of revolution is the set of points obtained by rotating a 2-dimensional set of points lying inside a plane $P$ around an axis lying in the plane $P$.

Example. \label{itemExampleFigureRotationalSolid}
%\multido{\rA=\deltaX+\deltaX}{10}{%
%\pstIIIDCylinder[fillstyle=none, linecolor=gray, increment=20, fillcolor=gray](0,0,0){\rA\space \deltaX\space add}{\rA\space sqrt}
%\pstIIIDCylinder[fillstyle=none, linecolor=gray, increment=20, fillcolor=gray](0,0,0){\rA}{\rA\space sqrt}
%}%

%\psSolid[action=draw*,h={\rA\space\rA\space mul},r1={\rA\space\deltaX\space add}, r0={\rA},object=anneau](0,0,0)

\optionalDisplay{

\psset{xunit=1.5cm,yunit=1.5cm}
\begin{pspicture*}(-0.3,-0.5)(4,4.5)
%\psset{lightsrc=80 30 30,viewpoint=100 45 30 rtp2xyz,Decran=110,linewidth=0.2pt}
\psline[linecolor=gray]{->}(0,0)(3.5,0) % x-axis
\psline[linecolor=gray]{->}(0,0)(0,3.5) % z-axis
\rput[l](3.5,0){$x$}
\rput[b](0,3.5){$z$}
\pscurve[linecolor=red]{->}(0.1,3)(0.2, 3.1)(0, 3.2)(-0.2,3.1 )(-0.05,3)
\pscustom[linewidth=2pt, linecolor=blue, fillstyle=solid,fillcolor=lightgray]{
\psplot{3}{0.3}{1 x div}
\psplot{0.3}{3}{2.71828 -1 x mul exp}
\closepath
%\pscurve(0.3, 3.33) (0.3, 1.349858808)
}
\pscustom[linewidth=0.5pt, linecolor=blue, fillstyle=solid,fillcolor=red]{
\psline(2, 0.135335283)(2.1,0.135335283)(2.1,0.5)(2,0.5)
\closepath
%\pscurve(0.3, 3.33) (0.3, 1.349858808)
}
\rput(0.3,0){\psline(0,-0.03)(0,0.03)}
\rput(3,0){\psline(0,-0.03)(0,0.03)}
\rput[t](0.3,-0.1){$a$}
\rput[t](3,-0.1){$b$}
\rput(1.6,1){$z=f_2(x)$}
\rput[t](0.6,0.3){$z=f_1(x)$}
\end{pspicture*}
\psset{xunit=1cm,yunit=1cm}
\begin{pspicture*}(-4,-3)(4,4.5)
%\psset{lightsrc=80 30 30,viewpoint=100 45 30 rtp2xyz,Decran=110,linewidth=0.2pt}
\psset{lightsrc=50 20 20,viewpoint=100 45 30 rtp2xyz,Decran=110,linewidth=0.2pt}
\newcommand{\deltaXcustom}{0.1}
\newcommand{\rB}{3.3\space\rA\space sub}
\newcommand{\innerCycleX}{
%\pstIIIDCylinder[fillstyle=solid, linecolor=gray, increment=30, fillcolor=green]
\pstThreeDEllipse[fillstyle=solid, fillcolor=lightgray](0,0,{1\space\rB\space div})(\rB\space\deltaXcustom\space add,0,0)(0,\rB\space\deltaXcustom\space add,0)
\pstThreeDEllipse[fillstyle=solid](0,0,{1\space\rB\space div})(\rB,0,0)(0,\rB,0)
%
\pstIIIDCylinder[fillstyle=none, linecolor=gray, increment=30, fillcolor=green]
(0,0,{2.71828\space -1\space \rB\space mul\space exp})
{\rB\space \deltaXcustom\space add}
{1 \space \rB\space div\space 2.71828\space -1\space \rB\space mul\space exp \space sub }
\pstIIIDCylinder[fillstyle=none, linecolor=gray, increment=30, fillcolor=white]
(0,0,{2.71828\space -1\space \rB\space mul\space exp})
{\rB}
{1 \space \rB\space div\space 2.71828\space -1\space \rB\space mul\space exp \space sub }
}
\multido{\rA=0.3+\deltaXcustom}{14}{%
\innerCycleX
}%
\pstThreeDEllipse[fillstyle=solid, fillcolor=red](0,0,{1\space 2\space div})( 2\space\deltaXcustom\space add,0,0)(0,2\space\deltaXcustom\space add,0)
\pstThreeDEllipse[fillstyle=solid, fillcolor=white](0,0,{1\space 2\space div})(2,0,0)(0,2,0)
\pstIIIDCylinder[fillstyle=none, linecolor=red, increment=1, fillcolor=green]
(0,0,{2.71828\space -1\space 2\space mul\space exp})
{2\space \deltaXcustom\space add}
{1 \space 2\space div\space 2.71828\space -1\space 2\space mul\space exp \space sub }
\pstIIIDCylinder[fillstyle=none, linecolor=red, increment=3, fillcolor=white]
(0,0,{2.71828\space -1\space 2\space mul\space exp})
{2}
{1 \space 2\space div\space 2.71828\space -1\space 2\space mul\space exp \space sub }
%
\multido{\rA=1.4+\deltaXcustom}{17}{%
\innerCycleX
}%
\defFunction[algebraic]%
{etoMinusT}(t)
{t}{0 }{e^(-t)}
\psSolid[object=courbe,r=0,
range=0.3 2.8,
linecolor=blue,
linewidth=0.03,
resolution=360,
function=etoMinusT]%
\defFunction[algebraic]%
{oneOverT}(t)
{t}{0 }{1/t}
\psSolid[object=courbe,r=0,
range=0.3 2.8,
linecolor=blue,
linewidth=0.03,
resolution=360,
function=oneOverT]%

\psSolid[object=line, linecolor=blue,
args=0.3 0 0.740818221 0.3 0 3.33]
\psSolid[object=line, linecolor=blue,
args=2.8 0 0.060810063 2.8 0 0.357142857]
\axesIIID[](-1,-1,-1)(4,4,4)
\end{pspicture*}
} %optionalDisplay end
For the purposes of these lectures, we introduce the notion of a ``curved trapezoid''. A figure in the $x,z$ plane is called a ``curved trapezoid'' if it is enclosed by two functions $f_1(x), f_2(x)$ in the interval $[a,b]$ with $f_1(x)<f_2(x)$ as $x$ varies in the interval $[a,b]$.

In other words, a ``curved trapezoid'' is a figure that can be given as the set of points $(x,z)$ for which $x\in [a, b]$ , $z\in [f_1(x), f_2(x)]$. A curved trapezoid is drawn in the preceding item.

The term ``curved trapezoid'' is not standard, and we use it only for the purpose of the current section.

Every ``well behaved'' figure can be subdivided into curved trapezoids. In our course, we shall only consider such ``well-behaved'' figures.

Let $F$ be the solid of revolution obtained by rotating a curved trapezoid $x\in[a,b], z\in [f_1(x), f_2(x)]$, $f_1(x)<f_2(x)$ around the $z$ axis, as shown on the preceding figure. Then the volume of $F$ is computed by

\begin{equation}\boxed{\label{eqVolumeRotationalSolid}
volume(F) = \int\limits_{a}^b 2\pi x (\underbrace{f_1(x)-f_2(x)}_{=: h(x)}) \diff x=  \int\limits_{a}^b 2\pi x h(x) \diff x\quad .
}
\end{equation}

A motivation for the above computation may be given as follows. The volume of the ``cylindrical shell'' figure enclosed between an inner cylinder of radius $r$ and an outer cylinder of radius $r+\Delta$ and height $h$ equals the difference of the volumes of the two cylinders
\[
h(\pi (r+\Delta)^2-\pi r^2)=h\pi (\cancel{r^2}+2r\Delta+\Delta^2- \cancel{ r^2}) = h\pi (2r\Delta+\Delta^2)\quad .
\]

\optionalDisplay{
\psset{xunit=2cm,yunit=2cm}
\psset{lightsrc=50 20 20,viewpoint=100 45 30 rtp2xyz,Decran=110,linewidth=0.2pt}

\begin{pspicture*}(-1,-1)(1,1)
\defFunction[algebraic]{innerC}(t)
{cos(t)}{sin(t)}{}
\defFunction[algebraic]{outerC}(t)
{1.1*cos(t)}{1.1*sin(t)}{}
\psSolid[object=prisme,h=0.6,
fillcolor=red,
resolution=30,
base=0 6.3 {outerC} CourbeR2+
6.3  0 {innerC} CourbeR2+](0,0,-0.6)
\psline[linecolor=black, linewidth=0.3pt, arrows={|-|}] (0,0.4) (0.55,0.4)
\psline[linecolor=black, linewidth=0.3pt, arrows={|-|}] (0,0.7) (0.607,0.7)
\rput(0.25, 0.52 ){\tiny $r$}
\rput(0.30, 0.82 ){\tiny $r+\Delta$}
\psline[linecolor=black, linewidth=0.3pt, arrows={|-|}] (0.7,0) (0.7,-0.3)
\rput[l](0.75, -0.15 ){\tiny $h$}
\end{pspicture*}
}

Let $N$ be a large integer and set $\Delta:= \frac{b-a}{N}$. Split the interval $[a,b]$ into $N$ equal segments $[a,a+\Delta], [a+ \Delta(x), a+ 2\Delta] ,\dots, [b-\Delta, b]$. The length of each segment is $\Delta$. Then the volume of our solid of revolution is approximated by the sum of the volumes of the cylindrical shells of inner radius $a+ k\Delta $, outer radius $a+(k+1)\Delta$, and height $h(a+k\Delta)$, where we have set
\[
h(x)\eqdef f_2(x)-f_1(x)\quad .
\]
In other words, the volume of a solid of revolution is approximated by
\[
\begin{array}{l}
\displaystyle\sum_{k=0}^{k=N-1} \underbrace{ \underbrace{h(a+k\Delta)}_{\mathrm{height}}(\underbrace{\pi (\underbrace{(a+k\Delta)+\Delta}_{ \mathrm{rad.~outer~cylinder}} ) ^2}_{\mathrm{area~base~outer~cyl.}}-\underbrace{\pi (\underbrace{a+k \Delta}_{\mathrm{rad.~inner~c.}})^2}_{\mathrm{area~base~inner~c.}})}_{\mathrm{volume~k^{th}~cylindrical~shell}} \\
=\displaystyle \underbrace{\sum_{k=0}^{k=N-1} 2\pi(a+k\Delta)  h(a+k\Delta) \Delta }_{\mathrm{approximates~} \int\limits_{a}^b 2\pi x h(x) \diff x }  + \Delta \left(\underbrace{\sum_{k=0}^{k=N-1} \pi  h(a+k\Delta)\Delta}_{\mathrm{approximates~} \int\limits_{a}^b h(x) \diff x }\right)
\end{array}
\]
As we subdivide into finer intervals, i.e., $N$ tends to zero, $\Delta$ (as a function of $N$) tends to zero, and the first summands tends to $\int\limits_{a}^b 2\pi x h(x) \diff x $. On the other hand, the second multiplicand of the second summand approximates $\int\limits_{a}^b h(x) \diff x $, in particular it is bounded. Therefore the entire second summand equals $\Delta$ times a bounded quantity, and therefore approaches zero as $\Delta$ tends to zero.


\textbf{Remark.} The above motivation may be regarded as a proof of the volume formula. The main reason we do not call the above motivation ``a proof'' is that we have not given a definition of volume. \textbf{Optional material, (!).} Here is an informal brief sketch  of a definition of volume. If a figure $X$ can be represented as a set of the form $X= \{(x,y,z)| x\in [a,b], y\in [f_1(x), f_2(x)], z\in [g_1(x,y),  g_2(x,y)]\}$, its volume can be defined as
\[
\int\limits_a^b \left(\int\limits_{f_1(x)}^{f_2(x)}\left(\int\limits_{g_1(x,y)}^{g_2(x,y)} 1 ~ \diff s \right)\diff t \right)dv \quad .
\]
By allowing finite sums of figures of the above kind, and by allowing subdivision of figures into components of the above kind, we can define volume of a much wider set of figures. This set of figures includes all 3-dimensional figures of practical significance to  the natural sciences.

However, there are a lot of important details to take care of - for example, what do we do if the figures of the above form intersect along a (2-dimensional) wall, and how do these integrals behave when we re-parametrize the coordinates system. In order to handle these details smoothly, as well as to define volume of figures that cannot be represented with finitely many operations by  figures of the above form,  we need to define volume as follows. (Disclaimer: non-defined mathematical terms follow. The terms are given as starting keywords for students who have additional interest in the subject and would like to google them.) The volume of a \emph{measurable} figure $X$ equals the \emph{infimium of the of the sums of volumes of (possibly infinitely many) rectangular parallelepipeds whose union covers $X$}. The words in the preceding sentence express the simplest idea of measuring volume: namely, approximate volume by increasingly finer rectangular parallelepipeds. However, molding this simple idea into a precise mathematical definition, although not difficult or conceptually hard (however somewhat lengthy, as the preceding ``complicated'' words indicate), is outside of the scope of the current course. We note that one area of mathematics in which a precise definition of volume is given is ``Measure Theory''.
\subsubsection{Strategy for computing the volume of a solid of revolution}

A typical problem for finding the volume of a solid of revolution will ask you to find the volume of a figure obtained by rotating a figure in the $(x,z)$ plane around an axis. The actual letters in the problem might be different from $(x,z)$. The axis of rotation might be the $x$-axis, the $z$-axis, or any other axis given by the equation of any line. The figure in the $(x,z)$ plane will usually be given as the figure enclosed by two or more curves in the plane.

If the axis of rotation does not pass through $(0,0)$, we should change the coordinate system so that the axis of rotation passes through zero. This is easy to do. Suppose the axis of rotation passes through some point $(s,t)$. Then if the change the coordinate system by setting $u\eqdef x-s, v\eqdef z-t$, then when $(x,z)=(s,t)$, we would have that $(u,v)=(0,0)$. In other words, we may replace everywhere $x=u+s$, $z=v+t$, and work with the new coordinates $(u,v)$.

Suppose now the axis of rotation passes through $(0,0)$, but is neither the $x$ nor the $z$ axis. Suppose in addition the axis of rotation passes through the point $(a,b)$, $a\neq 0$. Then the axis of rotation passes through the point $(\frac{a}{\sqrt{a^2+b^2}},\frac{b}{\sqrt{a^2+b^2}})$ as well (this is one of the two points on the line whose distance to $(0,0)$ is one). Then we can change the coordinate system by
\begin{equation}\label{eqRotateXZtoUV}
\begin{array}{rcl}
u &=&\frac{1}{\sqrt{a^2+b^2}}\left( bx-az\right)\\
v &=& \frac{1}{\sqrt{a^2+b^2}}\left( ax+bz\right)\quad .
\end{array}
\end{equation}
If $(x,y)= (\frac{a}{\sqrt{a^2+b^2}}, \frac{b}{ \sqrt{a^2 +b^2}})$, it follows that $(u,v)=(0,1)$, and so the axis of rotation passes through the point $(0,1)$ in the $(u,v)$-coordinates, i.e., the axis of rotation is the $v$-axis. It is important that the substitution \eqref{eqRotateXZtoUV} preserves volumes. This does not happen for all linear substitutions (transformations); the linear transformations for which this happens are called ``volume preserving''.

One way to motivate the fact that substitution \eqref{eqRotateXZtoUV} preserves volumes (without actually proving it), is to show that the transformation \eqref{eqRotateXZtoUV} is a rotation. The lengths $\sqrt{a^2+b^2}$, $|a|$ and $|b|$ form a right-angle triangle, and therefore $\frac{a}{\sqrt{a^2+b^2}}$ is the cosine of the angle $\varphi$ indicated on the figure below and $\frac{b}{\sqrt{a^2+b^2}}$ is the sine. If we set $\mathbf e_1\eqdef(1,0)$, $\mathbf e_2\eqdef(0,1)$, then a vector in the $(x,z)$ plane is given by $x\mathbf e_1+z\mathbf e_2$. If we set $\mathbf v_1\eqdef (\cos\varphi, -\sin \varphi), \mathbf v_2\eqdef (\sin \varphi, \cos \varphi)$, then a vector in the plane may be alternatively given as $u\mathbf v_1+ v\mathbf v_2$. Therefore if we want to change coordinate system so that the axes are aligned along $\mathbf v_1, \mathbf v_2$, we need to have that
\[
(x, z)=x\mathbf e_1+z\mathbf e_2= u \mathbf v_1 +v \mathbf v_2=( u\cos \varphi+ v\sin\varphi, -u\sin \varphi+v\cos\varphi)\quad ,
\]
and therefore
\[
\left|\begin{array}{rcl}
x&=& u\cos \varphi+ v\sin\varphi\\
z&=&  -u\sin \varphi+v\cos\varphi\quad .
\end{array}\right.
\]
Solving this system for $u,v$, we get that
\[
\left|\begin{array}{rcl}
u&=&  x\cos \varphi-z\sin\varphi\\
v&=& x\sin \varphi+z\cos\varphi\quad .
\end{array}\right.
\]
\optionalDisplay{
\psset{xunit=3cm,yunit=3cm}
\begin{pspicture*}(-2,-2)(3,2)
\psline[linecolor=gray]{->}(-1.5,0)(1.5,0) % x-axis
\psline[linecolor=gray]{->}(0,-1.5)(0,1.5) % y-axis
\rput[l](1.5,0){$x$}
\rput[b](0,1.5){$z$}
\rput[bl](0.03,1.03){$(0,1)$}
\rput[bl](1,-0.15){$(1,0)$}
\rput[c](0,1){$\bullet$}
\rput[c](1,0){$\bullet$}

\psplot[linestyle=dotted, arrows={<-}]{0}{0.5}{1 x x mul sub sqrt}
\psplot[linestyle=dotted, arrows={->}]{0.866}{1}{1 x x mul sub sqrt -1 mul}

\psplot[linecolor=red]{0}{0.05}{0.01000001 x x mul sub sqrt}

\rput(0.5, 0){\psplot{-0.1}{0}{ 0.01001 x x mul sub sqrt 1 mul}}
\rput[l](0.53, 0.34){$\cos\varphi$ $= \frac{a}{\sqrt{a^2+b^2}}$}
\rput[t](0.34, -0.03){$\sin\varphi$}

\psline(0,0)(0.5,0.866)
\psline(0,0)(0.866, -0.5)
\rput[c](0.5,0.866){$\bullet$}
\rput[c](0.866,-0.5){$\bullet$}

\rput(1.3,1){$( \sin \varphi, \cos \varphi) = \left( \frac{a}{\sqrt{a^2+b^2}}, \frac{b}{\sqrt{a^2+b^2}}\right)$}
\rput(1.3,-0.6){$(\cos\varphi, -\sin \varphi)= \left( \frac{b}{\sqrt{a^2+b^2}}, \frac{-a}{\sqrt{a^2+b^2}}\right)$}
\psline[linestyle=dotted](0.5,0.866)(1,1.74)

\rput(1.1,1.81){$(a,b)$}
\rput[c](1,1.74){$\bullet$}
\rput(0.8, 1.392){\pscurve[linecolor=red]{->}(0.0866, -0.05) (0.224, -0.014)(0.1, 0.174)(-0.124, 0.1866 )(-0.0866, 0.05)}

\psline(0.5,0)(0.5,0.866)
\rput[l](0.01,0.2){$\varphi$}
\end{pspicture*}
} %end optionalDisplay

Substituting $\sin \varphi = \frac{a}{\sqrt{a^2+b^2}}, \sin \varphi = \frac{b}{\sqrt{a^2+b^2}}$ we get the substitution \eqref{eqRotateXZtoUV}. Observation of the figure shows that this transformation in fact corresponds to rotation. As rotations preserve volumes, we have transformed the problem of rotating around an arbitrary axis to a problem of rotating around the vertical axis $u=0$.

A formal definition of rotation is given in the subject of linear algebra, or in more advanced Calculus I courses. Alternatively, a very good explanation may be found at the Wikipedia page

\url{http://en.wikipedia.org/wiki/Rotation_(mathematics)} .

Suppose now we are rotating the figure around the $z$ axis. The next task is to plot the curves in the $(x,z)$-plane that determine the figure.

Once we have plotted the figure, we use vertical lines to split it into ``curved trapezoids'', i.e.,  figures bounded by a line $x=a$, a curve $z=f_1(x)$, a line $x=b$ and a curve $z=f_2(x)$ with $a< b$ and $f_1(x)\leq f_2(x)$ for $x\in [a,b]$. One such figure is illustrated in \ref{itemExampleFigureRotationalSolid}.

If the figure lies entirely on one side of the $z$ axis, we simply apply the volume formula  \eqref{eqVolumeRotationalSolid}.
\subsubsection{Example problem}
\begin{problem}
Compute the volume of the rotational solid obtained by rotating the figure enclosed between the curves $z=-x+3$, $z= \frac{1}x$.
\end{problem}
\begin{solution}
First we plot the figure and the axis of rotation.

\optionalDisplay{
\psset{xunit=1.5cm,yunit=1.5cm}
\begin{pspicture*}(-3,-2)(4,4.5)
%\psset{lightsrc=80 30 30,viewpoint=100 45 30 rtp2xyz,Decran=110,linewidth=0.2pt}
\psline[linecolor=gray]{->}(-2,0)(3.5,0) % x-axis
\psline[linecolor=gray]{->}(0,-2)(0,3.5) % z-axis
\rput[l](3.5,0){$x$}
\rput[b](0,3.5){$z$}

\psline[linestyle=dotted](-4,3)(3,-4)
\psline[linestyle=dotted](4,3)(-3,-4)

\rput{45}(3,2.1){$u$}
\rput{45}(-2,1.1){$v$}
\rput{45}(-1,0){
\pscurve[linecolor=red]{->}(0,0)(0.2, 0.1)(0, 0.2)(-0.2,0.1 )(-0.05,0)
}
\pscustom[linewidth=2pt, linecolor=blue, fillstyle=solid,fillcolor=lightgray]{
\psplot{0.381966011}{2.618033989}{1 x div}
\psline(2.618033989, 0.381966011)(0.381966011,2.618033989)
\closepath
%\pscurve(0.3, 3.33) (0.3, 1.349858808)
}
\rput(1, 1){$\bullet$}
\rput(-0.5, -0.5){$\bullet$}
\psline[linestyle=dotted](-0.5, -0.5)(1,1)
\rput(1.6,1){$z=\frac{1}x$}
\end{pspicture*}
}

Next we change coordinates so that the axis of rotation becomes the $z$ axis. First, we need to change coordinate system so the axis of rotation passes through the origin. The axis of rotation passes through $(x,z)=(-1,0)$ and therefore we can change basis by $y=x+1$, i.e., $x=y-1$. In the new variables, the axis of rotation has equation $z+y=0$. $z+y$ passes through $(-1,1)$, and therefore we may set, according to \eqref{eqRotateXZtoUV},
\[
u=\frac{\sqrt{2}}{2}(y+z)\quad,\quad v=\frac{\sqrt{2}}{2}(-y+z)\quad .
\]
Solving for $x$ and $z$, we get that $z=\frac{\sqrt{2}}{2}(u+v)$, $x=y-1=\frac{\sqrt{2}}{2}(u-v)-1$. Our axis of revolution becomes $u=0$. Therefore we would like to express the bounding curves in the form $v=f(u)$ for some function $f$. Therefore the curve $z=\frac{1}{x}$ is expressed in $(u,v)$ as
\[
\frac{\sqrt{2}}{2}(u+v)= \frac{1}{\frac{\sqrt{2}}{2}(u-v)-1}
\]
We solve by expressing $v$ in terms of $u$:
\[
\begin{array}{l}
\frac{1}{2}(u^2-v^2)-\frac{\sqrt{2}}{2}(u+v)=1 \\
v^2+\sqrt{2}v+2-\sqrt{2}u- u^2=0 \\
v_{1,2}=\frac{-\sqrt{2}\pm \sqrt{2-4(2-\sqrt{2}u- u^2)} }2=
\frac{-\sqrt{2}\pm \sqrt{-6+\sqrt{2}u+ u^2} }2\quad .
\\
\end{array}
\]
The smallest possible value of $u$ is $\frac{3\sqrt{2}}2$, which is also seen on our picture.  The other bounding curve, $x+z-3=0$, becomes $y-1+z-3=\sqrt{2}u -4=0 $,i.e., $u=2\sqrt{2}$. We see that the figure in the $(u,v)$-plane is enclosed by the two functions $\frac{-\sqrt{2}+ \sqrt{-6+\sqrt{2}u+ u^2} }2>\frac{-\sqrt{2}- \sqrt{-6+\sqrt{2}u+ u^2} }2$ and $u\in [\frac{3\sqrt{2}}2, 2\sqrt{2}]$.  Applying the formula \eqref{eqVolumeRotationalSolid}, we get the integral
\[
\int\limits_{u=\frac{3\sqrt{2}}2}^{2\sqrt{2}} 2\pi u\left(\sqrt{-6+\sqrt{2}u+ u^2}\right)du\quad .
\]
We can solve this integral using Euler substitution, Section \ref{secEulerSubTrigIntegrals}. %This will be done in later versions of the notes.
\end{solution}

\subsection{Length of curves} \label{secCurveLength}
\index{curve}\index{curve!parametric} A parametric curve (or simply curve) is defined as an n-tuple of functions mapping a given interval $[a,b]$ to an n-tuple of real numbers. For the purposes of our course, a curve will map the interval $[a,b]$ onto either a two dimensional space (denoted by $\mathbb R^2$), or a 3-dimensional space (denoted by $\mathbb R^3$). Thus, a curve $f:[a,b]\to \mathbb R^2$ is given by the two functions (one for each coordinate):
\[
\begin{array}{rcl}
\gamma:[a,b]&\to& \mathbb R^2\\
\gamma(t)&\eqdef& (f_1(t), f_2(t))
\end{array}\quad .
\]
An alternative and less formal notation for a curve is
\[
\gamma:\left|
\begin{array}{rcl}
x(t)&=&f_1(t)\\
y(t)&=&f_2(t)\\
\end{array}\right., t\in [a,b]\quad .
\]
In a similar fashion, a curve in the 3-dimensional space is given by
\[
\gamma:\left|
\begin{array}{rcl}
x(t)&=&f_1(t)\\
y(t)&=&f_2(t)\\
z(t)&=&f_3(t)\\
\end{array}\right., t\in [a,b]\quad .
\]
This definition is extended to an arbitrary number of variables in the obvious way.

\index{curve!image}The \emph{image of a curve} $f:[a,b]\to \mathbb R^2$ is the set of points in $\mathbb R^2$ of the form $f(t)$ as $t$ runs over all points in the interval $[a,b]$.

Colloquially, as often happens during lectures and scientific talks, we call both the parametric curves (which are functions) and their  and images (which are sets of points in the plane) using the word ``curve''. We try to avoid this informality in the present notes.

It is important to distinguish between a parametric curve and the image of the curve. For example, the curves
\[
\gamma_1:\left|
\begin{array}{rcl}
x&=&t^2\\
y&=&t^2\\
\end{array}\right., t\in [0,1]\quad .
\]
\[\gamma_2:\left|
\begin{array}{rcl}
x(t)&=&t\\
y(t)&=&t\\
\end{array}\right., t\in [0,1]\quad .
\]
both represent the straight segment connecting $(0,0)$ and $(1,1)$, but ``traverse'' the segment at different ``speeds''. Another example is given by
\[
\gamma_1:\left|
\begin{array}{rcl}
x(t)&=&t\\
y(t)&=&\sqrt{1-t^2}\\
\end{array}\right., t\in [0,1]
\]
and
\[
\gamma_2:\left|
\begin{array}{rcl}
x(t)&=&\cos t\\
y(t)&=&\sin t\\
\end{array}\right., t\in [0, \frac{\pi}{2}]\quad .
\]
These two parametric curves correspond to a quarter of a circle (which quarter?), however the two parametric curves traverse the quarter circle in different directions (which ones?) and at different speeds.

Images of curves are often given implicitly with a relation between $x$ and $y$. For example, the unit circle is given by
\[
x^2+y^2=1\quad .
\]
In the previous points, we showed two different parametric curves of a quarter circle.

To parametrize a curve whose image is given by a relation between $x$ and $y$ is a highly non-trivial task. For example, if the relation is of the form $y^2=x^3+px+q$ for parameters $p,q$, the problem of deciding (using computer) whether there is a point with rational coordinates $(x,y)$ that satisfies the equation is an active area of mathematical research. %There are cryptographic schemes based on results in this area.

In fact, the Euler substitution in Section \ref{secEulerSubTrigIntegrals} is an example of a parametrization of curves of the form $y^2=\pm x^2\pm 1$.

\index{curve!length}Definition of curve length. Let $\gamma$ be given by
\[
\gamma:\left|
\begin{array}{rcl}
x(t)&=&f_1(t)\\
y(t)&=&f_2(t)\\
\end{array}\right., t\in [a,b]\quad .
\]
Suppose in addition $f_1(t), f_2(t)$ are differentiable. Then the \emph{length} of $\gamma$ is defined as
\begin{equation}\boxed{
\label{eqDefLengthCurve}
\int\limits_{a}^b \sqrt{f_1'(t)^2+f_2'(t)^2}\diff t \quad .
}
\end{equation}
Similarly, the length of a curve sitting in 3-dimensional space is given by
\[
\int\limits_{a}^b \sqrt{f_1'(t)^2+f_2'(t)^2+f_3'(t)^2}\diff t \quad .
\]

Suppose an image of a curve $C\subset \mathbb R^2$ has two different parametrizations
\begin{equation}\label{eqDefCurve1}
\gamma_1:\left|
\begin{array}{rcl}
x(t)&=&f_1(t)\\
y(t)&=&f_2(t)\\
\end{array}\right., t\in [a,b]\quad ,
\end{equation}
\begin{equation}\label{eqDefCurve2}
\gamma_2:\left|
\begin{array}{rcl}
x(s)&=&g_1(s)\\
y(s)&=&g_2(s)\\
\end{array}\right. , s\in [p,q]\quad ,
\end{equation}
both differentiable, such that both are injective (i.e., every point $(x(t), y(t))$ has exactly one preimage, see Section \ref{secInverseFunctionBasics}).
\begin{equation}\boxed{
\begin{array}{c}
\text{If the parametric curve is differentiable and injective (one to one),}\\
\text{the length of the image of a curve } C
\text{does not depend on the parametric curve.}
\end{array}
}
\end{equation}
We have that $t$ can be expressed as function of $s$, namely, $t(s)= \gamma_1(\gamma_2^{-1}(s))$. Furthermore, as a function of $s$ from $[a,b]$ to $[p,q]$, $t$ is bijective (this follows from the fact that both $\gamma_1$ and $\gamma_2$ are injective).

We will prove the preceding item if $t$ as a function of $s$ is  continuously differentiable.

\begin{proof} (on condition $t(s)$ is continuously differentiable). In order to avoid confusion throughout this proof, we will denote derivatives with respect to $s$ by $\frac{\diff}{\diff s}$, and derivatives with respect to $t$ by $\frac{\diff}{\diff t}$.

We claim that $\frac{\diff t}{\diff s}(s)$ does not change sign. Suppose the contrary. The endpoints of the interval $[a,b]$ (where $t$ lives) must be mapped to the endpoints $p,q$ of the interval where $s$ lives. Suppose first $t(a)=p<q=t(b)$.  Then there exist points $s_0$ and $s_1$ such that  $\frac{\diff t}{\diff s}(s_0)<0$. As $t(s)$ is continuously differentiable, there exists a small interval  $s_0\in [w, z]$ such that $\frac{\diff t}{\diff s}(s)<0$. Then by the fundamental theorem of Calculus $t(w)>t(z)$ but $w<z$. By the intermediate value theorem, in the interval $[z,b]$, the function $t(s)$ achieves all values between $t(z)$ and $t(b)=q$. In particular, it achieves the value $t(z)<t(w)<q$. But then $t$ as a function of $s$ achieves the value $t(w)$ at least twice (once at $w$ and once at a point in $[z,q]$). On the other hand, we already saw that $t$ is bijective, and cannot achieve the same value twice. Contradiction. The other case, in which $t(a)=q>p=t(b)$, is treated in a similar fashion.

The length of $C$ is by definition
\[
\int\limits_{a}^b \sqrt{f_1'(t)^2+f_2'(t)^2}\diff t \quad .
\]
Unlike computations of indefinite integrals, where we make variable changes on the fly, when dealing with definite integrals, we may make variable changes only when they are bijective (one-to-one and onto, Section \ref{secInverseFunctionBasics}). By the conditions on the parametrization of the curve we may make a variable change under the definite integral. Suppose first $\frac{\diff t}{\diff s}(s)\geq 0$ and therefore $t(a)=p<q=t(b)$. Therefore
\[
\begin{array}{l}
\phantom{=}\displaystyle \int\limits_{t(s)=a}^b \sqrt{\left(\frac{df_1}{\diff t}(t(s))\right)^2+\left(\frac{df_2}{\diff t}(t(s))\right)^2}\diff(t(s)) \\
\displaystyle =  \int \limits_{s=p}^q \sqrt{\frac{df_1}{\diff t}( t(s))^2+ \frac{df_2}{\diff t}(t(s))^2}\diff(t(s))\\
= \displaystyle \int\limits_{s=p}^q \sqrt{\frac{df_1}{\diff t}( t(s))^2 + \frac{df_2 }{\diff t}(t(s))^2} \frac{\diff t}{\diff s}\diff s \\
\displaystyle
\stackrel{\frac{\diff t}{\diff s}(s)\geq 0}{=}  \int\limits_{s=p}^q \sqrt{ \left( \underbrace{ \frac{\diff t}{\diff s}(s) f_1( t(s)) }_{ \text{chain rule} } \right)^2+ \left( \underbrace{ \frac{ \diff t}{\diff s}(s) \frac{df_2}{\diff t} (t(s))}_{ \text{chain rule}} \right)^2}\diff s \\
=\displaystyle \int\limits_{s=p}^q \sqrt{ \left(\frac{\diff}{\diff s}\left(f_1(t(s))\right)\right)^2+ \left(\frac{\diff}{\diff s}\left(f_2(t(s))\right)\right)^2}\diff s \\
=\displaystyle \int\limits_{s=p}^q \sqrt{ \frac{ dg_1}{ \diff s}( s)^2 + \frac{dg_2}{\diff s}(s)^2}\diff s,
\quad
\end{array}
\]
where in the very last equality we use the definition of $t$ as a function of $s$ and \eqref{eqDefCurve2}. This is exactly what we wanted to prove. The case when $t(a)=q>p=t(b)$ is handled similarly.
\end{proof}

A motivation for the definition may be given as follows. Let $[a,b]$ be subdivided into $N$ intervals $[a,a+\Delta ], \dots, [a+k\Delta, a+(k+1)\Delta],\dots, [b-\Delta, b]$, $\Delta=\frac{b-a}{N}$. Then the straight segment connecting the point $(x,y)=(f_1(t), f_2(t))$ with $(f_1(t+\Delta), f_2(t+\Delta))$ has length $\sqrt{(f_1(t)-f_1(t+\Delta))^2 + (f_2(t)-f_2(t+\Delta))^2}= \sqrt{\left(\frac{(f_1(t)-f_1(t+\Delta)}{\Delta}\right)^2 + \left(\frac{(f_2(t)- f_2(t+\Delta)}{\Delta} \right)^2}\Delta$. Summing up over all intervals, we get that the piecewise linear curve approximating our smooth curve has length
\[
\sum_{k=1}^{N-1} \sqrt{\left(\frac{(f_1(a+k\Delta)-f_1(a+(k-1)\Delta)}{\Delta}\right)^2 + \left(\frac{(f_2(a+k\Delta)- f_2(a+(k-1)\Delta)}{\Delta} \right)^2}\Delta\quad .
\]
As the number of intervals $N$ tends to $\infty$, $\Delta$ approaches 0, and by the definition of derivative, the radical term tends uniformly to $\sqrt{f_1'(t)^2+f_2'(t)^2}$. Therefore the entire sum approximates the integral \eqref{eqDefLengthCurve}.

\begin{figureFixed}
\begin{center}
\optionalDisplay{
\psset{xunit=1cm,yunit=1cm}
\begin{pspicture}(-1,-1)(4,4.3)
\fcAxesStandard{-0.5}{-0.5}{4}{4}
\rput[b](0,4.1){\tiny$y$}
\rput[b](4.1,0){\tiny$x$}

\psplot[linecolor=blue, plotstyle=curve]{0.2}{4} { x 2 div x 180 mul  sin 1 add  mul 1 add x 3 div  sub   }  %{ x x mul 4 div x 180 mul  sin 1 add  mul   }
\psplot[linecolor=black!50, plotpoints=11,plotstyle=line]{0.2}{4}{ x 2 div x 180 mul  sin 1 add  mul 1 add x 3 div  sub   }
\psline[linecolor=red!50](2.1,1.674467844)(2.1,2.650886477)(2.48,2.650886477)
\rput[t](1.9,2.3){\tiny$\diff y$}
\rput[t](2.28,2.85){\tiny$\diff x$}


\rput[t](1,1.8){\tiny$(x(t), y(t))$}
\pscurve[arrows={->}, linestyle=dotted](1,1.8)(1.3, 1.5)(2.1,1.674467844)
\pscurve[arrows={->}, linestyle=dotted](2,4)(2.5, 3.55)(2.48,2.650886477)

\rput[b](2,4){\tiny$(x(t+\Delta), y(t+\Delta))$}
%\pscurve[linecolor=blue](0.2,1)(0.4,0.9)(0.8,0.5)(1.2,0.3)(1.6,1.5)(2, 0.5)(2.4, 2)(3.2, 1.5)(3.6, 1.4)(4, 2)

\end{pspicture}
}
\caption{The length of a curve is the limit of \\
lengths of approximating polygon lines.}\label{figCurveLengthDef}
\end{center}
\end{figureFixed}

%\chapter{Polar coordinates}
\section{Polar coordinates}
Polar coordinates of a point $P=(x,y)$ in the plane are any two numbers $(r, \theta)$, $r\geq 0$, for which $(x,y)=(r\cos\theta, r\sin \theta)$. In other words, the relation between the Cartesian coordinates $(x,y)$ and the polar coordinates $(r,\theta)$ of a point $P$ is given by
\[
\left|\begin{array}{rcl}
x&=&r\cos \theta\\
y&=&r\sin\theta
\end{array}\right.
\]

In these lectures, we allow polar coordinates with $ r<0$; this is the commonly accepted convention.

To obtain polar coordinates from Cartesian coordinates we compute that $x^2+y^2= r^2(\cos^2 \theta+\sin^2\theta)=r^2$ and therefore $r=\pm\sqrt{x^2+y^2}$. Furthermore we have that $\frac{y}{x}= \frac{\cancel{r}\sin \theta}{\cancel{r}\cos \theta}=\tan \theta$. To summarize:
\[
\left|\begin{array}{rcl}
r&=&\pm\sqrt{x^2+y^2}\\
\tan\theta&=&\frac{y}{x} \quad \mathrm{if~}x\neq 0 \quad .
\end{array}\right.
\]
Note that the equality $\tan \theta = \frac{y}{x}$ is not always defined. Also note that  $\tan \theta=\frac{y}{x}$ does not determine $\theta$ uniquely, as it is not clear in which interval $(k\pi -\frac{\pi}{2}, k\pi+\frac{\pi}{2})$, $k\in \mathbb Z$ does $\theta$ lie.

$r$ denotes the distance from $P$ to the origin and $\theta$ denotes the angle between the segment $OP$ and the $x$-axis, as indicated on the picture below.

\optionalDisplay{
\psset{xunit=1cm,yunit=1cm}
\begin{pspicture*}(-1.5,-1.5)(5,4)
\psline[linecolor=gray](-1.5,0)(2.5,0) % x-axis
\psline[linecolor=gray](0,-1.5)(0,2.5) % y-axis
\parametricplot[linecolor=red]{0}{60}{t cos 0.25 mul t 1000 div 1 add mul t sin 0.25 mul t 1000 div 1 add mul}
\parametricplot[linecolor=red, arrows=->]{0}{780}{t cos 0.55 mul t 1000 div 1 add mul t sin 0.55 mul t 1000 div 1 add mul}
\psline(0,0)(2,  3.464)
\rput(2,  3.464){$\bullet$}
\rput[lt](2,  3.264){$P= (r\cos \theta, r\sin \theta)$}
\rput(0.35,  0.25){$\theta$}
\end{pspicture*}
} %optionalDisplay

Polar coordinates are not unique: if $(r, \theta)$ are polar coordinates of $P=(x,y)$, then so are $(r, \theta+2\pi)$. More precisely, the point $P_1$ with polar coordinates $(r_1, \theta_1)$ coincides with a point $P_2$ with polar coordinates $(r_2, \theta_2)$ if one of the three mutually exclusive possibilities holds:
\begin{itemize}
\item $r_1=r_2\neq 0$ and $\theta_1=\theta_2+2k\pi, k\in \mathbb Z $,
\item $r_1=-r_2\neq 0$ and $\theta_1=\theta_2+(2k+1)\pi, k\in \mathbb Z$,
\item $r_1=r_2=0 $ and $\theta$ is arbitrary.
\end{itemize}




We have already encountered polar coordinates when studying polar form of complex numbers. Let $z\eqdef x+iy$, $x,y\in \mathbb R$. Then we recall from section \ref{secPolarFormComplexNumbers} that there exist real numbers $\rho$, $r\eqdef e^\rho$ and $\theta$ such that $z=x+iy= e^{\rho+i\theta}= e^\rho(\cos \theta+i\sin \theta)= r(\cos \theta + i\sin \theta)$. We recall the latter equality is the polar form of the complex number $z$. Then $(r, \theta)$ are the polar coordinates of the point $(x,y)$.

The relationship between polar form of complex numbers and polar coordinates can be summed up in the following equalities.
\[
\begin{array}{rclclcl}
r(\cos \theta+i\sin\theta) &=&  x+iy&=& z&=&\Re (z)+i\Im(z)\\
r\cos\theta&=&x&=&\Re(z)\\
r\sin\theta&=&y&=&\Im(z)\\
\theta&&&=& \arg (z)\\
\tan \theta &=&\displaystyle\frac{y}{x}&=&\displaystyle\frac{\Im (z)}{\Re (z)}\\
r&=&\sqrt{x^2+y^2}&=& |z|&=&\sqrt{z\bar z}\\
\end{array}
\]

\index{cardioid}\index{spiral} Sometimes a curve is given in polar coordinates by specifying a relationship between $r$ and $\theta$, such as $r=\theta$ (spiral) or $r=1+\sin \theta$ (cardioid), $r=2\cos \theta$ (circle of radius one centered at $(1,0)$).

\optionalDisplay{
\psset{xunit=1cm,yunit=1cm}
\begin{pspicture*}(-5,-5)(5,5)
\rput (3,3){$r=\theta$}
\psaxes[labels=none, ticks=x]{<->}(0,0)(-4.5,-4.5)(4.5,4.5)
\parametricplot[linecolor=red, plotpoints=500]{0}{720}{t cos t mul 180 div t sin t mul 180 div }
\end{pspicture*}

\psset{xunit=1cm,yunit=1cm}
\begin{pspicture*}(-5,-5)(5,5)
\rput (3,3){$r=1+\sin\theta$}
\psaxes[labels=none, ticks=x]{<->}(0,0)(-4.5,-4.5)(4.5,4.5)
\parametricplot[linecolor=red, plotpoints=500]{0}{360}{1 t sin add t cos mul
 1 t sin add t sin mul}
\end{pspicture*}

\psset{xunit=1cm,yunit=1cm}
\begin{pspicture*}(-5,-5)(5,5)
\rput (3,3){$r=2\cos\theta$}
\psaxes[labels=none, ticks=x]{<->}(0,0)(-4.5,-4.5)(4.5,4.5)
\parametricplot[linecolor=red, plotpoints=500]{0}{360}
{ t cos 2 mul t cos mul
  t cos 2 mul t sin mul}
\end{pspicture*}
} %optionalDisplay

\index{area!enclosed by curve in polar coord.} Let $r=f(\theta), \theta\in [a,b]$ be a curve given in polar coordinates.  Let $A$ denote the figure given by the union of the segments connecting each point the curve to the origin. Suppose no two points on the curve lie on the same ray from the origin. Then the area of $A$ is given by
\begin{equation}\boxed{\label{eqAreaUnderPolarCurve}
area(A)\eqdef \int\limits_{\theta=a}^b \frac{f(\theta)^2}{2}d\theta\quad .
}
\end{equation}
A motivation for the above definition may be given as follows.
\psset{xunit=1cm,yunit=1cm}

\optionalDisplay{
\begin{pspicture*}(-4,-1)(7.3,5.4)
\newcommand{\dThetA}{20}
\rput(4,5){$r=f(\theta)$}
\psaxes[labels=none]{<->}(0,0)(-3.5,-0.5)(4.5,5.2)
\multido{\rb=10+\dThetA}{6}{%
\pstVerb{/rR \rb\space 30 div 2 add def
/rRPlusTheta \rb\space \dThetA\space add 30 div 2 add def
}
\pscustom[fillcolor=blue!30, fillstyle=solid, linecolor=blue]{\psline(0,0)(! rR \rb \space cos mul rR \rb \space sin mul) (! rRPlusTheta \rb \space\dThetA\space add cos mul rRPlusTheta \rb \space \dThetA\space add sin mul) }
}
\parametricplot[linecolor=red, plotpoints=500]{10}{130}
{ t 30 div 2 add  t cos mul
  t 30 div 2 add  t sin mul}
\parametricplot[linecolor=red]{30}{50} %{! \dThetA 10 add}
{t cos 1 mul
 t sin 1 mul }
\rput[lt](0.8, 0.8){\tiny$\Delta\theta$}
\parametricplot[linecolor=red, arrows=->]{0}{30} %{! \dThetA 10 add}
{t cos 0.5 mul
 t sin 0.5 mul }
\rput[lt](0.55,0.3){\tiny$\theta_1$}
\parametricplot[linecolor=red, arrows=->]{0}{50} %{! \dThetA 10 add}
{t cos 0.85 mul
 t sin 0.85 mul }
\rput[lt](0.85,0.4){\tiny$\theta_2$}

\rput[tr] (-0.1,-0.1){$O$}
\rput[tl] (2.7,1.5){$P=(f(\theta_1)\cos \theta_1, f(\theta_1)\sin \theta_1)$}
\rput[tl] (2.51,2.95){$Q=(f(\theta_2)\cos \theta_2, f(\theta_2)\sin \theta_2)$}
\end{pspicture*}
} %optionalDisplay

Let $N$ be a large number. Split the interval $[a,b]$ using $N+1$ equally spaced points $a=\theta_0 \leq\theta_1 \leq\dots \leq \theta_{N-1}\leq \theta_N=b$ into $N$ segments of the same length. Denote the length of each such segment by $\Delta \theta$. Then $\Delta\theta= \frac{b-a}{N}$. Then the area of $A$ is approximated by triangles with vertices on the curve. Those vertices are of the form $(f(\theta_j)\cos \theta_j, f(\theta_{j+1} )\cos \theta_{j+1})$ as drawn in the figure. Consider one such triangle, $OPQ$, as indicated in the figure. The area of triangle $OPQ $ is $\frac{|OP| |OQ|\sin (\Delta \theta )}{2}= \frac{f(\theta_1) f(\theta_2)\sin (\Delta \theta )}{2} $. In other words the area of $A$ is approximated by
\[
\sum_{j=0}^{N-1} \frac{f(\theta_j)f(\theta_{j+1})\sin (\Delta \theta )}{2}= \frac{\sin(\Delta\theta)}{ \Delta \theta } \sum_{j= 0}^{N-1} \frac{f(\theta_j)f(\theta_{j}+ \Delta\theta) \Delta \theta}{2}\quad .
\]
In the above sum, the multiplicand $\frac{ \sin( \Delta \theta )}{ \Delta\theta}$ tends to $1$ as $\Delta\theta$ tends to $0$. Therefore as $\Delta$ tends to zero, the expression tends to the limit of the second multiplicand. On the other hand, one can show that the second multiplicand approximates the integral $\int\limits_{\theta=a}^b \frac{f(\theta)^2}{2}d\theta$. This motivates definition of area given in \eqref{eqAreaUnderPolarCurve}.

Curve length of curve of the form $\gamma:|r=f(\theta),\theta\in [a,b]$. Then $(x,y)= (r\cos\theta, r\sin \theta)= (\underbrace{f(\theta)\cos\theta}_{=:g_1(\theta)}, \underbrace{ f(\theta)\sin\theta}_{=:g_2(\theta)} $. Recall the definition of curve length \eqref{eqDefLengthCurve}:
\[
\int\limits_{a}^b\sqrt{\left(\frac{dg_1}{d\theta}\right)^2+\left(\frac{dg_2}{d\theta}\right)^2}d\theta \quad.
\]
We substitute $g_1(\theta)=f(\theta)\cos\theta$, $g_2(\theta)=f(\theta)\sin\theta$ and carry out the operations:
\[
\begin{array}{l}
\displaystyle\int\limits_{a}^b\sqrt{ (-\sin\theta f(\theta) + \cos\theta f'(\theta) )^2+(\cos\theta f(\theta)+\sin \theta f'(\theta) )^2}\diff\theta=\\
\displaystyle \int \limits_{a}^b \sqrt{\sin^2\theta f^2(\theta)- \cancel{2\sin\theta\cos\theta f'(\theta)f(\theta)}+\cos^2\theta (f'(\theta))^2}\\
\phantom{\int\limits_a^b} \overline{+ \cos^2\theta f^2(\theta)+\cancel{2\sin\theta\cos\theta f'(\theta)f(\theta)} \sin^2 (f'(\theta))^2 }\diff\theta\quad .
\end{array}
\]
Regrouping the above expression using $\sin^2\theta+\cos^2\theta=1$ we finally get that the length of a curve given in polar coordinates as $r=f(\theta)$ is computed as
\begin{equation}\boxed{\label{eqCurveLengthPolarCoords}
\int\limits_{a}^b \sqrt{f^2(\theta)+ (f'(\theta))^2}d\theta
}
\end{equation}
Find the area of the figure that lies outside of figure enclosed by $r_1=3\cos\theta_1, \theta_1\in [0,\pi)$ and inside the figure enclosed by $r_2=1+\cos\theta_2, \theta\in [-\pi,\pi)$. %Find the perimeter of the curve.

\begin{solution}
\optionalDisplay{
\begin{pspicture}(-1, -4)(4,4)
\pscustom*[fillcolor=red!20, linecolor=red!20]{
\parametricplot[linecolor=red, plotpoints=1000]{-60}{60}{3 t cos mul t cos mul 3 t cos mul t sin mul}
\parametricplot[linecolor=red, plotpoints=1000]{60}{-60}{1 t cos add t cos mul 1 t cos add t sin mul}
}
\parametricplot[linecolor=red, plotpoints=1000]{0}{360}{1 t cos add t cos mul 1 t cos add t sin mul}
\parametricplot[linecolor=red, plotpoints=1000]{0}{360}{3 t cos mul t cos mul 3 t cos mul t sin mul}
\psaxes[labels=none]{<->}(0,0)(-1,-2.5)(3.5,2.5)
\rput[c](0.75,1.299){$\bullet$}
\rput[c](0.75,-1.299){$\bullet$}
\rput[b](0.85,1.51){$\left(\frac{3}4,\frac{3\sqrt{3}}{4} \right)$}
\rput[t](0.85,-1.51){$\left(\frac{3}4,-\frac{3\sqrt{3}}{4} \right)$}
\psline[linestyle=dotted](0,0)(0.75,1.299)
\psline[linestyle=dotted](0,0)(0.75,-1.299)
\end{pspicture}
} %optionalDisplay

We first plot the curves using computer to get the above picture. We see that the curve $r_1=3\cos\theta_1$ is a circle and that $r_2=1+\cos\theta_2$ is a cardioid (rotated at an angle of $-\frac{\pi}{2}$). Let us verify that indeed $r_1=3\cos\theta_1$ is a circle. Multiplying by $r_1$ we get $r_1^2=3r_1\cos\theta_1= 3x$. Therefore $x^2+y^2-3x=0$, or, after completing the square, $\left(x- \frac{3}{2} \right)^2+y^2 =\frac{9}{4}$. Thus $r_1=3\cos\theta_1$ is a circle centered at $\left(\frac{3}{2},0\right)$ and with radius $\frac{3}{2}$. From the picture we see that the two curves intersect in three points, one of which is the origin. We get an additional intersection of the two curves when $r_1=r_2$, $\theta_1=\theta_2$ and $1+\cos\theta=3\cos\theta$, where we have set $\theta:=\theta_1 = \theta_2$. In other words, $\cos\theta=\frac{1}{2}$ and as $\theta\in [0,\pi)$, we get $\theta=\pm \frac{\pi}{3}$. Thus we have a second intersection point $x=\frac{3}{4}$, $y=\frac{3\sqrt{3}}{4}$. By symmetry with respect to the $x$ axis we get that the final intersection point is $(x,y)=\left(\frac{3}4,-\frac{3\sqrt{3}}{4}\right) $.

Therefore the area of the enclosed curve is given by the area of the figure obtained by connecting $r=3\cos\theta, \theta\in(-\frac{\pi}{3}, \frac{\pi}{3})$ to the origin minus the area of the figure obtained by connecting $r=1+\cos\theta, \theta\in(-\frac{\pi}{3}, \frac{\pi}{3})$ to the origin. We can now use \eqref{eqAreaUnderPolarCurve} to get the answer

\[
\begin{array}{rcl}
\displaystyle\int\limits_{-\frac{\pi}{3}}^{\frac{\pi}{3}} \frac{1}2\left(9\cos^2\theta-(1+\cos\theta)^2\right) d\theta&=& \displaystyle\int\limits_{-\frac{\pi}{3}}^{\frac{\pi}{3}} \frac{1}2\left(8\underbrace{\cos^2\theta}_{=\frac{1+\cos(2\theta)}2  }-1-2\cos\theta \right) d\theta\\
&=&\displaystyle\frac{1}2\int\limits_{-\frac{\pi}{3}}^{\frac{\pi}{3}} \left(4+4 \cos 2\theta-1-2\cos\theta \right) d\theta
\\
&=&\pi+\left.\sin2\theta\right|_{\theta=-\frac{\pi}{3}}^{\theta=\frac{\pi}3}-\left.\sin\theta\right|_{\theta=-\frac{\pi}{3}}^{\theta=\frac{\pi}3}\\
&=&\pi
\end{array}
\]
\end{solution}


\chapter{A bit of differential equations}
\section{Some terminology and background}
A differential equation is an equation that contains at least one derivative (first, second, third,...) of an unknown function.

The \emph{order} of a differential equation is the order of the highest derivative that appears in the equation.

An example of a differential equation is
\begin{equation}\label{eqDFQsimplestExample}
\frac{\diff P}{\diff t} = k P\quad ,
\end{equation}
where $t$ is a variable, $k$ is a constant and $P(t)$ is a function.

Let us interpret $P$ as population size as a function of the time $t$, and $k$ as some natural rate of growth constant. If $k$ is positive (and the population $P$ is positive), so is $\frac{d P}{\diff t}$. As the first derivative of $P$ is positive, $P$ will increase with time, at an ever growing rate. We can plug in the function $P(t)= e^{k t}$ and verify that it satisfies the equation.

Differential equations are used to model processes in the natural sciences. For example, \ref{eqDFQsimplestExample} models ideal population growth with infinite resources and unlimited environment - the increase in population is proportional to the size of the population.

A more complicated example is the logistic differential equation
\[
\frac{\diff P }{\diff t} = k P\left(1- \frac{P}{M}\right)\quad.
\]
Here, $k$, $t$, $P$ have the same interpretation as before, and $M$ is the carrying capacity of the environment. In this model, while $P$ is smaller than the carrying capacity $M$,  $\frac{dP}{\diff t}$ will be positive, and the population will increase with time. However, if the population is larger than the carrying capacity $M$, $\frac{d P }{\diff t}$ will be negative and the population will decrease.

Another example of a differential equation is the motion of a spring
\[
m\frac{\diff^2 y (t)}{\diff t^2}= -k y,
\]
where $m$ and $k$ are constants ($m$ stands for mass and $k$ for the spring's constant). Here, we recall the notation $\frac{d^2 y (t)}{\diff t^2}= y''(t)$ means second derivative.


Differential equations may have \emph{initial conditions}, i.e., we may be given the value of the unknown function or its derivatives in one or more points. For example, in \eqref{eqDFQsimplestExample}, we may add the initial condition $P(t_0)= 10$, to say that at time $t=t_0$, $P(t)=10$.

If we are given initial conditions $y(x_0)=y_0$, we say that we have  an \emph{initial condition differential equation}.

Initial conditions are critical for a solution of a differential equation. For example, if $P(t_0)=0$, \eqref{eqDFQsimplestExample} has only solution $P(t)=0$, buf if $P(t_0)=10$, \eqref{eqDFQsimplestExample} has only solution $P(t)=10  e^{k(t-t_0)}$.
\subsection{Direction fields}
Suppose we have a differential equation of the form
\begin{equation}\label{eqDFQwithDirectionFlow}
\frac{\diff y}{\diff x}= F(x,y)\quad.
\end{equation}

At each point $(x_0,y_0)$, let us plot a segment of the line with slope $F(x_0,y_0)$ passing through $(x_0,y_0)$. The resulting picture is called a direction field.

Examples.

\optionalDisplay{
\psset{xunit=1cm,yunit=1cm}
\SpecialCoor
\begin{pspicture}(-5,-5)(5,5)
\psaxes{<->}(0,0)(-5,-5)(5,5)
\rput (5,5){The direction field $\frac{\diff y}{\diff x}=xy$}%
\fcDirectionFieldDefault{x y mul}{-4}{-4}{0.5}{17}
\end{pspicture}

\begin{pspicture}(-5,-5)(5,5)
\psaxes{<->}(0,0)(-5,-6)(5,5)
\rput (5,5){The direction field $\frac{\diff y}{\diff x}=x+y$}
\fcDirectionFieldDefault{x y add}{-4}{-4}{0.5}{17}
\end{pspicture}
}%optionalDisplay

$\frac{\diff y}{\diff x}= F(x,y)$ has different solutions depending on the initial condition given by the $y(x_0)=y_0$ starting point $(x_0, y_0)$. A solution of a differential equation of the form \eqref{eqDFQwithDirectionFlow} corresponds to the trajectory of a point ``flowing'' along the direction field. If drawn with increasing precision, the blue segments representing the direction field begin to resemble smooth curves; those curves indeed approximate solutions of the differential equation.

Euler's method for approximating solutions. Fix a small ``approximation step'' $h$. Let $x_n:=x_0+nh$ be the point at distance $n$ times our small approximation step from $x_0$. Suppose the initial condition differential equation problem
\[
\frac{\diff y}{\diff x}= F(x,y)\quad.
\]
with initial conditions $y(x_0)=y_0$  has a solution $y(x)$. Then approximate values for $y_n\approx y(x_n)$ are given by
\begin{equation}\label{eqEulerApproxDFQ}
y_n \eqdef y_{n-1} + h F(x_{n-1}, y_{n-1})\quad .
\end{equation}
A motivation for Euler's method can be given as follows. At a point $x$, for a very small interval $(x,x+h)$, the function $y(x)$ is approximately linear, and is approximated by $y(x+h)\approx y(x)+ h y'(x) $. On the other hand, $y'(x)$ equals $F(x, y(x))$. Now if we have already approximated $y(x_{n-1})\approx y_{n-1}$, we known $y'(x_{n-1}) \approx F(x_{n-1}, y_{n-1})$ and  we can approximate $y(x_{n-1}+h)\approx y_n $ by \eqref{eqEulerApproxDFQ}.

%Euler's method has a number of drawbacks, both computationally and algebraically, and is not used in practice. However, it illustrates the main idea of approximating solutions to differential equations: approximate at a point $x$ the value of $y(x+h)$ (as a function of $h$) by a well-known function in $h$ (linear, quadratic, third power, fourth power,\dots), move along the approximation for a very short distance $h$, and recompute.

We note that there are a number of issues which arise from Euler's method. For example, Euler's method requires that $F(x,y)$ be defined for values of $y$ traversed by the solution of the differential equation. This does not happen, for example, for the direction field given by $\frac{\diff y}{\diff x}= - \frac{x}{y}$: that direction field is not well defined as $y$ approaches $0$. However, we will soon be able to compute that solutions of the differential equation  $\frac{\diff y}{\diff x}= -\frac{x}{y}$ for $y>0$ behave locally likes arcs of circles whose radii depending on the starting point $(x_0, y_0)$. In particular, such solutions can be extended to values for which $y=0$.

\optionalDisplay{
\begin{pspicture}(-6,-1)(6,6)
\psaxes{<->}(0,0)(-6,-1)(6,6)
\rput (5,5){The direction field $\frac{\diff y}{\diff x}=-\frac{x}{y}$}
\fcDirectionFieldFull{x y div -1 mul}{-4}{0.001}{0.5}{17}{8}{0.2}{0.02}{linecolor=blue}%
\end{pspicture}
} %optionalDisplay
%\begin{pspicture}
%\end{pspicture}

\subsection{Separable equations} \label{secSeparableDFQs}
A differential equation is separable if it can be written in the form
\begin{equation}\label{eqDFQSeparable}
\frac{\diff y}{\diff x}= f(x)g(y)\quad .
\end{equation}
We recall that $y(x)$ denotes $y$ as a function of at $x$, and that $y(t)$ denotes the expression obtained by substituting $x=t$ in the expression for $y$.

A separable differential equation can be solved as follows. Divide out by $g(y)$:
\[
\frac{\diff y}{\diff x} \frac{1}{g(y)}= f(t)\quad .
\]
Let $x_0$ be a constant and $t$ a variable. We now integrate \eqref{eqDFQSeparable} from $x=x_0$ to $x=t$. This will ultimately give us an expression for $y(t)$.
\begin{equation}\label{eqSeparableDFQSolution1}
\begin{array}{rcl}
\displaystyle \int\limits_{x=x_0}^{x=t}\frac{1}{g(y(x))} \underbrace{\frac{\diff y}{\diff x}\diff x}_{= \diff (y(x))}&=&\displaystyle \int \limits_{ x=x_0}^{ x=t}  f(x)\diff x
\\
\displaystyle \int\limits_{x= x_0}^{x=t}\frac{ 1}{g( y(x))} \diff (y(x))&=&\displaystyle \int \limits_{ x=x_0}^{x=t}  f(x)\diff x
\quad .
\end{array}
\end{equation}
Suppose now $\displaystyle \frac{\diff y }{\diff x}$ is non-zero and does not change sign for $x\in [x_0, t]$.  Then $y$ is a one to one and onto map (=bijection) between $[x_0, t]$ and $[y(x_0), y(t)]$ (recall the definition of one to one and onto (=bijection) from Section \ref {secInverseFunctionBasics}). Therefore we may carry out the variable change $z=y(x)$, $z\in [y(x_0), y(t)]$:
\begin{equation}\label{eqSeparableDFQSolution2}
\displaystyle \int \limits_{ z=y(t)}^{z=y(x_0)}\frac{1}{g(z)} \diff z=\displaystyle \int\limits_{x=x_0}^{x=t}  f(x)\diff x\quad .
\end{equation}
Suppose we can solve $\displaystyle \int \frac{1}{g(z)}\diff z = G(z)+C_1$ and $\displaystyle \int  f(t)\diff t=F(t)+C_2$. Using the Fundamental theorem of Calculus we can rewrite \eqref{eqSeparableDFQSolution2} as
\[
\left.G(z)\right]_{z=y(x_0)}^{z=y(t)}=
G(y(t))-G(y(x_0)) = \left. F(x) \right]_{x=x_0}^{x=t}= F(t)-F(x_0)
\]
which we rearrange to:
\[
G(y)-F(t)-G(y(x_0))+ F(x_0)=0\quad .
\]
Finally, we may replace the dummy variable $t$ back to the variable $x$ to get a relation between $ x$ and $y$:
\begin{equation}\label{eqDFQSeparableSolution}
G(y)-F(t)-G(y(x_0))+ F(x_0)=-0
\end{equation}
If it is possible to solve the above equation for $y$ as a function of $x$, we get the desired solution of our differential equation. If we cannot solve for $y$ explicitly, we may leave the relationship  \eqref{eqDFQSeparableSolution} as an implicit solution to the differential equation.
\subsection{The logistic equation and other examples of separable differential equations}
\begin{example} Solve the logistic equation
\[
\frac{\diff P}{\diff t}= k P\left(1-\frac{P}{M}\right)\quad.
\]
where $k>0$ and $M>0$ (``environment carrying capacity'') are fixed positive constants and the initial population $P(0)$ is positive.
\end{example}
We recall that in the logistic equation, $P$ stands for population size.

\begin{solution}
Case 0. If the initial population is $P(0)$ is zero, the population will clearly stay zero, and so if $P(0)=0$ then $P(t)=0$ is the solution to our equation.

Case 1. If $P(0)=M$, then the population growth $\frac{\diff P}{\diff t}$ equals zero, the population is stable and constant function $ P(t)=P(0)$ is the solution to our equation.

Case 2. Suppose now $P(0)\neq M$ and $P(0)\neq 0$. As the logistic equation is separable, we can solve it using Section \ref{secSeparableDFQs}:

\begin{equation}\label{eqLogisticEq}
{
\renewcommand*{\arraystretch}{2}
\begin{array}{rcll|l}
\displaystyle \frac{\diff P}{\diff t}&=&\displaystyle k P\left(1-\frac{P}{M}\right)\\
\displaystyle \frac{\diff P}{\diff t}&=&\displaystyle \frac{kP(M-P)}{M}\\
\displaystyle \frac{M}{kP(M-P)}\frac{\diff P}{\diff t}&=&1 &&
\begin{array}{l}
\text{As } P(0)\neq M,\\
P(0)\neq 0,\\
\text{can integrate } \int\limits_{t=0}^{t=s}\diff t
\end{array}\\
\displaystyle \frac{1}{k} \int\limits_{t=0}^{t=s} \frac{M}{P(M-P)}\frac{\diff P}{ \diff t}\diff t&=&\displaystyle \int\limits_{t=0}^{t=s}1\diff t &&\text{V. change } \frac{\diff P}{\diff t}\diff t= \diff P\\
\displaystyle \frac{1}{k} \int\limits_{t=0}^{t=s} \frac{M}{P(M-P)} \diff P &=& \left[t\right]_{t=0}^{t=s} =s &&\text{split part. fractions}\\
\displaystyle \frac{1}{k}\int\limits_{t=0}^{t=s} \left( \frac{1}{P} +\frac{1}{M-P} \right)\diff P &=&s\\
\displaystyle \frac{1}{k}\left[ \ln | P|  - \ln |M-P| \right]_{t=0}^{t=s}&=&s\\
\displaystyle \frac{1}{k}\left(\ln |P(s)|-\ln |M-P(s)|-\right.\\
\left.(\ln P(0)-\ln |M-P(0)|) \right)&=&s\\
\displaystyle \ln \left|\frac{ P(s)(M-P(0))}{P(0)(M-P(s))}\right|&=&k s &&\text{exponentiate}\\
\displaystyle \left|\frac{ P(s)(M-P(0))}{P(0)(M-P(s))}\right|&=&e^{ks} \quad .
\end{array}
}
\end{equation}

\noindent Case 2.1. $P(0)<M$, i.e., the initial population is smaller than the carrying capacity $M$ of the environment. As assumed by our model, population varies continuously with time and therefore for all small enough $s $ we have that $P(s)<M$ as well. Then, for small $s$, the absolute value is no longer needed and we can compute
\begin{equation}\label{eqLogisticEquationCase1}
\begin{array}{rcl}
\displaystyle \frac{ P(s)(M-P(0))}{P(0)(M-P(s))}&=&\displaystyle e^{ks} \\
\displaystyle P(s)(M-P(0)) &=& \displaystyle  P(0)(M-P(s))e^{ks}\\
\displaystyle P(s)(M-P(0)+P(0)e^{ks})&=&\displaystyle P(0)M e^{ks}\\
\displaystyle P(s)&=&\displaystyle  MP(0) \frac{e^{ks}}{M-P(0)+P(0)e^{ks}} \\
\displaystyle P(s)&=&\displaystyle  M \frac{e^{ks}}{\frac{M}{P(0)}-1+e^{ks}} \\
\end{array}
\end{equation}
Since $\frac{M}{P(0)}>1 $, we have that $\frac{M}{P(0)}-1+e^{ks}>e^{ks}$ and therefore $ 1>\frac{e^{ks}}{\frac{M}{P(0)}-1+e^{ks}}>0 $. Finally we get that
\[
0<P(s)= M \frac{e^{ks}}{\frac{M}{P(0)}-1+e^{ks}} <M\quad .
\]
So far, our solution was shown to be valid only for small $s$. However, the solution $P(s)$ we obtained satisfies $P(s)<M$ for all values of $s$. Thus we conclude that, in Case 2.1, the absolute values in $ \left|\frac{ P(s)(M-P(0))}{P(0)(M-P(s))}\right|$ are not needed for any value $s$. Our solution to the logistic equation in Case 1 is then $\displaystyle P(s)=\displaystyle  M \frac{e^{ks}}{ \frac{M}{P(0)} -1+ e^{ks}}$.


\noindent Case 2.2. $P(0)>M$, i.e., the initial population is greater than the carrying capacity $M$ of the environment. As assumed by our model, population varies continuously with time and therefore for all small enough $s $ we have that $P(s)>M$ as well. Then, for small $s$, we  have that
\[
\left|\frac{ P(s)(M-P(0))}{P(0)(M-P(s))}\right|=\frac{-P(s)(M-P(0))}{-P(0)(M-P(s))}=\frac{ P(s)(M-P(0))}{P(0)(M-P(s))} \quad .
\]
Notice that by the above computation, the left hand side of \eqref{eqLogisticEq} coincides in cases 2.1 and 2.2. Therefore the computations in Case 2.2 are identical to the computations \eqref{eqLogisticEquationCase1} from Case 2.1. We can therefore conclude that, for small $s$, we have $\displaystyle P(s)=\displaystyle  M \frac{e^{ks}}{\frac{M}{P(0)}-1+e^{ks}} $. Now notice that $\frac{M}{P(0)}-1<0$. Therefore $ \frac{M}{ P(0) }-1+e^{ks} < e^{ks}$ and it follows that
\[
P(s)= M \frac{e^{ks}}{\frac{M}{P(0)}-1+e^{ks}} >M\quad .
\]
In this way, the solution for $P(s)$ obtained by us satisfies $P(s)>M$ for all values of $s$. We conclude that the absolute values in $ \left|\frac{ P(s)(M-P(0))}{P(0)(M-P(s))}\right|$ are not needed in Case 2.2 as well. Then the solution to Case 2.2 is identical to that of Case 2.1.

We conclude that the solution to the logistic equation in Case 2 is
\begin{equation}\label{eqLogisticEquationAnswer}
P(s)=\displaystyle  M P(0) \frac{e^{ks}}{ M -P(0)+ P(0)e^{ks}} \quad .
\end{equation}
Notice that if we substitute $P(0)=M$ in the above formula (Case 1), we get that $P(s)=M$, the same answer as in Case 1. Furthermore notice that when $P(0)=0 $ (Case 0), formula \eqref{eqLogisticEquationAnswer} yields $P(s)=0$, the same answer as that in Case 0. Therefore our formula \eqref{eqLogisticEquationAnswer} gives the solution to the logistic equation in all cases.
\end{solution}


In the graph below, we have plotted the solution to the logistic equation for $k=1$ and various initial population values $P(0)$. The shapes of the curves confirm visually the considerations made in Cases 1-4. Notice that the plots make sense for negative time as well: for $s<0$, the function $P(s)$ measures what must have been the population in the past so as to yield the population present at time $s=0$.

\optionalDisplay{
\psset{xunit=1cm, yunit=1cm}
\begin{pspicture}(-0.700000, -0.700000)(6.200000,2.435064)

\tiny
\psaxes[ticks=none, labels=none, arrows=<->](0, 0)(-0.500000, -0.550000)(6.000000, 2.235064)
\fcLabelOnXaxis{6}{$s$}
\fcLabelOnYaxis{2.235064}{$P$}
%Function formula: \frac{11/10*e^{x}}{11/10*e^{x}-1/10}

\psplot[linecolor=red, plotpoints=1000]{-0.100000}{6.000000 }{2.718281828 x exp 1.1000000 mul -0.1000000 2.718281828 x exp 1.1000000 mul add div }

%Function formula: \frac{2*e^{x}}{2*e^{x}-1}

\psplot[linecolor=green, plotpoints=1000]{-0.100000}{6.000000}{2.718281828 x exp 2.0000000 mul -1.0000000 2.718281828 x exp 2.0000000 mul add div }

%Function formula: \frac{3/10*e^{x}}{3/10*e^{x}+7/10}

\psplot[linecolor=red, plotpoints=1000]{-0.100000}{6.000000}{2.718281828 x exp 0.3000000 mul 0.7000000 2.718281828 x exp 0.3000000 mul add div }

%Function formula: \frac{1/5*e^{x}}{1/5*e^{x}+4/5}

\psplot[linecolor=blue, plotpoints=1000]{-0.100000}{6.000000}{2.718281828 x exp 0.2000000 mul 0.8000000 2.718281828 x exp 0.2000000 mul add div }

%Function formula: \frac{1/10*e^{x}}{1/10*e^{x}+9/10}

\psplot[linecolor=magenta, plotpoints=1000]{-0.100000}{6.000000}{2.718281828 x exp 0.1000000 mul 0.9000000 2.718281828 x exp 0.1000000 mul add div }

%Function formula: \frac{2/5*e^{x}}{2/5*e^{x}+3/5}

\psplot[linecolor=brown, plotpoints=1000]{-0.100000}{6.000000}{2.718281828 x exp 0.4000000 mul 0.6000000 2.718281828 x exp 0.4000000 mul add div }

%Function formula: \frac{3/2*e^{x}}{3/2*e^{x}-1/2}

\psplot[linecolor=cyan, plotpoints=1000]{-0.100000}{6.000000}{2.718281828 x exp 1.5000000 mul -0.5000000 2.718281828 x exp 1.5000000 mul add div }

%Function formula: \frac{13/10*e^{x}}{13/10*e^{x}-3/10}

\psplot[linecolor=blue, plotpoints=1000]{-0.100000}{6.000000}{2.718281828 x exp 1.3000000 mul -0.3000000 2.718281828 x exp 1.3000000 mul add div }

\end{pspicture}
}

\begin{problem}~\label{problemDFQseparable-yprime=ysquared-1}
\input{../../modules/diff-eq-separable/homework/separable-DFQs-1}
\end{problem}
\renewcommand{\additionalProblemLabel}{\ref{problemDFQseparable-yprime=ysquared-1}.}
\input{../../modules/diff-eq-separable/homework/separable-DFQs-1-solution}
\renewcommand{\additionalProblemLabel}{}

The following problem is the so called ``mixing problem''.
\begin{problem}
\input{../../modules/diff-eq-separable/homework/mixing-problem-1}
\end{problem}
\input{../../modules/diff-eq-separable/homework/mixing-problem-1-solution}

\subsubsection{Examples and Exercises}

\subsection{Linear differential equations}
A differential equation is \emph{linear} if it is of the form
\begin{equation}\label{eqDFQlinear}
\frac{\diff y}{\diff x} + P(x)y=Q(x)
\end{equation}
for some functions $P(x),Q(x)$.

A linear differential equation \eqref{eqDFQlinear} can be solved as follows.

Let us make the problem slightly more general by considering an equation of the form
\begin{equation}\label{eqDFQlinearStep1}
R(x)\frac{\diff y}{\diff x} + S(x)y=Q(x)\quad .
\end{equation}
Suppose that it so happened that $R'(x)=S(x)$. Then \eqref{eqDFQlinearStep1} becomes
\begin{equation}\label{eqDFQlinearStep2}
\begin{array}{rcl}
\underbrace{R(x)\frac{\diff y}{\diff x} + R'(x)y}_{\mathrm{product~rule~}(fg)'=f'g+g'f}&=&Q(x)\\
\frac{\diff}{\diff x} \left(R(x)y \right)&=& Q(x)\quad .
\end{array}
\end{equation}
In the above we can introduce a new function $z(x)= y(x)R(x)$, and the equation becomes
\[
\frac{\diff z}{\diff x}= Q(x)\quad ,
\]
which we already can solve by integrating both sides to obtain $z(x)= \int\limits_{t=x_0}^x Q(t)\diff t $, or finally, if $R(x)\neq 0$, we get $y(x)= \frac{1}{R(x)}\int\limits_{t=x_0}^x Q(t)\diff t $.
Let us now try to solve \eqref{eqDFQlinear} by finding a multiplier function $T(x)$ that does not vanish, such that when we multiply \eqref{eqDFQlinear}, we obtain an equation of the form \eqref{eqDFQlinearStep2}. Indeed, in order to get an equation of the form \eqref{eqDFQlinearStep2}, we need that $\frac{dT}{\diff x}= (TP(x))$. The latter is a separable differential equation for $T(x)$.  We solve the equation as studied in the preceding section.
\begin{equation}\label{eqDFQlinearStep3}
\begin{array}{rcl}
\frac{dT}{\diff x}\frac{1}{T}&=&P(x)\\
\int\frac{dT}{T}&=&\int P(x)\diff x\\
\ln |T|&=&  \int P(x)\diff x+C\\
T&=&De^{\int P(x)\diff x},
\end{array}
\end{equation}
where $C$ is an arbitrary constant and $D=e^C$. As we have no initial conditions on $\frac{dT}{\diff x}= (TP(x))$ (in fact, we made up that equation ourselves), we are free to choose arbitrary initial conditions. This is reflected in the fact that, somewhat informally, we used indefinite integral instead of definite in the solution of \eqref{eqDFQlinearStep3}.

Motivated by the preceding discussion, define $T(x)\eqdef e^{\int P(x)\diff x}$, where the constant added to the indefinite integral is chosen arbitrarily. $T(x)$ is thus defined only up to a constant.

Multiply \eqref{eqDFQlinear} by $T(x)$ on both sides and transform \eqref{eqDFQlinear} to \eqref{eqDFQlinearStep1}, which we already solved.
\subsubsection{Example of linear differential equations}
Solve
\begin{equation}\label{eqDFQlinearexample1}
\frac{\diff y}{\diff x}+ y x=x\quad .
\end{equation} Find a solution such that $y(0)=3$.

\begin{solution}
First we find an integrating factor $T(x)$. By the theoretical section, $T(x)$ is up to a multiplicative constant equal to $e^{\int x \diff x}= e^{\frac{x^2}2 +C}$. We may choose $T(x):= e^{\frac{x^2}2}$. We multiply \eqref{eqDFQlinearexample1} by $e^{\frac{x^2}2}$ and solve as follows.
\[
\begin{array}{rcl}
\displaystyle e^{\frac{x^2}2}\frac{\diff y}{\diff x}+ yxe^{\frac{x^2}2}&=&\displaystyle xe^{\frac{x^2}2}\\
\underbrace{\frac{\diff}{\diff x} \left(y(x) e^{\frac{x^2}2} \right)}_{\mathrm{rename~}x~\mathrm{to~}t\mathrm{~and~integrate}}&=&\underbrace{xe^{\frac{x^2}2}}_{\mathrm{rename~}x\mathrm{~to~}t\mathrm{~and~integrate}}\\
\displaystyle\int\limits_{t=x_0}^x \frac{\diff}{\diff t} \left(y(t) e^{\frac{t^2}2}\right) \diff t &=&\displaystyle \int\limits_{t=x_0}^x  te^{\frac{t^2}2}\diff t\\
\displaystyle\left.y(t) e^{\frac{t^2}2}\right|_{t=x_0}^x&=&\displaystyle \left.e^{\frac{t^2}2}\right|_{t=x_0}^{t=x}\\
\displaystyle y(x)e^{\frac{x^2}2}- y(x_0)e^{\frac{x_0^2}2}&=&\displaystyle e^{\frac{x^2}2}- e^{\frac{x_0^2}2}\\
y(x)&=&\displaystyle \frac{e^{\frac{x^2}2}- e^{\frac{x_0^2}2}+ y(x_0)e^{\frac{x_0^2}2}}{e^{\frac{x^2}2}}\\
y(x)&=&\displaystyle 1+{e^{\frac{x_0^2-x^2}2}}(y(x_0)-1)\quad.
\end{array}
\]

In order to find a solution for which $y(0)=3$, we simply plug in $x_0=0$, $y(x_0)=3$, to get $y(x)=1+2e^{\frac{-x^2}2}$.

\optionalDisplay{
\begin{pspicture}(-5,-5)(5,5)
\psaxes{<->}(0,0)(-5,-5)(5,5)
\newcommand{\Xstart}{1}
\newcommand{\Ystart}{1}
\psplot[linecolor=green]{-4}{4}{ 2.718281828 \Xstart\space \Xstart\space mul x x mul sub exp \Ystart\space 1 sub mul 1 add}
\renewcommand{\Xstart}{0}
\renewcommand{\Ystart}{-1}
\psplot[linecolor=green]{-4}{4}{ 2.718281828 \Xstart\space \Xstart\space mul x x mul sub exp \Ystart\space 1 sub mul 1 add}
\renewcommand{\Xstart}{0}
\renewcommand{\Ystart}{3}
\psplot[linecolor=green]{-4}{4}{ 2.718281828 \Xstart\space \Xstart\space mul x x mul sub exp \Ystart\space 1 sub mul 1 add}

\rput (5,5){The direction field $\frac{\diff y}{\diff x}=x-xy$}

\fcDirectionFieldDefault{x x y mul sub}{-4}{-4}{0.5}{17}
\end{pspicture}
} %end \optionalDisplay
\end{solution}


%\section{Index}
%\textbf{The index is incomplete at the moment. }
\printindex
\end{document}
